{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd9fcdb",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69fdb7fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T01:30:20.812518Z",
     "start_time": "2024-08-15T01:30:16.343444Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "['TITAN Xp', 'TITAN Xp', 'TITAN Xp', 'TITAN Xp']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch.distributed as dist\n",
    "import logging\n",
    "from PIL import Image\n",
    "from jpdvt import DiT_models, get_2d_sincos_pos_embed\n",
    "from copy import deepcopy\n",
    "from jpdvt_diffusion import create_diffusion\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import os, sys\n",
    "sys.path.append('/home/sjsong/Dropbox/GR/notebook_files/util')\n",
    "from tester import visualLoss\n",
    "\n",
    "gpu_ids = []\n",
    "device_names = []\n",
    "if torch.cuda.is_available():\n",
    "    for gpu_id in range(torch.cuda.device_count()):\n",
    "        gpu_ids += [gpu_id]\n",
    "        device_names += [torch.cuda.get_device_name(gpu_id)]\n",
    "print(gpu_ids)\n",
    "print(device_names)\n",
    "\n",
    "if len(gpu_ids) > 1:\n",
    "    gpu = 'cuda:' + str(gpu_ids[2])  # GPU Number\n",
    "else:\n",
    "    gpu = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ec88d",
   "metadata": {},
   "source": [
    "## Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460c03df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T01:30:20.824631Z",
     "start_time": "2024-08-15T01:30:20.817946Z"
    }
   },
   "outputs": [],
   "source": [
    "device = gpu\n",
    "LEARNING_RATE = 7e-05  # 1e-04\n",
    "BATCH_SIZE = 32  # 96\n",
    "NUM_EPOCHS = 200  # 500\n",
    "NUM_WORKERS = 2\n",
    "TASK_NAME = 'puzzle_cifar10'\n",
    "MODEL_NAME = 'jpdvt'\n",
    "pre_load_model_path = './save/xxx.pt'\n",
    "pre_model_path = f'./save/{TASK_NAME}_{MODEL_NAME}_ep{NUM_EPOCHS}_lr{LEARNING_RATE}_b{BATCH_SIZE}.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10322ed9",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caedc4a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T01:30:24.214550Z",
     "start_time": "2024-08-15T01:30:20.828247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 225\n",
    "AFTER_CROP_SIZE = 192\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dataset = Subset(train_dataset, list(range(int(0.2 * len(train_dataset)))))\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd71b4",
   "metadata": {},
   "source": [
    "## Pre-training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52452529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T01:33:34.274622Z",
     "start_time": "2024-08-15T01:33:34.202181Z"
    }
   },
   "outputs": [],
   "source": [
    "class PreTrainer(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.ema = None\n",
    "        self.epochs = []\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.accuracies_puzzle = []\n",
    "\n",
    "    def process(self, load=False, reload=False):\n",
    "        self.build_model(load)\n",
    "        self.pretrain_model(reload)\n",
    "        self.save_model()\n",
    "\n",
    "    def build_model(self, load):\n",
    "        self.model = DiT_models['JPDVT'](input_size=AFTER_CROP_SIZE).to(device)\n",
    "        print(f'Parameter: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}')\n",
    "        if load:\n",
    "            checkpoint = torch.load(pre_load_model_path)\n",
    "            self.epochs = checkpoint['epochs']\n",
    "            self.model.load_state_dict(checkpoint['model'])\n",
    "            self.losses = checkpoint['losses']\n",
    "            print(f'Parameter: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}')\n",
    "            print(f'Epoch: {self.epochs[-1]}')\n",
    "            print(f'****** Reset epochs and losses ******')\n",
    "            self.epochs = []\n",
    "            self.losses = []\n",
    "\n",
    "    def pretrain_model(self, reload):\n",
    "        self.ema = deepcopy(self.model).to(device)\n",
    "        requires_grad(self.ema, False)\n",
    "        diffusion = create_diffusion(timestep_respacing=\"\")\n",
    "        update_ema(self.ema, self.model, decay=0)\n",
    "        model = self.model.train()\n",
    "        ema = self.ema.eval()\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0)\n",
    "        range_epochs = range(NUM_EPOCHS)\n",
    "\n",
    "        for epoch in range_epochs:\n",
    "            print(f\"epoch {epoch + 1} learning rate : {optimizer.param_groups[0]['lr']}\")\n",
    "            running_loss = 0.\n",
    "            for batch_idx, (x, _) in tqdm(enumerate(train_loader, 0), total=len(train_loader)):\n",
    "                x = x.to(device)\n",
    "                centercrop = transforms.CenterCrop((64, 64))\n",
    "                patchs = rearrange(x, 'b c (p1 h1) (p2 w1)-> b c (p1 p2) h1 w1', p1=3, p2=3, h1=INPUT_SIZE//3, w1=INPUT_SIZE//3)\n",
    "                patchs = centercrop(patchs)\n",
    "                x = rearrange(patchs, 'b c (p1 p2) h1 w1-> b c (p1 h1) (p2 w1)', p1=3, p2=3, h1=AFTER_CROP_SIZE // 3, w1=AFTER_CROP_SIZE // 3)\n",
    "\n",
    "                time_emb = torch.tensor(get_2d_sincos_pos_embed(8, 3)).unsqueeze(0).float().to(device)\n",
    "\n",
    "                t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device)\n",
    "                model_kwargs = None\n",
    "                loss_dict = diffusion.training_losses(model, x, t, time_emb, model_kwargs,\n",
    "                                                      block_size=AFTER_CROP_SIZE // 3, patch_size=16,\n",
    "                                                      add_mask=False)\n",
    "                loss = loss_dict[\"loss\"].mean()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                update_ema(ema, model)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                inter = 100\n",
    "                if batch_idx % inter == inter - 1:\n",
    "                    print(f'[Epoch {epoch + 1}] [Batch {batch_idx + 1}] Loss: {running_loss / inter:.4f}')\n",
    "                    self.epochs.append(epoch + 1)\n",
    "                    self.losses.append(running_loss / inter)\n",
    "                    running_loss = 0.\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            self.save_model()\n",
    "            visualLoss(self.losses)\n",
    "            self.val_model(epoch)\n",
    "        print('****** Finished Fine-tuning ******')\n",
    "        self.model = model\n",
    "\n",
    "    def val_model(self, epoch=-1):\n",
    "        model = self.model.train()\n",
    "\n",
    "        diffusion = create_diffusion('1000')\n",
    "        time_emb = torch.tensor(get_2d_sincos_pos_embed(8, 3)).unsqueeze(0).float().to(device)\n",
    "        time_emb = time_emb.repeat(BATCH_SIZE, 1, 1)\n",
    "        time_emb_noise = torch.tensor(get_2d_sincos_pos_embed(8, 12)).unsqueeze(0).float().to(device)\n",
    "        time_emb_noise = time_emb_noise.repeat(BATCH_SIZE, 1, 1)\n",
    "        time_emb_noise = torch.randn_like(time_emb_noise)\n",
    "        model_kwargs = None\n",
    "\n",
    "        abs_results = []\n",
    "        frag_results = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x, _) in tqdm(enumerate(val_loader, 0), total=len(val_loader)):\n",
    "                x = x.to(device)\n",
    "                x_crop = torch.zeros(BATCH_SIZE, 3, 192, 192).to(device)\n",
    "                for idx, x_ in enumerate(x):\n",
    "                    centercrop = transforms.CenterCrop((64, 64))\n",
    "                    patchs = rearrange(x_, 'c (p1 h1) (p2 w1)-> c (p1 p2) h1 w1', p1=3, p2=3, h1=INPUT_SIZE//3, w1=INPUT_SIZE//3)\n",
    "                    patchs = centercrop(patchs)\n",
    "                    x_ = rearrange(patchs, 'c (p1 p2) h1 w1-> c (p1 h1) (p2 w1)', p1=3, p2=3, h1=AFTER_CROP_SIZE // 3, w1=AFTER_CROP_SIZE // 3)\n",
    "\n",
    "                    indices = np.random.permutation(9)\n",
    "                    x_ = rearrange(x_, 'c (p1 h1) (p2 w1)-> c (p1 p2) h1 w1', p1=3, p2=3, h1=AFTER_CROP_SIZE // 3, w1=AFTER_CROP_SIZE // 3)\n",
    "                    x_ = x_[:, indices, :, :]\n",
    "                    x_ = rearrange(x_, 'c (p1 p2) h1 w1-> c (p1 h1) (p2 w1)', p1=3, p2=3, h1=AFTER_CROP_SIZE // 3, w1=AFTER_CROP_SIZE // 3)\n",
    "                    x_crop[idx] = x_\n",
    "                x = x_crop\n",
    "                \n",
    "                samples = diffusion.p_sample_loop(\n",
    "                    model.forward, x, time_emb_noise.shape, time_emb_noise, clip_denoised=False,\n",
    "                    model_kwargs=model_kwargs, progress=True, device=device\n",
    "                )\n",
    "                for sample, img in zip(samples, x):\n",
    "                    sample = rearrange(sample, '(p1 h1 p2 w1) d-> (p1 p2) (h1 w1) d', p1=3, p2=3, h1=AFTER_CROP_SIZE // 48, w1=AFTER_CROP_SIZE // 48)\n",
    "                    sample = sample.mean(1)\n",
    "                    dist = pairwise_distances(sample.cpu().numpy(), time_emb[0].cpu().numpy(), metric='manhattan')\n",
    "                    order = find_permutation(dist)\n",
    "                    pred = np.asarray(order).argsort()\n",
    "                    abs_results.append(int((pred == indices).all()))\n",
    "                    frag_results.append(np.sum(pred == indices))\n",
    "\n",
    "        puzzle_acc = 100 * np.asarray(abs_results).sum() / len(abs_results)\n",
    "        frag_acc = 100 * np.sum(frag_results) / (len(frag_results) * len(indices))\n",
    "        print(f'[Epoch {epoch + 1}] Accuracy (Puzzle-level) on the test set: {puzzle_acc:.2f}%')\n",
    "        print(f'[Epoch {epoch + 1}] Accuracy (Fragment-level) on the test set: {frag_acc:.2f}%')\n",
    "        self.accuracies.append(frag_acc)\n",
    "        self.accuracies_puzzle.append(puzzle_acc)\n",
    "\n",
    "    def save_model(self):\n",
    "        checkpoint = {\n",
    "            'epochs': self.epochs,\n",
    "            'model': self.model.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'losses': self.losses,\n",
    "            'accuracies': self.accuracies,\n",
    "            'accuracies_puzzle': self.accuracies_puzzle,\n",
    "        }\n",
    "        torch.save(checkpoint, pre_model_path)\n",
    "        # if self.epochs[-1] % 50 == 0:\n",
    "        #     torch.save(checkpoint, pre_model_path[:-3]+f'_{self.epochs[-1]}l{NUM_EPOCHS}.pt')\n",
    "        print(f\"****** Model checkpoint saved at epochs {self.epochs[-1]} ******\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    End DDP training.\n",
    "    \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def create_logger(logging_dir):\n",
    "    \"\"\"\n",
    "    Create a logger that writes to a log file and stdout.\n",
    "    \"\"\"\n",
    "    if dist.get_rank() == 0:  # real logger\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='[\\033[34m%(asctime)s\\033[0m] %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S',\n",
    "            handlers=[logging.StreamHandler(), logging.FileHandler(f\"{logging_dir}/log.txt\")]\n",
    "        )\n",
    "        logger = logging.getLogger(__name__)\n",
    "    else:  # dummy logger (does nothing)\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.addHandler(logging.NullHandler())\n",
    "    return logger\n",
    "\n",
    "\n",
    "def center_crop_arr(pil_image, image_size):\n",
    "    \"\"\"\n",
    "    Center cropping implementation from ADM.\n",
    "    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n",
    "    \"\"\"\n",
    "    while min(*pil_image.size) >= 2 * image_size:\n",
    "        pil_image = pil_image.resize(\n",
    "            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n",
    "        )\n",
    "\n",
    "    scale = image_size / min(*pil_image.size)\n",
    "    pil_image = pil_image.resize(\n",
    "        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n",
    "    )\n",
    "\n",
    "    arr = np.array(pil_image)\n",
    "    crop_y = (arr.shape[0] - image_size) // 2\n",
    "    crop_x = (arr.shape[1] - image_size) // 2\n",
    "    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])\n",
    "\n",
    "\n",
    "def find_permutation(distance_matrix):\n",
    "    sort_list = []\n",
    "    for m in range(distance_matrix.shape[1]):\n",
    "        order = distance_matrix[:, 0].argmin()\n",
    "        sort_list.append(order)\n",
    "        distance_matrix = distance_matrix[:, 1:]\n",
    "        distance_matrix[order, :] = 2024\n",
    "    return sort_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51c9876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T01:35:55.973567Z",
     "start_time": "2024-08-15T01:33:34.773506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 130747208\n",
      "epoch 1 learning rate : 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373104/3147777988.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_idx, (x, _) in tqdm(enumerate(train_loader, 0), total=len(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0a75b8e2b740f7ae66351f94ba357a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] [Batch 2] Loss: 0.4991\n",
      "****** Model checkpoint saved at epochs 1 ******\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373104/3147777988.py:94: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_idx, (x, _) in tqdm(enumerate(val_loader, 0), total=len(val_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908e3c3020b44cdea4142ad76b4cd696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725ea51825bb436c8b0544acacc50a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 1, 1, 1, 1, 2, 3, 0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95902855bbc3449fad6f32adece1d0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 1, 1, 1, 1, 2, 3, 0, 0, 1, 2, 2, 1, 0, 1, 0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ce231f9af844549f2d2fdb83c2c0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m      2\u001B[0m     trainer \u001B[38;5;241m=\u001B[39m PreTrainer()\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36mPreTrainer.process\u001B[0;34m(self, load, reload)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, load\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, reload\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_model(load)\n\u001B[0;32m---> 13\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpretrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreload\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_model()\n",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36mPreTrainer.pretrain_model\u001B[0;34m(self, reload)\u001B[0m\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_model()\n\u001B[1;32m     75\u001B[0m     visualLoss(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlosses)\n\u001B[0;32m---> 76\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mval_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m****** Finished Fine-tuning ******\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36mPreTrainer.val_model\u001B[0;34m(self, epoch)\u001B[0m\n\u001B[1;32m    103\u001B[0m x \u001B[38;5;241m=\u001B[39m x[:, :, indices, :, :]\n\u001B[1;32m    104\u001B[0m x \u001B[38;5;241m=\u001B[39m rearrange(x, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m b c (p1 p2) h1 w1->b c (p1 h1) (p2 w1)\u001B[39m\u001B[38;5;124m'\u001B[39m, p1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, p2\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, h1\u001B[38;5;241m=\u001B[39mAFTER_CROP_SIZE \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m3\u001B[39m, w1\u001B[38;5;241m=\u001B[39mAFTER_CROP_SIZE \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m--> 106\u001B[0m samples \u001B[38;5;241m=\u001B[39m \u001B[43mdiffusion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp_sample_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime_emb_noise\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime_emb_noise\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip_denoised\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sample, img \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(samples, x):\n\u001B[1;32m    111\u001B[0m     sample \u001B[38;5;241m=\u001B[39m rearrange(sample, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(p1 h1 p2 w1) d-> (p1 p2) (h1 w1) d\u001B[39m\u001B[38;5;124m'\u001B[39m, p1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, p2\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, h1\u001B[38;5;241m=\u001B[39mAFTER_CROP_SIZE \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m48\u001B[39m, w1\u001B[38;5;241m=\u001B[39mAFTER_CROP_SIZE \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m48\u001B[39m)\n",
      "File \u001B[0;32m/home/sjsong/Dropbox/GR/notebook_files/jpdvt_gaussian_diffusion.py:465\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_sample_loop\u001B[0;34m(self, model, condition, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001B[0m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;124;03mGenerate samples from the model.\u001B[39;00m\n\u001B[1;32m    448\u001B[0m \u001B[38;5;124;03m:param model: the model module.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;124;03m:return: a non-differentiable batch of samples.\u001B[39;00m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    464\u001B[0m final \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 465\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_sample_loop_progressive(\n\u001B[1;32m    466\u001B[0m         model,\n\u001B[1;32m    467\u001B[0m         condition,\n\u001B[1;32m    468\u001B[0m         shape,\n\u001B[1;32m    469\u001B[0m         noise\u001B[38;5;241m=\u001B[39mnoise,\n\u001B[1;32m    470\u001B[0m         clip_denoised\u001B[38;5;241m=\u001B[39mclip_denoised,\n\u001B[1;32m    471\u001B[0m         denoised_fn\u001B[38;5;241m=\u001B[39mdenoised_fn,\n\u001B[1;32m    472\u001B[0m         cond_fn\u001B[38;5;241m=\u001B[39mcond_fn,\n\u001B[1;32m    473\u001B[0m         model_kwargs\u001B[38;5;241m=\u001B[39mmodel_kwargs,\n\u001B[1;32m    474\u001B[0m         device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m    475\u001B[0m         progress\u001B[38;5;241m=\u001B[39mprogress,\n\u001B[1;32m    476\u001B[0m ):\n\u001B[1;32m    477\u001B[0m     final \u001B[38;5;241m=\u001B[39m sample\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m final[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/home/sjsong/Dropbox/GR/notebook_files/jpdvt_gaussian_diffusion.py:518\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_sample_loop_progressive\u001B[0;34m(self, model, condition, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001B[0m\n\u001B[1;32m    516\u001B[0m t \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mtensor([i] \u001B[38;5;241m*\u001B[39m shape[\u001B[38;5;241m0\u001B[39m], device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m    517\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 518\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcondition\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    521\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnoise\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclip_denoised\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclip_denoised\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdenoised_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdenoised_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcond_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcond_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    528\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m out\n\u001B[1;32m    529\u001B[0m     img \u001B[38;5;241m=\u001B[39m out[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/home/sjsong/Dropbox/GR/notebook_files/jpdvt_gaussian_diffusion.py:415\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_sample\u001B[0;34m(self, model, condition, x, t, clip_denoised, denoised_fn, cond_fn, model_kwargs)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mp_sample\u001B[39m(\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    390\u001B[0m         model,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    397\u001B[0m         model_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    398\u001B[0m ):\n\u001B[1;32m    399\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;124;03m    Sample x_{t-1} from the model at the given timestep.\u001B[39;00m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;124;03m    :param model: the model to sample from.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    413\u001B[0m \u001B[38;5;124;03m             - 'pred_xstart': a prediction of x_0.\u001B[39;00m\n\u001B[1;32m    414\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 415\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp_mean_variance\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcondition\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    419\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    420\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclip_denoised\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclip_denoised\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    421\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdenoised_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdenoised_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    422\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    423\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    424\u001B[0m     noise \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mrandn_like(x)\n\u001B[1;32m    425\u001B[0m     nonzero_mask \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    426\u001B[0m         (t \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39m([\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mlen\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)))\n\u001B[1;32m    427\u001B[0m     )  \u001B[38;5;66;03m# no noise when t == 0\u001B[39;00m\n",
      "File \u001B[0;32m/home/sjsong/Dropbox/GR/notebook_files/jpdvt_respace.py:92\u001B[0m, in \u001B[0;36mSpacedDiffusion.p_mean_variance\u001B[0;34m(self, model, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mp_mean_variance\u001B[39m(\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28mself\u001B[39m, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m     91\u001B[0m ):  \u001B[38;5;66;03m# pylint: disable=signature-differs\u001B[39;00m\n\u001B[0;32m---> 92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp_mean_variance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/home/sjsong/Dropbox/GR/notebook_files/jpdvt_gaussian_diffusion.py:282\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_mean_variance\u001B[0;34m(self, model, condition, x, t, clip_denoised, denoised_fn, model_kwargs)\u001B[0m\n\u001B[1;32m    280\u001B[0m B, C \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m t\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m (B,)\n\u001B[0;32m--> 282\u001B[0m _, x_output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcondition\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;66;03m# breakpoint()\u001B[39;00m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x_output, \u001B[38;5;28mtuple\u001B[39m):\n",
      "File \u001B[0;32m/home/sjsong/Dropbox/GR/notebook_files/jpdvt_respace.py:129\u001B[0m, in \u001B[0;36m_WrappedModel.__call__\u001B[0;34m(self, x, ts, time_emb, **kwargs)\u001B[0m\n\u001B[1;32m    126\u001B[0m new_ts \u001B[38;5;241m=\u001B[39m map_tensor[ts]\n\u001B[1;32m    127\u001B[0m \u001B[38;5;66;03m# if self.rescale_timesteps:\u001B[39;00m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m#     new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\u001B[39;00m\n\u001B[0;32m--> 129\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_ts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/home/sjsong/Dropbox/GR/notebook_files/jpdvt.py:256\u001B[0m, in \u001B[0;36mDiT.forward\u001B[0;34m(self, x, t, time_emb, y)\u001B[0m\n\u001B[1;32m    254\u001B[0m c \u001B[38;5;241m=\u001B[39m t                               \u001B[38;5;66;03m# (N, D)\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[0;32m--> 256\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m                      \u001B[38;5;66;03m# (N, T, D)\u001B[39;00m\n\u001B[1;32m    257\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_layer(x, c)                \u001B[38;5;66;03m# (N, T, patch_size ** 2 * out_channels)\u001B[39;00m\n\u001B[1;32m    258\u001B[0m time_emb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime_emb_out1(x)\n",
      "File \u001B[0;32m/home/sjsong/python3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m/home/sjsong/Dropbox/GR/notebook_files/jpdvt.py:121\u001B[0m, in \u001B[0;36mDiTBlock.forward\u001B[0;34m(self, x, c)\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, c):\n\u001B[1;32m    120\u001B[0m     shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madaLN_modulation(c)\u001B[38;5;241m.\u001B[39mchunk(\u001B[38;5;241m6\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 121\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m gate_msa\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodulate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshift_msa\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_msa\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m gate_mlp\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(modulate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x), shift_mlp, scale_mlp))\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m/home/sjsong/python3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m/home/sjsong/python3.9.7/lib/python3.9/site-packages/timm/models/vision_transformer.py:194\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    191\u001B[0m attn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_drop(attn)\n\u001B[1;32m    193\u001B[0m x \u001B[38;5;241m=\u001B[39m (attn \u001B[38;5;241m@\u001B[39m v)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mreshape(B, N, C)\n\u001B[0;32m--> 194\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    195\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj_drop(x)\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m/home/sjsong/python3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m/home/sjsong/python3.9.7/lib/python3.9/site-packages/torch/nn/modules/linear.py:94\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 94\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/home/sjsong/python3.9.7/lib/python3.9/site-packages/torch/nn/functional.py:1753\u001B[0m, in \u001B[0;36mlinear\u001B[0;34m(input, weight, bias)\u001B[0m\n\u001B[1;32m   1751\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight):\n\u001B[1;32m   1752\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(linear, (\u001B[38;5;28minput\u001B[39m, weight), \u001B[38;5;28minput\u001B[39m, weight, bias\u001B[38;5;241m=\u001B[39mbias)\n\u001B[0;32m-> 1753\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainer = PreTrainer()\n",
    "    trainer.process(load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f1292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T01:33:32.445681Z",
     "start_time": "2024-08-15T01:33:32.445652Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "ㅇㅁㅇ\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
