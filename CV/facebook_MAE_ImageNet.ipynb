{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f500d3",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a152f21",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099f3a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "['NVIDIA GeForce RTX 3090', 'NVIDIA GeForce RTX 3090', 'NVIDIA GeForce RTX 3090', 'NVIDIA GeForce RTX 3090']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from functools import partial\n",
    "\n",
    "import facebook_vit\n",
    "from mae_util import interpolate_pos_embed\n",
    "from timm.models.layers import trunc_normal_\n",
    "from facebook_mae import MaskedAutoencoderViT\n",
    "\n",
    "import facebook_mae\n",
    "import mae_misc as misc\n",
    "from mae_misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "\n",
    "gpu_ids = []\n",
    "device_names = []\n",
    "if torch.cuda.is_available():\n",
    "    for gpu_id in range(torch.cuda.device_count()):\n",
    "        gpu_ids += [gpu_id]\n",
    "        device_names += [torch.cuda.get_device_name(gpu_id)]\n",
    "print(gpu_ids)\n",
    "print(device_names)\n",
    "\n",
    "if len(gpu_ids) > 1:\n",
    "    gpu = 'cuda:' + str(gpu_ids[0])  # GPU Number\n",
    "else:\n",
    "    gpu = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e4b63c",
   "metadata": {},
   "source": [
    "## Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bd3005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = gpu\n",
    "BATCH_SIZE = 64  # 1024\n",
    "NUM_EPOCHS = 15  # 100\n",
    "WARMUP_EPOCHS = 5  # 5\n",
    "NUM_WORKERS = 2\n",
    "LEARNING_RATE = 4e-06  # paper: 1e-03 -> implementation: 5e-04\n",
    "pre_model_path = f'./save/mae_base_i2012_ep{NUM_EPOCHS}_lr{LEARNING_RATE}.pt'\n",
    "load_model_path = './save/MAE/mae_finetuned_vit_base_given.pth'\n",
    "fine_model_path = f'./save/mae_vit_base_i2012_ep{NUM_EPOCHS}_lr{LEARNING_RATE}.pt'\n",
    "dynamic_model_path = f'./save/mae_vit_base_i2012_ep'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e612562",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b07ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "train_set = torchvision.datasets.ImageFolder('../datasets/ImageNet/train', transform=transform_train)\n",
    "train_loader = data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_set = torchvision.datasets.ImageFolder('../datasets/ImageNet/val', transform=transform_test)\n",
    "test_loader = data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fb641",
   "metadata": {},
   "source": [
    "## Fine-tuning Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f016f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuner(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.epochs = [0]\n",
    "        self.losses = [0]\n",
    "        self.accuracies = [0]\n",
    "\n",
    "    def process(self, load=False):\n",
    "        self.build_model(load)\n",
    "        self.finetune_model()\n",
    "        self.save_model()\n",
    "\n",
    "    def build_model(self, load):\n",
    "        self.model = facebook_vit.__dict__['vit_base_patch16'](\n",
    "            num_classes=1000,\n",
    "            drop_path_rate=0.1,\n",
    "            )\n",
    "        print(f'Parameter: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}')\n",
    "        self.optimizer = SGD(self.model.parameters(), lr=0)\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "        if load:\n",
    "            checkpoint = torch.load(load_model_path)\n",
    "            checkpoint_model = checkpoint['model']\n",
    "            state_dict = self.model.state_dict()\n",
    "            for k in ['head.weight', 'head.bias']:\n",
    "                if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "                    print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                    del checkpoint_model[k]\n",
    "            interpolate_pos_embed(self.model, checkpoint_model)\n",
    "            msg = self.model.load_state_dict(checkpoint_model, strict=False)\n",
    "            print(msg)\n",
    "            trunc_normal_(self.model.head.weight, std=2e-5)\n",
    "            self.model.to(device)\n",
    "\n",
    "            if 'given' not in str(load_model_path):\n",
    "                self.epochs = checkpoint['epochs']\n",
    "                self.losses = checkpoint['losses']\n",
    "                self.accuracies = checkpoint['accuracies']\n",
    "            print(f'Parameter: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}')\n",
    "            print(f'Epoch: {self.epochs[-1]}')\n",
    "            print(f'****** Reset epochs and losses ******')\n",
    "            self.epochs = []\n",
    "            self.losses = []\n",
    "            self.accuracies = []\n",
    "\n",
    "    def finetune_model(self):\n",
    "        model = self.model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            if epoch < WARMUP_EPOCHS:\n",
    "                lr_warmup = ((epoch + 1) / WARMUP_EPOCHS) * LEARNING_RATE\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_warmup\n",
    "                if epoch + 1 == WARMUP_EPOCHS:\n",
    "                    scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "            print(f\"epoch {epoch+1} learning rate : {optimizer.param_groups[0]['lr']}\")\n",
    "            running_loss = 0.0\n",
    "            saving_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, data in tqdm_notebook(enumerate(train_loader, 0), total=len(train_loader)):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                saving_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                if i % 100 == 99:\n",
    "                    print(f'[Epoch {epoch}, Batch {i + 1:5d}] loss: {running_loss / 100:.3f}, acc: {correct/total*100:.2f} %')\n",
    "                    running_loss = 0.0\n",
    "                if i % 1000 == 999:\n",
    "                    self.epochs.append(epoch + 1)\n",
    "                    self.losses.append(saving_loss/1000)\n",
    "                    self.accuracies.append(correct/total*100)\n",
    "                    saving_loss = 0.0\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            self.scheduler = scheduler\n",
    "            self.save_model()\n",
    "            scheduler.step()\n",
    "        print('****** Finished Fine-tuning ******')\n",
    "\n",
    "    def save_model(self):\n",
    "        checkpoint = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler': self.scheduler.state_dict(),\n",
    "            'epochs': self.epochs,\n",
    "            'losses': self.losses,\n",
    "            'accuracies': self.accuracies,\n",
    "        }\n",
    "        torch.save(checkpoint, fine_model_path)\n",
    "#         torch.save(checkpoint, dynamic_model_path+str(self.epochs[-1])+f'_lr{LEARNING_RATE}.pt')\n",
    "        print(f\"****** Model checkpoint saved at epochs {self.epochs[-1]} ******\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b492a",
   "metadata": {},
   "source": [
    "## Pre-training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86332952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainer(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.epochs = [0]\n",
    "        self.losses = [0]\n",
    "        self.accuracies = [0]\n",
    "\n",
    "    def process(self, load=False):\n",
    "        self.build_model(load)\n",
    "        self.pretrain_model()\n",
    "        self.save_model()\n",
    "\n",
    "    def build_model(self, load):\n",
    "        self.model = facebook_mae.__dict__['mae_vit_base_patch16_dec512d8b'](norm_pix_loss=False).to(device)\n",
    "        print(f'Parameter: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}')\n",
    "        self.optimizer = SGD(self.model.parameters(), lr=0)\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "        if load:\n",
    "            checkpoint = torch.load(load_model_path)\n",
    "            self.model.load_state_dict(checkpoint['model'])\n",
    "            if 'given' not in str(load_model_path):\n",
    "                self.epochs = checkpoint['epochs']\n",
    "                self.losses = checkpoint['losses']\n",
    "                self.accuracies = checkpoint['accuracies']\n",
    "            print(f'Parameter: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}')\n",
    "            print(f'Epoch: {self.epochs[-1]}')\n",
    "            print(f'****** Reset epochs and losses ******')\n",
    "            self.epochs = []\n",
    "            self.losses = []\n",
    "            self.accuracies = []\n",
    "\n",
    "    def pretrain_model(self):\n",
    "        model = self.model.train()\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95))\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "        loss_scaler = NativeScaler()\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            if epoch < WARMUP_EPOCHS:\n",
    "                lr_warmup = ((epoch + 1) / WARMUP_EPOCHS) * LEARNING_RATE\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_warmup\n",
    "                if epoch + 1 == WARMUP_EPOCHS:\n",
    "                    scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "            print(f\"epoch {epoch + 1} learning rate : {optimizer.param_groups[0]['lr']}\")\n",
    "            running_loss = 0.0\n",
    "            saving_loss = 0.0\n",
    "            for i, data in tqdm_notebook(enumerate(train_loader, 0), total=len(train_loader)):\n",
    "                samples, _ = data\n",
    "                samples = samples.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss, _, _ = model(samples, mask_ratio=.75)\n",
    "                loss_scaler(loss, optimizer, parameters=model.parameters(), update_grad=True)\n",
    "                # Scaler include loss.backward() and optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                saving_loss += loss.item()\n",
    "\n",
    "                if i % 100 == 99:\n",
    "                    print(f'[Epoch {epoch}, Batch {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                    running_loss = 0.0\n",
    "                if i % 1000 == 999:\n",
    "                    self.epochs.append(epoch + 1)\n",
    "                    self.losses.append(saving_loss/1000)\n",
    "                    saving_loss = 0.0\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            self.scheduler = scheduler\n",
    "            self.save_model()\n",
    "            scheduler.step()\n",
    "        print('****** Finished Fine-tuning ******')\n",
    "\n",
    "    def save_model(self):\n",
    "        checkpoint = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler': self.scheduler.state_dict(),\n",
    "            'epochs': self.epochs,\n",
    "            'losses': self.losses,\n",
    "        }\n",
    "        torch.save(checkpoint, pre_model_path)\n",
    "#         torch.save(checkpoint, dynamic_model_path+str(self.epochs[-1])+f'_lr{LEARNING_RATE}.pt')\n",
    "        print(f\"****** Model checkpoint saved at epochs {self.epochs[-1]} ******\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c987226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 86567656\n",
      "_IncompatibleKeys(missing_keys=['norm.weight', 'norm.bias'], unexpected_keys=['fc_norm.weight', 'fc_norm.bias'])\n",
      "Parameter: 86567656\n",
      "Epoch: 0\n",
      "****** Reset epochs and losses ******\n",
      "epoch 1 learning rate : 8e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c4c83017c245b3a7d7896447c73500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0, Batch   100] loss: 6.919, acc: 0.08 %\n",
      "[Epoch 0, Batch   200] loss: 6.919, acc: 0.09 %\n",
      "[Epoch 0, Batch   300] loss: 6.919, acc: 0.08 %\n",
      "[Epoch 0, Batch   400] loss: 6.916, acc: 0.10 %\n",
      "[Epoch 0, Batch   500] loss: 6.912, acc: 0.09 %\n",
      "[Epoch 0, Batch   600] loss: 6.897, acc: 0.09 %\n",
      "[Epoch 0, Batch   700] loss: 6.883, acc: 0.09 %\n",
      "[Epoch 0, Batch   800] loss: 6.877, acc: 0.09 %\n",
      "[Epoch 0, Batch   900] loss: 6.871, acc: 0.09 %\n",
      "[Epoch 0, Batch  1000] loss: 6.860, acc: 0.10 %\n",
      "[Epoch 0, Batch  1100] loss: 6.849, acc: 0.17 %\n",
      "[Epoch 0, Batch  1200] loss: 6.839, acc: 0.15 %\n",
      "[Epoch 0, Batch  1300] loss: 6.830, acc: 0.15 %\n",
      "[Epoch 0, Batch  1400] loss: 6.824, acc: 0.14 %\n",
      "[Epoch 0, Batch  1500] loss: 6.814, acc: 0.14 %\n",
      "[Epoch 0, Batch  1600] loss: 6.808, acc: 0.14 %\n",
      "[Epoch 0, Batch  1700] loss: 6.799, acc: 0.16 %\n",
      "[Epoch 0, Batch  1800] loss: 6.785, acc: 0.16 %\n",
      "[Epoch 0, Batch  1900] loss: 6.783, acc: 0.17 %\n",
      "[Epoch 0, Batch  2000] loss: 6.776, acc: 0.18 %\n",
      "[Epoch 0, Batch  2100] loss: 6.765, acc: 0.23 %\n",
      "[Epoch 0, Batch  2200] loss: 6.755, acc: 0.30 %\n",
      "[Epoch 0, Batch  2300] loss: 6.744, acc: 0.34 %\n",
      "[Epoch 0, Batch  2400] loss: 6.739, acc: 0.40 %\n",
      "[Epoch 0, Batch  2500] loss: 6.731, acc: 0.42 %\n",
      "[Epoch 0, Batch  2600] loss: 6.725, acc: 0.42 %\n",
      "[Epoch 0, Batch  2700] loss: 6.712, acc: 0.41 %\n",
      "[Epoch 0, Batch  2800] loss: 6.704, acc: 0.41 %\n",
      "[Epoch 0, Batch  2900] loss: 6.697, acc: 0.42 %\n",
      "[Epoch 0, Batch  3000] loss: 6.690, acc: 0.43 %\n",
      "[Epoch 0, Batch  3100] loss: 6.681, acc: 0.50 %\n",
      "[Epoch 0, Batch  3200] loss: 6.669, acc: 0.54 %\n",
      "[Epoch 0, Batch  3300] loss: 6.665, acc: 0.48 %\n",
      "[Epoch 0, Batch  3400] loss: 6.656, acc: 0.49 %\n",
      "[Epoch 0, Batch  3500] loss: 6.651, acc: 0.51 %\n",
      "[Epoch 0, Batch  3600] loss: 6.639, acc: 0.54 %\n",
      "[Epoch 0, Batch  3700] loss: 6.632, acc: 0.60 %\n",
      "[Epoch 0, Batch  3800] loss: 6.621, acc: 0.64 %\n",
      "[Epoch 0, Batch  3900] loss: 6.615, acc: 0.65 %\n",
      "[Epoch 0, Batch  4000] loss: 6.608, acc: 0.66 %\n",
      "[Epoch 0, Batch  4100] loss: 6.598, acc: 0.92 %\n",
      "[Epoch 0, Batch  4200] loss: 6.590, acc: 0.91 %\n",
      "[Epoch 0, Batch  4300] loss: 6.584, acc: 0.96 %\n",
      "[Epoch 0, Batch  4400] loss: 6.573, acc: 0.94 %\n",
      "[Epoch 0, Batch  4500] loss: 6.566, acc: 1.00 %\n",
      "[Epoch 0, Batch  4600] loss: 6.558, acc: 1.00 %\n",
      "[Epoch 0, Batch  4700] loss: 6.550, acc: 1.04 %\n",
      "[Epoch 0, Batch  4800] loss: 6.542, acc: 1.05 %\n",
      "[Epoch 0, Batch  4900] loss: 6.534, acc: 1.06 %\n",
      "[Epoch 0, Batch  5000] loss: 6.529, acc: 1.08 %\n",
      "[Epoch 0, Batch  5100] loss: 6.520, acc: 1.34 %\n",
      "[Epoch 0, Batch  5200] loss: 6.513, acc: 1.30 %\n",
      "[Epoch 0, Batch  5300] loss: 6.503, acc: 1.40 %\n",
      "[Epoch 0, Batch  5400] loss: 6.494, acc: 1.48 %\n",
      "[Epoch 0, Batch  5500] loss: 6.489, acc: 1.49 %\n",
      "[Epoch 0, Batch  5600] loss: 6.480, acc: 1.54 %\n",
      "[Epoch 0, Batch  5700] loss: 6.475, acc: 1.57 %\n",
      "[Epoch 0, Batch  5800] loss: 6.462, acc: 1.64 %\n",
      "[Epoch 0, Batch  5900] loss: 6.459, acc: 1.67 %\n",
      "[Epoch 0, Batch  6000] loss: 6.448, acc: 1.72 %\n",
      "[Epoch 0, Batch  6100] loss: 6.441, acc: 2.14 %\n",
      "[Epoch 0, Batch  6200] loss: 6.433, acc: 2.09 %\n",
      "[Epoch 0, Batch  6300] loss: 6.428, acc: 2.31 %\n",
      "[Epoch 0, Batch  6400] loss: 6.416, acc: 2.42 %\n",
      "[Epoch 0, Batch  6500] loss: 6.414, acc: 2.49 %\n",
      "[Epoch 0, Batch  6600] loss: 6.403, acc: 2.54 %\n",
      "[Epoch 0, Batch  6700] loss: 6.393, acc: 2.59 %\n",
      "[Epoch 0, Batch  6800] loss: 6.390, acc: 2.65 %\n",
      "[Epoch 0, Batch  6900] loss: 6.377, acc: 2.74 %\n",
      "[Epoch 0, Batch  7000] loss: 6.373, acc: 2.83 %\n",
      "[Epoch 0, Batch  7100] loss: 6.365, acc: 3.42 %\n",
      "[Epoch 0, Batch  7200] loss: 6.355, acc: 3.34 %\n",
      "[Epoch 0, Batch  7300] loss: 6.351, acc: 3.43 %\n",
      "[Epoch 0, Batch  7400] loss: 6.337, acc: 3.55 %\n",
      "[Epoch 0, Batch  7500] loss: 6.334, acc: 3.80 %\n",
      "[Epoch 0, Batch  7600] loss: 6.327, acc: 3.93 %\n",
      "[Epoch 0, Batch  7700] loss: 6.318, acc: 3.97 %\n",
      "[Epoch 0, Batch  7800] loss: 6.311, acc: 4.02 %\n",
      "[Epoch 0, Batch  7900] loss: 6.305, acc: 4.12 %\n",
      "[Epoch 0, Batch  8000] loss: 6.296, acc: 4.18 %\n",
      "[Epoch 0, Batch  8100] loss: 6.287, acc: 4.56 %\n",
      "[Epoch 0, Batch  8200] loss: 6.282, acc: 4.83 %\n",
      "[Epoch 0, Batch  8300] loss: 6.273, acc: 4.89 %\n",
      "[Epoch 0, Batch  8400] loss: 6.272, acc: 4.93 %\n",
      "[Epoch 0, Batch  8500] loss: 6.255, acc: 5.06 %\n",
      "[Epoch 0, Batch  8600] loss: 6.250, acc: 5.16 %\n",
      "[Epoch 0, Batch  8700] loss: 6.244, acc: 5.32 %\n",
      "[Epoch 0, Batch  8800] loss: 6.240, acc: 5.43 %\n",
      "[Epoch 0, Batch  8900] loss: 6.227, acc: 5.47 %\n",
      "[Epoch 0, Batch  9000] loss: 6.220, acc: 5.53 %\n",
      "[Epoch 0, Batch  9100] loss: 6.217, acc: 6.33 %\n",
      "[Epoch 0, Batch  9200] loss: 6.207, acc: 7.07 %\n",
      "[Epoch 0, Batch  9300] loss: 6.204, acc: 7.08 %\n",
      "[Epoch 0, Batch  9400] loss: 6.192, acc: 7.17 %\n",
      "[Epoch 0, Batch  9500] loss: 6.186, acc: 7.23 %\n",
      "[Epoch 0, Batch  9600] loss: 6.176, acc: 7.27 %\n",
      "[Epoch 0, Batch  9700] loss: 6.173, acc: 7.33 %\n",
      "[Epoch 0, Batch  9800] loss: 6.163, acc: 7.38 %\n",
      "[Epoch 0, Batch  9900] loss: 6.157, acc: 7.45 %\n",
      "[Epoch 0, Batch 10000] loss: 6.150, acc: 7.51 %\n",
      "[Epoch 0, Batch 10100] loss: 6.140, acc: 8.97 %\n",
      "[Epoch 0, Batch 10200] loss: 6.136, acc: 9.08 %\n",
      "[Epoch 0, Batch 10300] loss: 6.130, acc: 9.16 %\n",
      "[Epoch 0, Batch 10400] loss: 6.121, acc: 9.43 %\n",
      "[Epoch 0, Batch 10500] loss: 6.110, acc: 9.62 %\n",
      "[Epoch 0, Batch 10600] loss: 6.104, acc: 9.77 %\n",
      "[Epoch 0, Batch 10700] loss: 6.104, acc: 9.88 %\n",
      "[Epoch 0, Batch 10800] loss: 6.092, acc: 10.12 %\n",
      "[Epoch 0, Batch 10900] loss: 6.086, acc: 10.24 %\n",
      "[Epoch 0, Batch 11000] loss: 6.079, acc: 10.42 %\n",
      "[Epoch 0, Batch 11100] loss: 6.074, acc: 12.41 %\n",
      "[Epoch 0, Batch 11200] loss: 6.064, acc: 12.21 %\n",
      "[Epoch 0, Batch 11300] loss: 6.055, acc: 12.57 %\n",
      "[Epoch 0, Batch 11400] loss: 6.051, acc: 12.62 %\n",
      "[Epoch 0, Batch 11500] loss: 6.044, acc: 12.72 %\n",
      "[Epoch 0, Batch 11600] loss: 6.038, acc: 12.82 %\n",
      "[Epoch 0, Batch 11700] loss: 6.027, acc: 12.97 %\n",
      "[Epoch 0, Batch 11800] loss: 6.023, acc: 13.08 %\n",
      "[Epoch 0, Batch 11900] loss: 6.017, acc: 13.17 %\n",
      "[Epoch 0, Batch 12000] loss: 6.009, acc: 13.41 %\n",
      "[Epoch 0, Batch 12100] loss: 6.003, acc: 15.42 %\n",
      "[Epoch 0, Batch 12200] loss: 5.994, acc: 15.20 %\n",
      "[Epoch 0, Batch 12300] loss: 5.990, acc: 15.33 %\n",
      "[Epoch 0, Batch 12400] loss: 5.980, acc: 15.64 %\n",
      "[Epoch 0, Batch 12500] loss: 5.972, acc: 15.65 %\n",
      "[Epoch 0, Batch 12600] loss: 5.969, acc: 15.68 %\n",
      "[Epoch 0, Batch 12700] loss: 5.961, acc: 15.81 %\n",
      "[Epoch 0, Batch 12800] loss: 5.958, acc: 15.96 %\n",
      "[Epoch 0, Batch 12900] loss: 5.946, acc: 16.02 %\n",
      "[Epoch 0, Batch 13000] loss: 5.940, acc: 16.23 %\n",
      "[Epoch 0, Batch 13100] loss: 5.937, acc: 17.84 %\n",
      "[Epoch 0, Batch 13200] loss: 5.928, acc: 17.94 %\n",
      "[Epoch 0, Batch 13300] loss: 5.919, acc: 17.95 %\n",
      "[Epoch 0, Batch 13400] loss: 5.918, acc: 18.02 %\n",
      "[Epoch 0, Batch 13500] loss: 5.913, acc: 18.14 %\n",
      "[Epoch 0, Batch 13600] loss: 5.902, acc: 18.34 %\n",
      "[Epoch 0, Batch 13700] loss: 5.889, acc: 18.58 %\n",
      "[Epoch 0, Batch 13800] loss: 5.888, acc: 18.71 %\n",
      "[Epoch 0, Batch 13900] loss: 5.882, acc: 18.87 %\n",
      "[Epoch 0, Batch 14000] loss: 5.873, acc: 18.97 %\n",
      "[Epoch 0, Batch 14100] loss: 5.863, acc: 20.67 %\n",
      "[Epoch 0, Batch 14200] loss: 5.857, acc: 20.51 %\n",
      "[Epoch 0, Batch 14300] loss: 5.851, acc: 20.54 %\n",
      "[Epoch 0, Batch 14400] loss: 5.847, acc: 20.71 %\n",
      "[Epoch 0, Batch 14500] loss: 5.838, acc: 20.93 %\n",
      "[Epoch 0, Batch 14600] loss: 5.834, acc: 21.08 %\n",
      "[Epoch 0, Batch 14700] loss: 5.825, acc: 21.38 %\n",
      "[Epoch 0, Batch 14800] loss: 5.812, acc: 21.48 %\n",
      "[Epoch 0, Batch 14900] loss: 5.812, acc: 21.59 %\n",
      "[Epoch 0, Batch 15000] loss: 5.810, acc: 21.70 %\n",
      "[Epoch 0, Batch 15100] loss: 5.796, acc: 23.31 %\n",
      "[Epoch 0, Batch 15200] loss: 5.794, acc: 23.30 %\n",
      "[Epoch 0, Batch 15300] loss: 5.788, acc: 23.64 %\n",
      "[Epoch 0, Batch 15400] loss: 5.775, acc: 23.91 %\n",
      "[Epoch 0, Batch 15500] loss: 5.771, acc: 23.92 %\n",
      "[Epoch 0, Batch 15600] loss: 5.766, acc: 24.01 %\n",
      "[Epoch 0, Batch 15700] loss: 5.760, acc: 24.05 %\n",
      "[Epoch 0, Batch 15800] loss: 5.759, acc: 24.20 %\n",
      "[Epoch 0, Batch 15900] loss: 5.747, acc: 24.35 %\n",
      "[Epoch 0, Batch 16000] loss: 5.740, acc: 24.45 %\n",
      "[Epoch 0, Batch 16100] loss: 5.725, acc: 26.30 %\n",
      "[Epoch 0, Batch 16200] loss: 5.729, acc: 25.27 %\n",
      "[Epoch 0, Batch 16300] loss: 5.721, acc: 25.61 %\n",
      "[Epoch 0, Batch 16400] loss: 5.717, acc: 25.54 %\n",
      "[Epoch 0, Batch 16500] loss: 5.709, acc: 25.76 %\n",
      "[Epoch 0, Batch 16600] loss: 5.699, acc: 26.12 %\n",
      "[Epoch 0, Batch 16700] loss: 5.700, acc: 26.33 %\n",
      "[Epoch 0, Batch 16800] loss: 5.688, acc: 26.52 %\n",
      "[Epoch 0, Batch 16900] loss: 5.687, acc: 26.65 %\n",
      "[Epoch 0, Batch 17000] loss: 5.675, acc: 26.78 %\n",
      "[Epoch 0, Batch 17100] loss: 5.665, acc: 29.91 %\n",
      "[Epoch 0, Batch 17200] loss: 5.668, acc: 29.65 %\n",
      "[Epoch 0, Batch 17300] loss: 5.657, acc: 29.35 %\n",
      "[Epoch 0, Batch 17400] loss: 5.647, acc: 29.30 %\n",
      "[Epoch 0, Batch 17500] loss: 5.640, acc: 29.27 %\n",
      "[Epoch 0, Batch 17600] loss: 5.634, acc: 29.33 %\n",
      "[Epoch 0, Batch 17700] loss: 5.634, acc: 29.46 %\n",
      "[Epoch 0, Batch 17800] loss: 5.630, acc: 29.52 %\n",
      "[Epoch 0, Batch 17900] loss: 5.624, acc: 29.70 %\n",
      "[Epoch 0, Batch 18000] loss: 5.615, acc: 29.73 %\n",
      "[Epoch 0, Batch 18100] loss: 5.603, acc: 31.22 %\n",
      "[Epoch 0, Batch 18200] loss: 5.605, acc: 30.77 %\n",
      "[Epoch 0, Batch 18300] loss: 5.598, acc: 31.07 %\n",
      "[Epoch 0, Batch 18400] loss: 5.589, acc: 31.20 %\n",
      "[Epoch 0, Batch 18500] loss: 5.581, acc: 31.49 %\n",
      "[Epoch 0, Batch 18600] loss: 5.583, acc: 31.77 %\n",
      "[Epoch 0, Batch 18700] loss: 5.575, acc: 31.99 %\n",
      "[Epoch 0, Batch 18800] loss: 5.561, acc: 32.17 %\n",
      "[Epoch 0, Batch 18900] loss: 5.556, acc: 32.31 %\n",
      "[Epoch 0, Batch 19000] loss: 5.550, acc: 32.49 %\n",
      "[Epoch 0, Batch 19100] loss: 5.543, acc: 32.81 %\n",
      "[Epoch 0, Batch 19200] loss: 5.535, acc: 33.74 %\n",
      "[Epoch 0, Batch 19300] loss: 5.535, acc: 33.60 %\n",
      "[Epoch 0, Batch 19400] loss: 5.535, acc: 33.73 %\n",
      "[Epoch 0, Batch 19500] loss: 5.519, acc: 34.01 %\n",
      "[Epoch 0, Batch 19600] loss: 5.518, acc: 34.32 %\n",
      "[Epoch 0, Batch 19700] loss: 5.513, acc: 34.52 %\n",
      "[Epoch 0, Batch 19800] loss: 5.506, acc: 34.64 %\n",
      "[Epoch 0, Batch 19900] loss: 5.490, acc: 34.74 %\n",
      "[Epoch 0, Batch 20000] loss: 5.485, acc: 34.87 %\n",
      "****** Model checkpoint saved at epochs 1 ******\n",
      "epoch 2 learning rate : 1.6e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed9cce8dfc345d59dbd92616e823cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch   100] loss: 5.475, acc: 36.86 %\n",
      "[Epoch 1, Batch   200] loss: 5.458, acc: 36.35 %\n",
      "[Epoch 1, Batch   300] loss: 5.461, acc: 36.27 %\n",
      "[Epoch 1, Batch   400] loss: 5.441, acc: 36.58 %\n",
      "[Epoch 1, Batch   500] loss: 5.427, acc: 36.92 %\n",
      "[Epoch 1, Batch   600] loss: 5.421, acc: 37.00 %\n",
      "[Epoch 1, Batch   700] loss: 5.404, acc: 37.19 %\n",
      "[Epoch 1, Batch   800] loss: 5.388, acc: 37.39 %\n",
      "[Epoch 1, Batch   900] loss: 5.382, acc: 37.45 %\n",
      "[Epoch 1, Batch  1000] loss: 5.368, acc: 37.64 %\n",
      "[Epoch 1, Batch  1100] loss: 5.360, acc: 39.17 %\n",
      "[Epoch 1, Batch  1200] loss: 5.347, acc: 38.70 %\n",
      "[Epoch 1, Batch  1300] loss: 5.329, acc: 38.77 %\n",
      "[Epoch 1, Batch  1400] loss: 5.331, acc: 38.73 %\n",
      "[Epoch 1, Batch  1500] loss: 5.313, acc: 38.73 %\n",
      "[Epoch 1, Batch  1600] loss: 5.301, acc: 38.70 %\n",
      "[Epoch 1, Batch  1700] loss: 5.293, acc: 38.82 %\n",
      "[Epoch 1, Batch  1800] loss: 5.280, acc: 38.85 %\n",
      "[Epoch 1, Batch  1900] loss: 5.264, acc: 38.97 %\n",
      "[Epoch 1, Batch  2000] loss: 5.258, acc: 38.98 %\n",
      "[Epoch 1, Batch  2100] loss: 5.242, acc: 40.41 %\n",
      "[Epoch 1, Batch  2200] loss: 5.239, acc: 40.19 %\n",
      "[Epoch 1, Batch  2300] loss: 5.221, acc: 40.78 %\n",
      "[Epoch 1, Batch  2400] loss: 5.196, acc: 40.87 %\n",
      "[Epoch 1, Batch  2500] loss: 5.197, acc: 40.89 %\n",
      "[Epoch 1, Batch  2600] loss: 5.186, acc: 40.85 %\n",
      "[Epoch 1, Batch  2700] loss: 5.182, acc: 40.82 %\n",
      "[Epoch 1, Batch  2800] loss: 5.170, acc: 40.94 %\n",
      "[Epoch 1, Batch  2900] loss: 5.153, acc: 40.91 %\n",
      "[Epoch 1, Batch  3000] loss: 5.141, acc: 40.92 %\n",
      "[Epoch 1, Batch  3100] loss: 5.143, acc: 40.97 %\n",
      "[Epoch 1, Batch  3200] loss: 5.126, acc: 40.88 %\n",
      "[Epoch 1, Batch  3300] loss: 5.125, acc: 40.91 %\n",
      "[Epoch 1, Batch  3400] loss: 5.096, acc: 41.18 %\n",
      "[Epoch 1, Batch  3500] loss: 5.092, acc: 41.45 %\n",
      "[Epoch 1, Batch  3600] loss: 5.068, acc: 41.77 %\n",
      "[Epoch 1, Batch  3700] loss: 5.070, acc: 41.89 %\n",
      "[Epoch 1, Batch  3800] loss: 5.065, acc: 41.98 %\n",
      "[Epoch 1, Batch  3900] loss: 5.038, acc: 42.19 %\n",
      "[Epoch 1, Batch  4000] loss: 5.039, acc: 42.29 %\n",
      "[Epoch 1, Batch  4100] loss: 5.026, acc: 44.47 %\n",
      "[Epoch 1, Batch  4200] loss: 5.006, acc: 44.75 %\n",
      "[Epoch 1, Batch  4300] loss: 4.990, acc: 44.92 %\n",
      "[Epoch 1, Batch  4400] loss: 4.985, acc: 44.70 %\n",
      "[Epoch 1, Batch  4500] loss: 4.971, acc: 44.61 %\n",
      "[Epoch 1, Batch  4600] loss: 4.974, acc: 44.45 %\n",
      "[Epoch 1, Batch  4700] loss: 4.958, acc: 44.49 %\n",
      "[Epoch 1, Batch  4800] loss: 4.944, acc: 44.64 %\n",
      "[Epoch 1, Batch  4900] loss: 4.934, acc: 44.59 %\n",
      "[Epoch 1, Batch  5000] loss: 4.919, acc: 44.71 %\n",
      "[Epoch 1, Batch  5100] loss: 4.902, acc: 46.34 %\n",
      "[Epoch 1, Batch  5200] loss: 4.902, acc: 46.06 %\n",
      "[Epoch 1, Batch  5300] loss: 4.889, acc: 46.11 %\n",
      "[Epoch 1, Batch  5400] loss: 4.884, acc: 45.98 %\n",
      "[Epoch 1, Batch  5500] loss: 4.881, acc: 45.90 %\n",
      "[Epoch 1, Batch  5600] loss: 4.865, acc: 45.87 %\n",
      "[Epoch 1, Batch  5700] loss: 4.854, acc: 46.06 %\n",
      "[Epoch 1, Batch  5800] loss: 4.850, acc: 46.11 %\n",
      "[Epoch 1, Batch  5900] loss: 4.828, acc: 46.17 %\n",
      "[Epoch 1, Batch  6000] loss: 4.817, acc: 46.33 %\n",
      "[Epoch 1, Batch  6100] loss: 4.816, acc: 47.06 %\n",
      "[Epoch 1, Batch  6200] loss: 4.796, acc: 47.39 %\n",
      "[Epoch 1, Batch  6300] loss: 4.796, acc: 47.40 %\n",
      "[Epoch 1, Batch  6400] loss: 4.786, acc: 47.28 %\n",
      "[Epoch 1, Batch  6500] loss: 4.764, acc: 47.38 %\n",
      "[Epoch 1, Batch  6600] loss: 4.751, acc: 47.48 %\n",
      "[Epoch 1, Batch  6700] loss: 4.750, acc: 47.59 %\n",
      "[Epoch 1, Batch  6800] loss: 4.733, acc: 47.56 %\n",
      "[Epoch 1, Batch  6900] loss: 4.720, acc: 47.54 %\n",
      "[Epoch 1, Batch  7000] loss: 4.716, acc: 47.61 %\n",
      "[Epoch 1, Batch  7100] loss: 4.712, acc: 48.05 %\n",
      "[Epoch 1, Batch  7200] loss: 4.700, acc: 47.91 %\n",
      "[Epoch 1, Batch  7300] loss: 4.687, acc: 47.81 %\n",
      "[Epoch 1, Batch  7400] loss: 4.670, acc: 47.98 %\n",
      "[Epoch 1, Batch  7500] loss: 4.659, acc: 48.02 %\n",
      "[Epoch 1, Batch  7600] loss: 4.661, acc: 48.10 %\n",
      "[Epoch 1, Batch  7700] loss: 4.647, acc: 48.29 %\n",
      "[Epoch 1, Batch  7800] loss: 4.631, acc: 48.28 %\n",
      "[Epoch 1, Batch  7900] loss: 4.636, acc: 48.41 %\n",
      "[Epoch 1, Batch  8000] loss: 4.631, acc: 48.53 %\n",
      "[Epoch 1, Batch  8100] loss: 4.613, acc: 49.09 %\n",
      "[Epoch 1, Batch  8200] loss: 4.594, acc: 49.17 %\n",
      "[Epoch 1, Batch  8300] loss: 4.593, acc: 49.18 %\n",
      "[Epoch 1, Batch  8400] loss: 4.564, acc: 49.38 %\n",
      "[Epoch 1, Batch  8500] loss: 4.568, acc: 49.53 %\n",
      "[Epoch 1, Batch  8600] loss: 4.565, acc: 49.68 %\n",
      "[Epoch 1, Batch  8700] loss: 4.556, acc: 49.70 %\n",
      "[Epoch 1, Batch  8800] loss: 4.544, acc: 49.74 %\n",
      "[Epoch 1, Batch  8900] loss: 4.528, acc: 49.77 %\n",
      "[Epoch 1, Batch  9000] loss: 4.544, acc: 49.72 %\n",
      "[Epoch 1, Batch  9100] loss: 4.521, acc: 50.14 %\n",
      "[Epoch 1, Batch  9200] loss: 4.505, acc: 50.52 %\n",
      "[Epoch 1, Batch  9300] loss: 4.489, acc: 50.52 %\n",
      "[Epoch 1, Batch  9400] loss: 4.494, acc: 50.46 %\n",
      "[Epoch 1, Batch  9500] loss: 4.468, acc: 50.56 %\n",
      "[Epoch 1, Batch  9600] loss: 4.479, acc: 50.68 %\n",
      "[Epoch 1, Batch  9700] loss: 4.454, acc: 50.78 %\n",
      "[Epoch 1, Batch  9800] loss: 4.443, acc: 50.73 %\n",
      "[Epoch 1, Batch  9900] loss: 4.426, acc: 50.90 %\n",
      "[Epoch 1, Batch 10000] loss: 4.434, acc: 50.95 %\n",
      "[Epoch 1, Batch 10100] loss: 4.418, acc: 52.52 %\n",
      "[Epoch 1, Batch 10200] loss: 4.413, acc: 51.62 %\n",
      "[Epoch 1, Batch 10300] loss: 4.402, acc: 51.74 %\n",
      "[Epoch 1, Batch 10400] loss: 4.379, acc: 51.92 %\n",
      "[Epoch 1, Batch 10500] loss: 4.372, acc: 51.99 %\n",
      "[Epoch 1, Batch 10600] loss: 4.373, acc: 51.94 %\n",
      "[Epoch 1, Batch 10700] loss: 4.365, acc: 52.00 %\n",
      "[Epoch 1, Batch 10800] loss: 4.370, acc: 51.82 %\n",
      "[Epoch 1, Batch 10900] loss: 4.354, acc: 51.87 %\n",
      "[Epoch 1, Batch 11000] loss: 4.337, acc: 51.97 %\n",
      "[Epoch 1, Batch 11100] loss: 4.316, acc: 54.31 %\n",
      "[Epoch 1, Batch 11200] loss: 4.328, acc: 53.57 %\n",
      "[Epoch 1, Batch 11300] loss: 4.320, acc: 53.64 %\n",
      "[Epoch 1, Batch 11400] loss: 4.313, acc: 53.39 %\n",
      "[Epoch 1, Batch 11500] loss: 4.287, acc: 53.36 %\n",
      "[Epoch 1, Batch 11600] loss: 4.273, acc: 53.43 %\n",
      "[Epoch 1, Batch 11700] loss: 4.276, acc: 53.36 %\n",
      "[Epoch 1, Batch 11800] loss: 4.257, acc: 53.59 %\n",
      "[Epoch 1, Batch 11900] loss: 4.274, acc: 53.48 %\n",
      "[Epoch 1, Batch 12000] loss: 4.246, acc: 53.50 %\n",
      "[Epoch 1, Batch 12100] loss: 4.233, acc: 54.19 %\n",
      "[Epoch 1, Batch 12200] loss: 4.237, acc: 54.38 %\n",
      "[Epoch 1, Batch 12300] loss: 4.226, acc: 54.29 %\n",
      "[Epoch 1, Batch 12400] loss: 4.202, acc: 54.48 %\n",
      "[Epoch 1, Batch 12500] loss: 4.197, acc: 54.70 %\n",
      "[Epoch 1, Batch 12600] loss: 4.196, acc: 54.67 %\n",
      "[Epoch 1, Batch 12700] loss: 4.192, acc: 54.60 %\n",
      "[Epoch 1, Batch 12800] loss: 4.167, acc: 54.69 %\n",
      "[Epoch 1, Batch 12900] loss: 4.156, acc: 54.77 %\n",
      "[Epoch 1, Batch 13000] loss: 4.156, acc: 54.80 %\n",
      "[Epoch 1, Batch 13100] loss: 4.151, acc: 56.97 %\n",
      "[Epoch 1, Batch 13200] loss: 4.141, acc: 55.81 %\n",
      "[Epoch 1, Batch 13300] loss: 4.134, acc: 55.47 %\n",
      "[Epoch 1, Batch 13400] loss: 4.128, acc: 55.46 %\n",
      "[Epoch 1, Batch 13500] loss: 4.113, acc: 55.10 %\n",
      "[Epoch 1, Batch 13600] loss: 4.104, acc: 55.22 %\n",
      "[Epoch 1, Batch 13700] loss: 4.090, acc: 55.37 %\n",
      "[Epoch 1, Batch 13800] loss: 4.081, acc: 55.39 %\n",
      "[Epoch 1, Batch 13900] loss: 4.091, acc: 55.44 %\n",
      "[Epoch 1, Batch 14000] loss: 4.067, acc: 55.45 %\n",
      "[Epoch 1, Batch 14100] loss: 4.050, acc: 56.80 %\n",
      "[Epoch 1, Batch 14200] loss: 4.044, acc: 56.57 %\n",
      "[Epoch 1, Batch 14300] loss: 4.050, acc: 56.38 %\n",
      "[Epoch 1, Batch 14400] loss: 4.046, acc: 56.32 %\n",
      "[Epoch 1, Batch 14500] loss: 4.024, acc: 56.46 %\n",
      "[Epoch 1, Batch 14600] loss: 4.024, acc: 56.65 %\n",
      "[Epoch 1, Batch 14700] loss: 3.990, acc: 56.77 %\n",
      "[Epoch 1, Batch 14800] loss: 3.994, acc: 56.73 %\n",
      "[Epoch 1, Batch 14900] loss: 3.996, acc: 56.58 %\n",
      "[Epoch 1, Batch 15000] loss: 3.986, acc: 56.61 %\n",
      "[Epoch 1, Batch 15100] loss: 3.968, acc: 56.45 %\n",
      "[Epoch 1, Batch 15200] loss: 3.965, acc: 56.27 %\n",
      "[Epoch 1, Batch 15300] loss: 3.962, acc: 56.70 %\n",
      "[Epoch 1, Batch 15400] loss: 3.957, acc: 56.56 %\n",
      "[Epoch 1, Batch 15500] loss: 3.972, acc: 56.40 %\n",
      "[Epoch 1, Batch 15600] loss: 3.934, acc: 56.26 %\n",
      "[Epoch 1, Batch 15700] loss: 3.938, acc: 56.25 %\n",
      "[Epoch 1, Batch 15800] loss: 3.926, acc: 56.33 %\n",
      "[Epoch 1, Batch 15900] loss: 3.903, acc: 56.49 %\n",
      "[Epoch 1, Batch 16000] loss: 3.911, acc: 56.60 %\n",
      "[Epoch 1, Batch 16100] loss: 3.881, acc: 57.28 %\n",
      "[Epoch 1, Batch 16200] loss: 3.880, acc: 58.13 %\n",
      "[Epoch 1, Batch 16300] loss: 3.892, acc: 57.95 %\n",
      "[Epoch 1, Batch 16400] loss: 3.859, acc: 57.90 %\n",
      "[Epoch 1, Batch 16500] loss: 3.867, acc: 57.98 %\n",
      "[Epoch 1, Batch 16600] loss: 3.858, acc: 57.86 %\n",
      "[Epoch 1, Batch 16700] loss: 3.838, acc: 57.83 %\n",
      "[Epoch 1, Batch 16800] loss: 3.825, acc: 58.04 %\n",
      "[Epoch 1, Batch 16900] loss: 3.814, acc: 58.10 %\n",
      "[Epoch 1, Batch 17000] loss: 3.812, acc: 58.26 %\n",
      "[Epoch 1, Batch 17100] loss: 3.817, acc: 58.59 %\n",
      "[Epoch 1, Batch 17200] loss: 3.794, acc: 58.87 %\n",
      "[Epoch 1, Batch 17300] loss: 3.806, acc: 58.70 %\n",
      "[Epoch 1, Batch 17400] loss: 3.779, acc: 58.66 %\n",
      "[Epoch 1, Batch 17500] loss: 3.771, acc: 58.93 %\n",
      "[Epoch 1, Batch 17600] loss: 3.768, acc: 58.78 %\n",
      "[Epoch 1, Batch 17700] loss: 3.746, acc: 59.00 %\n",
      "[Epoch 1, Batch 17800] loss: 3.751, acc: 59.06 %\n",
      "[Epoch 1, Batch 17900] loss: 3.731, acc: 59.13 %\n",
      "[Epoch 1, Batch 18000] loss: 3.731, acc: 59.10 %\n",
      "[Epoch 1, Batch 18100] loss: 3.716, acc: 60.16 %\n",
      "[Epoch 1, Batch 18200] loss: 3.721, acc: 59.54 %\n",
      "[Epoch 1, Batch 18300] loss: 3.714, acc: 59.24 %\n",
      "[Epoch 1, Batch 18400] loss: 3.709, acc: 59.19 %\n",
      "[Epoch 1, Batch 18500] loss: 3.716, acc: 58.92 %\n",
      "[Epoch 1, Batch 18600] loss: 3.696, acc: 58.94 %\n",
      "[Epoch 1, Batch 18700] loss: 3.690, acc: 58.98 %\n",
      "[Epoch 1, Batch 18800] loss: 3.669, acc: 59.04 %\n",
      "[Epoch 1, Batch 18900] loss: 3.667, acc: 59.20 %\n",
      "[Epoch 1, Batch 19000] loss: 3.637, acc: 59.28 %\n",
      "[Epoch 1, Batch 19100] loss: 3.649, acc: 59.44 %\n",
      "[Epoch 1, Batch 19200] loss: 3.640, acc: 59.70 %\n",
      "[Epoch 1, Batch 19300] loss: 3.639, acc: 59.67 %\n",
      "[Epoch 1, Batch 19400] loss: 3.635, acc: 59.49 %\n",
      "[Epoch 1, Batch 19500] loss: 3.621, acc: 59.39 %\n",
      "[Epoch 1, Batch 19600] loss: 3.605, acc: 59.57 %\n",
      "[Epoch 1, Batch 19700] loss: 3.591, acc: 59.81 %\n",
      "[Epoch 1, Batch 19800] loss: 3.589, acc: 59.84 %\n",
      "[Epoch 1, Batch 19900] loss: 3.594, acc: 59.85 %\n",
      "[Epoch 1, Batch 20000] loss: 3.586, acc: 59.84 %\n",
      "****** Model checkpoint saved at epochs 2 ******\n",
      "epoch 3 learning rate : 2.4e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c60fb2b9ac4e7f8550d59c25d13e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2, Batch   100] loss: 3.578, acc: 60.58 %\n",
      "[Epoch 2, Batch   200] loss: 3.563, acc: 60.27 %\n",
      "[Epoch 2, Batch   300] loss: 3.544, acc: 60.54 %\n",
      "[Epoch 2, Batch   400] loss: 3.532, acc: 60.79 %\n",
      "[Epoch 2, Batch   500] loss: 3.516, acc: 60.94 %\n",
      "[Epoch 2, Batch   600] loss: 3.493, acc: 61.16 %\n",
      "[Epoch 2, Batch   700] loss: 3.510, acc: 61.02 %\n",
      "[Epoch 2, Batch   800] loss: 3.482, acc: 61.04 %\n",
      "[Epoch 2, Batch   900] loss: 3.474, acc: 61.03 %\n",
      "[Epoch 2, Batch  1000] loss: 3.446, acc: 61.10 %\n",
      "[Epoch 2, Batch  1100] loss: 3.469, acc: 60.36 %\n",
      "[Epoch 2, Batch  1200] loss: 3.440, acc: 60.36 %\n",
      "[Epoch 2, Batch  1300] loss: 3.434, acc: 60.71 %\n",
      "[Epoch 2, Batch  1400] loss: 3.417, acc: 60.87 %\n",
      "[Epoch 2, Batch  1500] loss: 3.396, acc: 61.06 %\n",
      "[Epoch 2, Batch  1600] loss: 3.413, acc: 60.96 %\n",
      "[Epoch 2, Batch  1700] loss: 3.386, acc: 61.14 %\n",
      "[Epoch 2, Batch  1800] loss: 3.361, acc: 61.26 %\n",
      "[Epoch 2, Batch  1900] loss: 3.383, acc: 61.21 %\n",
      "[Epoch 2, Batch  2000] loss: 3.351, acc: 61.27 %\n",
      "[Epoch 2, Batch  2100] loss: 3.357, acc: 62.02 %\n",
      "[Epoch 2, Batch  2200] loss: 3.339, acc: 62.32 %\n",
      "[Epoch 2, Batch  2300] loss: 3.334, acc: 62.40 %\n",
      "[Epoch 2, Batch  2400] loss: 3.311, acc: 62.38 %\n",
      "[Epoch 2, Batch  2500] loss: 3.298, acc: 62.32 %\n",
      "[Epoch 2, Batch  2600] loss: 3.296, acc: 62.20 %\n",
      "[Epoch 2, Batch  2700] loss: 3.286, acc: 62.24 %\n",
      "[Epoch 2, Batch  2800] loss: 3.259, acc: 62.28 %\n",
      "[Epoch 2, Batch  2900] loss: 3.254, acc: 62.26 %\n",
      "[Epoch 2, Batch  3000] loss: 3.252, acc: 62.32 %\n",
      "[Epoch 2, Batch  3100] loss: 3.222, acc: 62.59 %\n",
      "[Epoch 2, Batch  3200] loss: 3.237, acc: 62.38 %\n",
      "[Epoch 2, Batch  3300] loss: 3.220, acc: 62.89 %\n",
      "[Epoch 2, Batch  3400] loss: 3.196, acc: 62.97 %\n",
      "[Epoch 2, Batch  3500] loss: 3.195, acc: 63.02 %\n",
      "[Epoch 2, Batch  3600] loss: 3.192, acc: 63.08 %\n",
      "[Epoch 2, Batch  3700] loss: 3.196, acc: 63.11 %\n",
      "[Epoch 2, Batch  3800] loss: 3.154, acc: 63.08 %\n",
      "[Epoch 2, Batch  3900] loss: 3.164, acc: 63.14 %\n",
      "[Epoch 2, Batch  4000] loss: 3.133, acc: 63.15 %\n",
      "[Epoch 2, Batch  4100] loss: 3.136, acc: 62.69 %\n",
      "[Epoch 2, Batch  4200] loss: 3.121, acc: 62.99 %\n",
      "[Epoch 2, Batch  4300] loss: 3.107, acc: 63.19 %\n",
      "[Epoch 2, Batch  4400] loss: 3.088, acc: 63.56 %\n",
      "[Epoch 2, Batch  4500] loss: 3.091, acc: 63.60 %\n",
      "[Epoch 2, Batch  4600] loss: 3.083, acc: 63.61 %\n",
      "[Epoch 2, Batch  4700] loss: 3.075, acc: 63.59 %\n",
      "[Epoch 2, Batch  4800] loss: 3.068, acc: 63.60 %\n",
      "[Epoch 2, Batch  4900] loss: 3.069, acc: 63.68 %\n",
      "[Epoch 2, Batch  5000] loss: 3.034, acc: 63.73 %\n",
      "[Epoch 2, Batch  5100] loss: 3.032, acc: 64.38 %\n",
      "[Epoch 2, Batch  5200] loss: 3.040, acc: 64.40 %\n",
      "[Epoch 2, Batch  5300] loss: 2.999, acc: 64.42 %\n",
      "[Epoch 2, Batch  5400] loss: 2.985, acc: 64.54 %\n",
      "[Epoch 2, Batch  5500] loss: 3.003, acc: 64.57 %\n",
      "[Epoch 2, Batch  5600] loss: 2.991, acc: 64.58 %\n",
      "[Epoch 2, Batch  5700] loss: 2.950, acc: 64.73 %\n",
      "[Epoch 2, Batch  5800] loss: 2.942, acc: 64.84 %\n",
      "[Epoch 2, Batch  5900] loss: 2.930, acc: 64.86 %\n",
      "[Epoch 2, Batch  6000] loss: 2.920, acc: 64.95 %\n",
      "[Epoch 2, Batch  6100] loss: 2.940, acc: 65.53 %\n",
      "[Epoch 2, Batch  6200] loss: 2.914, acc: 65.95 %\n",
      "[Epoch 2, Batch  6300] loss: 2.897, acc: 65.93 %\n",
      "[Epoch 2, Batch  6400] loss: 2.906, acc: 65.75 %\n",
      "[Epoch 2, Batch  6500] loss: 2.895, acc: 65.52 %\n",
      "[Epoch 2, Batch  6600] loss: 2.894, acc: 65.53 %\n",
      "[Epoch 2, Batch  6700] loss: 2.880, acc: 65.50 %\n",
      "[Epoch 2, Batch  6800] loss: 2.876, acc: 65.51 %\n",
      "[Epoch 2, Batch  6900] loss: 2.870, acc: 65.47 %\n",
      "[Epoch 2, Batch  7000] loss: 2.866, acc: 65.36 %\n",
      "[Epoch 2, Batch  7100] loss: 2.825, acc: 65.75 %\n",
      "[Epoch 2, Batch  7200] loss: 2.838, acc: 65.59 %\n",
      "[Epoch 2, Batch  7300] loss: 2.821, acc: 65.73 %\n",
      "[Epoch 2, Batch  7400] loss: 2.812, acc: 65.84 %\n",
      "[Epoch 2, Batch  7500] loss: 2.792, acc: 65.99 %\n",
      "[Epoch 2, Batch  7600] loss: 2.799, acc: 66.07 %\n",
      "[Epoch 2, Batch  7700] loss: 2.778, acc: 66.21 %\n",
      "[Epoch 2, Batch  7800] loss: 2.807, acc: 66.20 %\n",
      "[Epoch 2, Batch  7900] loss: 2.758, acc: 66.18 %\n",
      "[Epoch 2, Batch  8000] loss: 2.738, acc: 66.24 %\n",
      "[Epoch 2, Batch  8100] loss: 2.741, acc: 66.59 %\n",
      "[Epoch 2, Batch  8200] loss: 2.743, acc: 66.95 %\n",
      "[Epoch 2, Batch  8300] loss: 2.736, acc: 67.06 %\n",
      "[Epoch 2, Batch  8400] loss: 2.718, acc: 67.15 %\n",
      "[Epoch 2, Batch  8500] loss: 2.726, acc: 67.19 %\n",
      "[Epoch 2, Batch  8600] loss: 2.699, acc: 67.22 %\n",
      "[Epoch 2, Batch  8700] loss: 2.706, acc: 67.23 %\n",
      "[Epoch 2, Batch  8800] loss: 2.680, acc: 67.26 %\n",
      "[Epoch 2, Batch  8900] loss: 2.685, acc: 67.22 %\n",
      "[Epoch 2, Batch  9000] loss: 2.666, acc: 67.21 %\n",
      "[Epoch 2, Batch  9100] loss: 2.649, acc: 67.44 %\n",
      "[Epoch 2, Batch  9200] loss: 2.669, acc: 66.87 %\n",
      "[Epoch 2, Batch  9300] loss: 2.630, acc: 67.23 %\n",
      "[Epoch 2, Batch  9400] loss: 2.628, acc: 67.20 %\n",
      "[Epoch 2, Batch  9500] loss: 2.651, acc: 67.07 %\n",
      "[Epoch 2, Batch  9600] loss: 2.589, acc: 67.29 %\n",
      "[Epoch 2, Batch  9700] loss: 2.615, acc: 67.29 %\n",
      "[Epoch 2, Batch  9800] loss: 2.587, acc: 67.48 %\n",
      "[Epoch 2, Batch  9900] loss: 2.583, acc: 67.55 %\n",
      "[Epoch 2, Batch 10000] loss: 2.557, acc: 67.65 %\n",
      "[Epoch 2, Batch 10100] loss: 2.563, acc: 67.98 %\n",
      "[Epoch 2, Batch 10200] loss: 2.587, acc: 67.80 %\n",
      "[Epoch 2, Batch 10300] loss: 2.567, acc: 67.83 %\n",
      "[Epoch 2, Batch 10400] loss: 2.549, acc: 67.91 %\n",
      "[Epoch 2, Batch 10500] loss: 2.549, acc: 67.81 %\n",
      "[Epoch 2, Batch 10600] loss: 2.548, acc: 67.83 %\n",
      "[Epoch 2, Batch 10700] loss: 2.505, acc: 68.01 %\n",
      "[Epoch 2, Batch 10800] loss: 2.517, acc: 68.06 %\n",
      "[Epoch 2, Batch 10900] loss: 2.491, acc: 68.12 %\n",
      "[Epoch 2, Batch 11000] loss: 2.512, acc: 68.10 %\n",
      "[Epoch 2, Batch 11100] loss: 2.487, acc: 68.58 %\n",
      "[Epoch 2, Batch 11200] loss: 2.478, acc: 68.79 %\n",
      "[Epoch 2, Batch 11300] loss: 2.478, acc: 68.74 %\n",
      "[Epoch 2, Batch 11400] loss: 2.455, acc: 68.82 %\n",
      "[Epoch 2, Batch 11500] loss: 2.477, acc: 68.84 %\n",
      "[Epoch 2, Batch 11600] loss: 2.443, acc: 68.93 %\n",
      "[Epoch 2, Batch 11700] loss: 2.435, acc: 68.97 %\n",
      "[Epoch 2, Batch 11800] loss: 2.429, acc: 68.96 %\n",
      "[Epoch 2, Batch 11900] loss: 2.407, acc: 69.01 %\n",
      "[Epoch 2, Batch 12000] loss: 2.439, acc: 69.03 %\n",
      "[Epoch 2, Batch 12100] loss: 2.433, acc: 69.22 %\n",
      "[Epoch 2, Batch 12200] loss: 2.404, acc: 69.09 %\n",
      "[Epoch 2, Batch 12300] loss: 2.385, acc: 69.07 %\n",
      "[Epoch 2, Batch 12400] loss: 2.386, acc: 69.33 %\n",
      "[Epoch 2, Batch 12500] loss: 2.366, acc: 69.55 %\n",
      "[Epoch 2, Batch 12600] loss: 2.382, acc: 69.44 %\n",
      "[Epoch 2, Batch 12700] loss: 2.336, acc: 69.52 %\n",
      "[Epoch 2, Batch 12800] loss: 2.354, acc: 69.60 %\n",
      "[Epoch 2, Batch 12900] loss: 2.325, acc: 69.63 %\n",
      "[Epoch 2, Batch 13000] loss: 2.350, acc: 69.61 %\n",
      "[Epoch 2, Batch 13100] loss: 2.333, acc: 69.73 %\n",
      "[Epoch 2, Batch 13200] loss: 2.335, acc: 69.34 %\n",
      "[Epoch 2, Batch 13300] loss: 2.339, acc: 69.38 %\n",
      "[Epoch 2, Batch 13400] loss: 2.303, acc: 69.69 %\n",
      "[Epoch 2, Batch 13500] loss: 2.291, acc: 69.57 %\n",
      "[Epoch 2, Batch 13600] loss: 2.307, acc: 69.70 %\n",
      "[Epoch 2, Batch 13700] loss: 2.256, acc: 69.90 %\n",
      "[Epoch 2, Batch 13800] loss: 2.324, acc: 69.80 %\n",
      "[Epoch 2, Batch 13900] loss: 2.295, acc: 69.82 %\n",
      "[Epoch 2, Batch 14000] loss: 2.274, acc: 69.86 %\n",
      "[Epoch 2, Batch 14100] loss: 2.243, acc: 69.81 %\n",
      "[Epoch 2, Batch 14200] loss: 2.280, acc: 69.60 %\n",
      "[Epoch 2, Batch 14300] loss: 2.230, acc: 69.99 %\n",
      "[Epoch 2, Batch 14400] loss: 2.219, acc: 70.09 %\n",
      "[Epoch 2, Batch 14500] loss: 2.221, acc: 70.22 %\n",
      "[Epoch 2, Batch 14600] loss: 2.209, acc: 70.31 %\n",
      "[Epoch 2, Batch 14700] loss: 2.212, acc: 70.40 %\n",
      "[Epoch 2, Batch 14800] loss: 2.222, acc: 70.37 %\n",
      "[Epoch 2, Batch 14900] loss: 2.192, acc: 70.39 %\n",
      "[Epoch 2, Batch 15000] loss: 2.216, acc: 70.34 %\n",
      "[Epoch 2, Batch 15100] loss: 2.199, acc: 71.06 %\n",
      "[Epoch 2, Batch 15200] loss: 2.189, acc: 70.79 %\n",
      "[Epoch 2, Batch 15300] loss: 2.166, acc: 70.72 %\n",
      "[Epoch 2, Batch 15400] loss: 2.182, acc: 70.56 %\n",
      "[Epoch 2, Batch 15500] loss: 2.185, acc: 70.55 %\n",
      "[Epoch 2, Batch 15600] loss: 2.150, acc: 70.49 %\n",
      "[Epoch 2, Batch 15700] loss: 2.152, acc: 70.54 %\n",
      "[Epoch 2, Batch 15800] loss: 2.109, acc: 70.60 %\n",
      "[Epoch 2, Batch 15900] loss: 2.146, acc: 70.65 %\n",
      "[Epoch 2, Batch 16000] loss: 2.145, acc: 70.72 %\n",
      "[Epoch 2, Batch 16100] loss: 2.105, acc: 71.38 %\n",
      "[Epoch 2, Batch 16200] loss: 2.116, acc: 71.38 %\n",
      "[Epoch 2, Batch 16300] loss: 2.077, acc: 71.63 %\n",
      "[Epoch 2, Batch 16400] loss: 2.078, acc: 71.60 %\n",
      "[Epoch 2, Batch 16500] loss: 2.098, acc: 71.53 %\n",
      "[Epoch 2, Batch 16600] loss: 2.101, acc: 71.56 %\n",
      "[Epoch 2, Batch 16700] loss: 2.089, acc: 71.51 %\n",
      "[Epoch 2, Batch 16800] loss: 2.092, acc: 71.47 %\n",
      "[Epoch 2, Batch 16900] loss: 2.078, acc: 71.43 %\n",
      "[Epoch 2, Batch 17000] loss: 2.058, acc: 71.46 %\n",
      "[Epoch 2, Batch 17100] loss: 2.058, acc: 71.36 %\n",
      "[Epoch 2, Batch 17200] loss: 2.023, acc: 72.05 %\n",
      "[Epoch 2, Batch 17300] loss: 2.032, acc: 72.21 %\n",
      "[Epoch 2, Batch 17400] loss: 2.013, acc: 72.16 %\n",
      "[Epoch 2, Batch 17500] loss: 2.039, acc: 72.03 %\n",
      "[Epoch 2, Batch 17600] loss: 2.032, acc: 72.00 %\n",
      "[Epoch 2, Batch 17700] loss: 2.021, acc: 71.93 %\n",
      "[Epoch 2, Batch 17800] loss: 2.042, acc: 71.74 %\n",
      "[Epoch 2, Batch 17900] loss: 2.012, acc: 71.70 %\n",
      "[Epoch 2, Batch 18000] loss: 2.010, acc: 71.72 %\n",
      "[Epoch 2, Batch 18100] loss: 1.981, acc: 72.56 %\n",
      "[Epoch 2, Batch 18200] loss: 1.996, acc: 72.12 %\n",
      "[Epoch 2, Batch 18300] loss: 1.992, acc: 72.24 %\n",
      "[Epoch 2, Batch 18400] loss: 1.974, acc: 72.29 %\n",
      "[Epoch 2, Batch 18500] loss: 1.985, acc: 72.22 %\n",
      "[Epoch 2, Batch 18600] loss: 1.978, acc: 72.11 %\n",
      "[Epoch 2, Batch 18700] loss: 1.972, acc: 72.05 %\n",
      "[Epoch 2, Batch 18800] loss: 1.959, acc: 72.12 %\n",
      "[Epoch 2, Batch 18900] loss: 1.970, acc: 72.10 %\n",
      "[Epoch 2, Batch 19000] loss: 1.939, acc: 72.07 %\n",
      "[Epoch 2, Batch 19100] loss: 1.934, acc: 71.83 %\n",
      "[Epoch 2, Batch 19200] loss: 1.953, acc: 71.59 %\n",
      "[Epoch 2, Batch 19300] loss: 1.924, acc: 71.92 %\n",
      "[Epoch 2, Batch 19400] loss: 1.946, acc: 71.90 %\n",
      "[Epoch 2, Batch 19500] loss: 1.941, acc: 71.92 %\n",
      "[Epoch 2, Batch 19600] loss: 1.883, acc: 72.15 %\n",
      "[Epoch 2, Batch 19700] loss: 1.907, acc: 72.15 %\n",
      "[Epoch 2, Batch 19800] loss: 1.912, acc: 72.17 %\n",
      "[Epoch 2, Batch 19900] loss: 1.911, acc: 72.23 %\n",
      "[Epoch 2, Batch 20000] loss: 1.898, acc: 72.20 %\n",
      "****** Model checkpoint saved at epochs 3 ******\n",
      "epoch 4 learning rate : 3.2e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3769538863664df090ecb087000e5aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3, Batch   100] loss: 1.866, acc: 72.92 %\n",
      "[Epoch 3, Batch   200] loss: 1.857, acc: 73.34 %\n",
      "[Epoch 3, Batch   300] loss: 1.882, acc: 72.97 %\n",
      "[Epoch 3, Batch   400] loss: 1.853, acc: 73.02 %\n",
      "[Epoch 3, Batch   500] loss: 1.867, acc: 72.99 %\n",
      "[Epoch 3, Batch   600] loss: 1.819, acc: 72.89 %\n",
      "[Epoch 3, Batch   700] loss: 1.831, acc: 72.85 %\n",
      "[Epoch 3, Batch   800] loss: 1.849, acc: 72.83 %\n",
      "[Epoch 3, Batch   900] loss: 1.836, acc: 72.97 %\n",
      "[Epoch 3, Batch  1000] loss: 1.842, acc: 72.90 %\n",
      "[Epoch 3, Batch  1100] loss: 1.829, acc: 72.69 %\n",
      "[Epoch 3, Batch  1200] loss: 1.808, acc: 73.03 %\n",
      "[Epoch 3, Batch  1300] loss: 1.817, acc: 72.88 %\n",
      "[Epoch 3, Batch  1400] loss: 1.820, acc: 72.73 %\n",
      "[Epoch 3, Batch  1500] loss: 1.840, acc: 72.59 %\n",
      "[Epoch 3, Batch  1600] loss: 1.788, acc: 72.73 %\n",
      "[Epoch 3, Batch  1700] loss: 1.765, acc: 72.88 %\n",
      "[Epoch 3, Batch  1800] loss: 1.766, acc: 73.01 %\n",
      "[Epoch 3, Batch  1900] loss: 1.798, acc: 73.04 %\n",
      "[Epoch 3, Batch  2000] loss: 1.797, acc: 73.03 %\n",
      "[Epoch 3, Batch  2100] loss: 1.788, acc: 72.97 %\n",
      "[Epoch 3, Batch  2200] loss: 1.772, acc: 72.86 %\n",
      "[Epoch 3, Batch  2300] loss: 1.733, acc: 73.10 %\n",
      "[Epoch 3, Batch  2400] loss: 1.747, acc: 73.18 %\n",
      "[Epoch 3, Batch  2500] loss: 1.700, acc: 73.47 %\n",
      "[Epoch 3, Batch  2600] loss: 1.716, acc: 73.49 %\n",
      "[Epoch 3, Batch  2700] loss: 1.716, acc: 73.60 %\n",
      "[Epoch 3, Batch  2800] loss: 1.717, acc: 73.54 %\n",
      "[Epoch 3, Batch  2900] loss: 1.707, acc: 73.54 %\n",
      "[Epoch 3, Batch  3000] loss: 1.691, acc: 73.60 %\n",
      "[Epoch 3, Batch  3100] loss: 1.699, acc: 73.95 %\n",
      "[Epoch 3, Batch  3200] loss: 1.694, acc: 73.63 %\n",
      "[Epoch 3, Batch  3300] loss: 1.703, acc: 73.58 %\n",
      "[Epoch 3, Batch  3400] loss: 1.680, acc: 73.62 %\n",
      "[Epoch 3, Batch  3500] loss: 1.668, acc: 73.64 %\n",
      "[Epoch 3, Batch  3600] loss: 1.660, acc: 73.71 %\n",
      "[Epoch 3, Batch  3700] loss: 1.695, acc: 73.68 %\n",
      "[Epoch 3, Batch  3800] loss: 1.635, acc: 73.80 %\n",
      "[Epoch 3, Batch  3900] loss: 1.673, acc: 73.83 %\n",
      "[Epoch 3, Batch  4000] loss: 1.666, acc: 73.83 %\n",
      "[Epoch 3, Batch  4100] loss: 1.641, acc: 74.55 %\n",
      "[Epoch 3, Batch  4200] loss: 1.666, acc: 73.97 %\n",
      "[Epoch 3, Batch  4300] loss: 1.617, acc: 74.40 %\n",
      "[Epoch 3, Batch  4400] loss: 1.657, acc: 74.18 %\n",
      "[Epoch 3, Batch  4500] loss: 1.656, acc: 73.82 %\n",
      "[Epoch 3, Batch  4600] loss: 1.599, acc: 73.91 %\n",
      "[Epoch 3, Batch  4700] loss: 1.638, acc: 73.91 %\n",
      "[Epoch 3, Batch  4800] loss: 1.605, acc: 73.95 %\n",
      "[Epoch 3, Batch  4900] loss: 1.649, acc: 73.86 %\n",
      "[Epoch 3, Batch  5000] loss: 1.594, acc: 73.94 %\n",
      "[Epoch 3, Batch  5100] loss: 1.616, acc: 72.89 %\n",
      "[Epoch 3, Batch  5200] loss: 1.578, acc: 73.50 %\n",
      "[Epoch 3, Batch  5300] loss: 1.585, acc: 73.93 %\n",
      "[Epoch 3, Batch  5400] loss: 1.578, acc: 74.03 %\n",
      "[Epoch 3, Batch  5500] loss: 1.593, acc: 73.98 %\n",
      "[Epoch 3, Batch  5600] loss: 1.600, acc: 73.99 %\n",
      "[Epoch 3, Batch  5700] loss: 1.597, acc: 74.04 %\n",
      "[Epoch 3, Batch  5800] loss: 1.547, acc: 74.14 %\n",
      "[Epoch 3, Batch  5900] loss: 1.544, acc: 74.24 %\n",
      "[Epoch 3, Batch  6000] loss: 1.545, acc: 74.36 %\n",
      "[Epoch 3, Batch  6100] loss: 1.577, acc: 74.25 %\n",
      "[Epoch 3, Batch  6200] loss: 1.558, acc: 73.99 %\n",
      "[Epoch 3, Batch  6300] loss: 1.557, acc: 74.20 %\n",
      "[Epoch 3, Batch  6400] loss: 1.569, acc: 74.14 %\n",
      "[Epoch 3, Batch  6500] loss: 1.565, acc: 74.23 %\n",
      "[Epoch 3, Batch  6600] loss: 1.520, acc: 74.38 %\n",
      "[Epoch 3, Batch  6700] loss: 1.540, acc: 74.36 %\n",
      "[Epoch 3, Batch  6800] loss: 1.521, acc: 74.38 %\n",
      "[Epoch 3, Batch  6900] loss: 1.472, acc: 74.60 %\n",
      "[Epoch 3, Batch  7000] loss: 1.552, acc: 74.49 %\n",
      "[Epoch 3, Batch  7100] loss: 1.545, acc: 74.69 %\n",
      "[Epoch 3, Batch  7200] loss: 1.483, acc: 74.96 %\n",
      "[Epoch 3, Batch  7300] loss: 1.553, acc: 74.51 %\n",
      "[Epoch 3, Batch  7400] loss: 1.517, acc: 74.50 %\n",
      "[Epoch 3, Batch  7500] loss: 1.512, acc: 74.51 %\n",
      "[Epoch 3, Batch  7600] loss: 1.488, acc: 74.66 %\n",
      "[Epoch 3, Batch  7700] loss: 1.494, acc: 74.79 %\n",
      "[Epoch 3, Batch  7800] loss: 1.477, acc: 74.87 %\n",
      "[Epoch 3, Batch  7900] loss: 1.502, acc: 74.85 %\n",
      "[Epoch 3, Batch  8000] loss: 1.462, acc: 74.87 %\n",
      "[Epoch 3, Batch  8100] loss: 1.452, acc: 75.73 %\n",
      "[Epoch 3, Batch  8200] loss: 1.476, acc: 75.56 %\n",
      "[Epoch 3, Batch  8300] loss: 1.481, acc: 75.39 %\n",
      "[Epoch 3, Batch  8400] loss: 1.470, acc: 75.43 %\n",
      "[Epoch 3, Batch  8500] loss: 1.473, acc: 75.43 %\n",
      "[Epoch 3, Batch  8600] loss: 1.478, acc: 75.27 %\n",
      "[Epoch 3, Batch  8700] loss: 1.482, acc: 75.15 %\n",
      "[Epoch 3, Batch  8800] loss: 1.474, acc: 75.19 %\n",
      "[Epoch 3, Batch  8900] loss: 1.456, acc: 75.17 %\n",
      "[Epoch 3, Batch  9000] loss: 1.465, acc: 75.18 %\n",
      "[Epoch 3, Batch  9100] loss: 1.454, acc: 75.00 %\n",
      "[Epoch 3, Batch  9200] loss: 1.434, acc: 75.34 %\n",
      "[Epoch 3, Batch  9300] loss: 1.443, acc: 74.99 %\n",
      "[Epoch 3, Batch  9400] loss: 1.466, acc: 74.77 %\n",
      "[Epoch 3, Batch  9500] loss: 1.391, acc: 75.12 %\n",
      "[Epoch 3, Batch  9600] loss: 1.415, acc: 75.29 %\n",
      "[Epoch 3, Batch  9700] loss: 1.432, acc: 75.30 %\n",
      "[Epoch 3, Batch  9800] loss: 1.410, acc: 75.21 %\n",
      "[Epoch 3, Batch  9900] loss: 1.415, acc: 75.27 %\n",
      "[Epoch 3, Batch 10000] loss: 1.403, acc: 75.29 %\n",
      "[Epoch 3, Batch 10100] loss: 1.426, acc: 75.33 %\n",
      "[Epoch 3, Batch 10200] loss: 1.388, acc: 75.50 %\n",
      "[Epoch 3, Batch 10300] loss: 1.401, acc: 75.61 %\n",
      "[Epoch 3, Batch 10400] loss: 1.392, acc: 75.64 %\n",
      "[Epoch 3, Batch 10500] loss: 1.429, acc: 75.46 %\n",
      "[Epoch 3, Batch 10600] loss: 1.410, acc: 75.41 %\n",
      "[Epoch 3, Batch 10700] loss: 1.407, acc: 75.43 %\n",
      "[Epoch 3, Batch 10800] loss: 1.381, acc: 75.46 %\n",
      "[Epoch 3, Batch 10900] loss: 1.366, acc: 75.57 %\n",
      "[Epoch 3, Batch 11000] loss: 1.391, acc: 75.63 %\n",
      "[Epoch 3, Batch 11100] loss: 1.419, acc: 74.20 %\n",
      "[Epoch 3, Batch 11200] loss: 1.378, acc: 75.06 %\n",
      "[Epoch 3, Batch 11300] loss: 1.351, acc: 75.53 %\n",
      "[Epoch 3, Batch 11400] loss: 1.361, acc: 75.57 %\n",
      "[Epoch 3, Batch 11500] loss: 1.320, acc: 75.79 %\n",
      "[Epoch 3, Batch 11600] loss: 1.363, acc: 75.74 %\n",
      "[Epoch 3, Batch 11700] loss: 1.339, acc: 75.96 %\n",
      "[Epoch 3, Batch 11800] loss: 1.341, acc: 75.97 %\n",
      "[Epoch 3, Batch 11900] loss: 1.377, acc: 75.95 %\n",
      "[Epoch 3, Batch 12000] loss: 1.340, acc: 75.99 %\n",
      "[Epoch 3, Batch 12100] loss: 1.361, acc: 75.25 %\n",
      "[Epoch 3, Batch 12200] loss: 1.377, acc: 75.16 %\n",
      "[Epoch 3, Batch 12300] loss: 1.351, acc: 75.55 %\n",
      "[Epoch 3, Batch 12400] loss: 1.334, acc: 75.58 %\n",
      "[Epoch 3, Batch 12500] loss: 1.352, acc: 75.67 %\n",
      "[Epoch 3, Batch 12600] loss: 1.330, acc: 75.80 %\n",
      "[Epoch 3, Batch 12700] loss: 1.370, acc: 75.77 %\n",
      "[Epoch 3, Batch 12800] loss: 1.354, acc: 75.80 %\n",
      "[Epoch 3, Batch 12900] loss: 1.286, acc: 75.96 %\n",
      "[Epoch 3, Batch 13000] loss: 1.341, acc: 75.98 %\n",
      "[Epoch 3, Batch 13100] loss: 1.350, acc: 75.34 %\n",
      "[Epoch 3, Batch 13200] loss: 1.315, acc: 75.71 %\n",
      "[Epoch 3, Batch 13300] loss: 1.293, acc: 76.09 %\n",
      "[Epoch 3, Batch 13400] loss: 1.346, acc: 76.17 %\n",
      "[Epoch 3, Batch 13500] loss: 1.256, acc: 76.35 %\n",
      "[Epoch 3, Batch 13600] loss: 1.300, acc: 76.43 %\n",
      "[Epoch 3, Batch 13700] loss: 1.324, acc: 76.45 %\n",
      "[Epoch 3, Batch 13800] loss: 1.295, acc: 76.51 %\n",
      "[Epoch 3, Batch 13900] loss: 1.292, acc: 76.58 %\n",
      "[Epoch 3, Batch 14000] loss: 1.276, acc: 76.64 %\n",
      "[Epoch 3, Batch 14100] loss: 1.287, acc: 76.25 %\n",
      "[Epoch 3, Batch 14200] loss: 1.308, acc: 76.31 %\n",
      "[Epoch 3, Batch 14300] loss: 1.318, acc: 76.15 %\n",
      "[Epoch 3, Batch 14400] loss: 1.273, acc: 76.33 %\n",
      "[Epoch 3, Batch 14500] loss: 1.305, acc: 76.30 %\n",
      "[Epoch 3, Batch 14600] loss: 1.295, acc: 76.43 %\n",
      "[Epoch 3, Batch 14700] loss: 1.343, acc: 76.31 %\n",
      "[Epoch 3, Batch 14800] loss: 1.289, acc: 76.32 %\n",
      "[Epoch 3, Batch 14900] loss: 1.280, acc: 76.30 %\n",
      "[Epoch 3, Batch 15000] loss: 1.298, acc: 76.31 %\n",
      "[Epoch 3, Batch 15100] loss: 1.269, acc: 77.25 %\n",
      "[Epoch 3, Batch 15200] loss: 1.285, acc: 76.98 %\n",
      "[Epoch 3, Batch 15300] loss: 1.251, acc: 76.70 %\n",
      "[Epoch 3, Batch 15400] loss: 1.284, acc: 76.61 %\n",
      "[Epoch 3, Batch 15500] loss: 1.277, acc: 76.56 %\n",
      "[Epoch 3, Batch 15600] loss: 1.306, acc: 76.43 %\n",
      "[Epoch 3, Batch 15700] loss: 1.271, acc: 76.44 %\n",
      "[Epoch 3, Batch 15800] loss: 1.304, acc: 76.31 %\n",
      "[Epoch 3, Batch 15900] loss: 1.319, acc: 76.12 %\n",
      "[Epoch 3, Batch 16000] loss: 1.271, acc: 76.11 %\n",
      "[Epoch 3, Batch 16100] loss: 1.264, acc: 76.25 %\n",
      "[Epoch 3, Batch 16200] loss: 1.276, acc: 76.29 %\n",
      "[Epoch 3, Batch 16300] loss: 1.230, acc: 76.49 %\n",
      "[Epoch 3, Batch 16400] loss: 1.279, acc: 76.40 %\n",
      "[Epoch 3, Batch 16500] loss: 1.280, acc: 76.43 %\n",
      "[Epoch 3, Batch 16600] loss: 1.252, acc: 76.42 %\n",
      "[Epoch 3, Batch 16700] loss: 1.222, acc: 76.48 %\n",
      "[Epoch 3, Batch 16800] loss: 1.199, acc: 76.65 %\n",
      "[Epoch 3, Batch 16900] loss: 1.233, acc: 76.64 %\n",
      "[Epoch 3, Batch 17000] loss: 1.281, acc: 76.60 %\n",
      "[Epoch 3, Batch 17100] loss: 1.249, acc: 76.64 %\n",
      "[Epoch 3, Batch 17200] loss: 1.244, acc: 76.40 %\n",
      "[Epoch 3, Batch 17300] loss: 1.247, acc: 76.43 %\n",
      "[Epoch 3, Batch 17400] loss: 1.220, acc: 76.53 %\n",
      "[Epoch 3, Batch 17500] loss: 1.232, acc: 76.54 %\n",
      "[Epoch 3, Batch 17600] loss: 1.228, acc: 76.57 %\n",
      "[Epoch 3, Batch 17700] loss: 1.239, acc: 76.65 %\n",
      "[Epoch 3, Batch 17800] loss: 1.238, acc: 76.63 %\n",
      "[Epoch 3, Batch 17900] loss: 1.234, acc: 76.59 %\n",
      "[Epoch 3, Batch 18000] loss: 1.248, acc: 76.57 %\n",
      "[Epoch 3, Batch 18100] loss: 1.225, acc: 76.70 %\n",
      "[Epoch 3, Batch 18200] loss: 1.213, acc: 76.80 %\n",
      "[Epoch 3, Batch 18300] loss: 1.207, acc: 76.88 %\n",
      "[Epoch 3, Batch 18400] loss: 1.231, acc: 76.76 %\n",
      "[Epoch 3, Batch 18500] loss: 1.255, acc: 76.70 %\n",
      "[Epoch 3, Batch 18600] loss: 1.262, acc: 76.54 %\n",
      "[Epoch 3, Batch 18700] loss: 1.239, acc: 76.54 %\n",
      "[Epoch 3, Batch 18800] loss: 1.223, acc: 76.54 %\n",
      "[Epoch 3, Batch 18900] loss: 1.250, acc: 76.51 %\n",
      "[Epoch 3, Batch 19000] loss: 1.201, acc: 76.56 %\n",
      "[Epoch 3, Batch 19100] loss: 1.240, acc: 76.97 %\n",
      "[Epoch 3, Batch 19200] loss: 1.244, acc: 76.45 %\n",
      "[Epoch 3, Batch 19300] loss: 1.205, acc: 76.69 %\n",
      "[Epoch 3, Batch 19400] loss: 1.175, acc: 76.79 %\n",
      "[Epoch 3, Batch 19500] loss: 1.204, acc: 76.80 %\n",
      "[Epoch 3, Batch 19600] loss: 1.207, acc: 76.87 %\n",
      "[Epoch 3, Batch 19700] loss: 1.207, acc: 76.86 %\n",
      "[Epoch 3, Batch 19800] loss: 1.211, acc: 76.92 %\n",
      "[Epoch 3, Batch 19900] loss: 1.179, acc: 76.99 %\n",
      "[Epoch 3, Batch 20000] loss: 1.205, acc: 76.95 %\n",
      "****** Model checkpoint saved at epochs 4 ******\n",
      "epoch 5 learning rate : 4e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79e836b28644d7087d1d9aeee5d1719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4, Batch   100] loss: 1.166, acc: 78.12 %\n",
      "[Epoch 4, Batch   200] loss: 1.184, acc: 77.54 %\n",
      "[Epoch 4, Batch   300] loss: 1.161, acc: 77.72 %\n",
      "[Epoch 4, Batch   400] loss: 1.181, acc: 77.51 %\n",
      "[Epoch 4, Batch   500] loss: 1.232, acc: 77.26 %\n",
      "[Epoch 4, Batch   600] loss: 1.159, acc: 77.33 %\n",
      "[Epoch 4, Batch   700] loss: 1.200, acc: 77.23 %\n",
      "[Epoch 4, Batch   800] loss: 1.160, acc: 77.24 %\n",
      "[Epoch 4, Batch   900] loss: 1.180, acc: 77.24 %\n",
      "[Epoch 4, Batch  1000] loss: 1.151, acc: 77.26 %\n",
      "[Epoch 4, Batch  1100] loss: 1.155, acc: 78.09 %\n",
      "[Epoch 4, Batch  1200] loss: 1.138, acc: 77.77 %\n",
      "[Epoch 4, Batch  1300] loss: 1.161, acc: 77.67 %\n",
      "[Epoch 4, Batch  1400] loss: 1.179, acc: 77.62 %\n",
      "[Epoch 4, Batch  1500] loss: 1.127, acc: 77.63 %\n",
      "[Epoch 4, Batch  1600] loss: 1.164, acc: 77.58 %\n",
      "[Epoch 4, Batch  1700] loss: 1.159, acc: 77.55 %\n",
      "[Epoch 4, Batch  1800] loss: 1.134, acc: 77.59 %\n",
      "[Epoch 4, Batch  1900] loss: 1.138, acc: 77.59 %\n",
      "[Epoch 4, Batch  2000] loss: 1.191, acc: 77.48 %\n",
      "[Epoch 4, Batch  2100] loss: 1.174, acc: 76.52 %\n",
      "[Epoch 4, Batch  2200] loss: 1.174, acc: 76.78 %\n",
      "[Epoch 4, Batch  2300] loss: 1.183, acc: 76.72 %\n",
      "[Epoch 4, Batch  2400] loss: 1.181, acc: 76.67 %\n",
      "[Epoch 4, Batch  2500] loss: 1.154, acc: 76.60 %\n",
      "[Epoch 4, Batch  2600] loss: 1.144, acc: 76.67 %\n",
      "[Epoch 4, Batch  2700] loss: 1.133, acc: 76.78 %\n",
      "[Epoch 4, Batch  2800] loss: 1.162, acc: 76.79 %\n",
      "[Epoch 4, Batch  2900] loss: 1.118, acc: 76.92 %\n",
      "[Epoch 4, Batch  3000] loss: 1.167, acc: 76.92 %\n",
      "[Epoch 4, Batch  3100] loss: 1.166, acc: 76.86 %\n",
      "[Epoch 4, Batch  3200] loss: 1.166, acc: 76.86 %\n",
      "[Epoch 4, Batch  3300] loss: 1.159, acc: 76.93 %\n",
      "[Epoch 4, Batch  3400] loss: 1.153, acc: 77.07 %\n",
      "[Epoch 4, Batch  3500] loss: 1.172, acc: 77.08 %\n",
      "[Epoch 4, Batch  3600] loss: 1.135, acc: 77.13 %\n",
      "[Epoch 4, Batch  3700] loss: 1.108, acc: 77.31 %\n",
      "[Epoch 4, Batch  3800] loss: 1.112, acc: 77.36 %\n",
      "[Epoch 4, Batch  3900] loss: 1.155, acc: 77.30 %\n",
      "[Epoch 4, Batch  4000] loss: 1.126, acc: 77.25 %\n",
      "[Epoch 4, Batch  4100] loss: 1.150, acc: 76.41 %\n",
      "[Epoch 4, Batch  4200] loss: 1.105, acc: 77.31 %\n",
      "[Epoch 4, Batch  4300] loss: 1.126, acc: 77.27 %\n",
      "[Epoch 4, Batch  4400] loss: 1.116, acc: 77.36 %\n",
      "[Epoch 4, Batch  4500] loss: 1.118, acc: 77.45 %\n",
      "[Epoch 4, Batch  4600] loss: 1.130, acc: 77.45 %\n",
      "[Epoch 4, Batch  4700] loss: 1.127, acc: 77.50 %\n",
      "[Epoch 4, Batch  4800] loss: 1.150, acc: 77.41 %\n",
      "[Epoch 4, Batch  4900] loss: 1.131, acc: 77.42 %\n",
      "[Epoch 4, Batch  5000] loss: 1.116, acc: 77.46 %\n",
      "[Epoch 4, Batch  5100] loss: 1.133, acc: 76.88 %\n",
      "[Epoch 4, Batch  5200] loss: 1.086, acc: 77.37 %\n",
      "[Epoch 4, Batch  5300] loss: 1.116, acc: 77.49 %\n",
      "[Epoch 4, Batch  5400] loss: 1.096, acc: 77.70 %\n",
      "[Epoch 4, Batch  5500] loss: 1.112, acc: 77.76 %\n",
      "[Epoch 4, Batch  5600] loss: 1.111, acc: 77.76 %\n",
      "[Epoch 4, Batch  5700] loss: 1.117, acc: 77.70 %\n",
      "[Epoch 4, Batch  5800] loss: 1.083, acc: 77.72 %\n",
      "[Epoch 4, Batch  5900] loss: 1.124, acc: 77.65 %\n",
      "[Epoch 4, Batch  6000] loss: 1.114, acc: 77.64 %\n",
      "[Epoch 4, Batch  6100] loss: 1.124, acc: 76.58 %\n",
      "[Epoch 4, Batch  6200] loss: 1.080, acc: 77.38 %\n",
      "[Epoch 4, Batch  6300] loss: 1.101, acc: 77.60 %\n",
      "[Epoch 4, Batch  6400] loss: 1.103, acc: 77.67 %\n",
      "[Epoch 4, Batch  6500] loss: 1.111, acc: 77.71 %\n",
      "[Epoch 4, Batch  6600] loss: 1.130, acc: 77.58 %\n",
      "[Epoch 4, Batch  6700] loss: 1.081, acc: 77.60 %\n",
      "[Epoch 4, Batch  6800] loss: 1.101, acc: 77.58 %\n",
      "[Epoch 4, Batch  6900] loss: 1.093, acc: 77.62 %\n",
      "[Epoch 4, Batch  7000] loss: 1.070, acc: 77.67 %\n",
      "[Epoch 4, Batch  7100] loss: 1.089, acc: 78.06 %\n",
      "[Epoch 4, Batch  7200] loss: 1.055, acc: 78.09 %\n",
      "[Epoch 4, Batch  7300] loss: 1.066, acc: 78.21 %\n",
      "[Epoch 4, Batch  7400] loss: 1.093, acc: 77.99 %\n",
      "[Epoch 4, Batch  7500] loss: 1.082, acc: 77.84 %\n",
      "[Epoch 4, Batch  7600] loss: 1.097, acc: 77.91 %\n",
      "[Epoch 4, Batch  7700] loss: 1.073, acc: 78.03 %\n",
      "[Epoch 4, Batch  7800] loss: 1.106, acc: 78.03 %\n",
      "[Epoch 4, Batch  7900] loss: 1.053, acc: 78.12 %\n",
      "[Epoch 4, Batch  8000] loss: 1.097, acc: 78.11 %\n",
      "[Epoch 4, Batch  8100] loss: 1.077, acc: 78.31 %\n",
      "[Epoch 4, Batch  8200] loss: 1.091, acc: 77.95 %\n",
      "[Epoch 4, Batch  8300] loss: 1.109, acc: 77.90 %\n",
      "[Epoch 4, Batch  8400] loss: 1.072, acc: 78.01 %\n",
      "[Epoch 4, Batch  8500] loss: 1.042, acc: 78.13 %\n",
      "[Epoch 4, Batch  8600] loss: 1.062, acc: 78.22 %\n",
      "[Epoch 4, Batch  8700] loss: 1.058, acc: 78.35 %\n",
      "[Epoch 4, Batch  8800] loss: 1.064, acc: 78.36 %\n",
      "[Epoch 4, Batch  8900] loss: 1.080, acc: 78.26 %\n",
      "[Epoch 4, Batch  9000] loss: 1.080, acc: 78.21 %\n",
      "[Epoch 4, Batch  9100] loss: 1.077, acc: 78.17 %\n",
      "[Epoch 4, Batch  9200] loss: 1.073, acc: 78.14 %\n",
      "[Epoch 4, Batch  9300] loss: 1.072, acc: 77.93 %\n",
      "[Epoch 4, Batch  9400] loss: 1.026, acc: 78.11 %\n",
      "[Epoch 4, Batch  9500] loss: 1.081, acc: 77.99 %\n",
      "[Epoch 4, Batch  9600] loss: 1.058, acc: 78.06 %\n",
      "[Epoch 4, Batch  9700] loss: 1.120, acc: 77.89 %\n",
      "[Epoch 4, Batch  9800] loss: 1.063, acc: 77.84 %\n",
      "[Epoch 4, Batch  9900] loss: 1.061, acc: 77.81 %\n",
      "[Epoch 4, Batch 10000] loss: 1.106, acc: 77.78 %\n",
      "[Epoch 4, Batch 10100] loss: 1.120, acc: 76.59 %\n",
      "[Epoch 4, Batch 10200] loss: 1.081, acc: 77.01 %\n",
      "[Epoch 4, Batch 10300] loss: 1.071, acc: 77.37 %\n",
      "[Epoch 4, Batch 10400] loss: 1.056, acc: 77.60 %\n",
      "[Epoch 4, Batch 10500] loss: 1.060, acc: 77.72 %\n",
      "[Epoch 4, Batch 10600] loss: 1.062, acc: 77.75 %\n",
      "[Epoch 4, Batch 10700] loss: 1.082, acc: 77.70 %\n",
      "[Epoch 4, Batch 10800] loss: 1.082, acc: 77.66 %\n",
      "[Epoch 4, Batch 10900] loss: 1.090, acc: 77.65 %\n",
      "[Epoch 4, Batch 11000] loss: 1.059, acc: 77.59 %\n",
      "[Epoch 4, Batch 11100] loss: 1.070, acc: 77.66 %\n",
      "[Epoch 4, Batch 11200] loss: 1.018, acc: 78.04 %\n",
      "[Epoch 4, Batch 11300] loss: 1.032, acc: 78.14 %\n",
      "[Epoch 4, Batch 11400] loss: 1.065, acc: 77.93 %\n",
      "[Epoch 4, Batch 11500] loss: 1.071, acc: 77.83 %\n",
      "[Epoch 4, Batch 11600] loss: 1.063, acc: 77.80 %\n",
      "[Epoch 4, Batch 11700] loss: 1.080, acc: 77.72 %\n",
      "[Epoch 4, Batch 11800] loss: 1.053, acc: 77.80 %\n",
      "[Epoch 4, Batch 11900] loss: 1.053, acc: 77.78 %\n",
      "[Epoch 4, Batch 12000] loss: 1.056, acc: 77.82 %\n",
      "[Epoch 4, Batch 12100] loss: 1.067, acc: 77.72 %\n",
      "[Epoch 4, Batch 12200] loss: 1.022, acc: 78.34 %\n",
      "[Epoch 4, Batch 12300] loss: 1.052, acc: 77.98 %\n",
      "[Epoch 4, Batch 12400] loss: 1.059, acc: 77.96 %\n",
      "[Epoch 4, Batch 12500] loss: 1.050, acc: 77.98 %\n",
      "[Epoch 4, Batch 12600] loss: 1.046, acc: 77.95 %\n",
      "[Epoch 4, Batch 12700] loss: 1.021, acc: 78.07 %\n",
      "[Epoch 4, Batch 12800] loss: 1.044, acc: 78.07 %\n",
      "[Epoch 4, Batch 12900] loss: 1.054, acc: 78.01 %\n",
      "[Epoch 4, Batch 13000] loss: 1.062, acc: 77.99 %\n",
      "[Epoch 4, Batch 13100] loss: 1.043, acc: 78.00 %\n",
      "[Epoch 4, Batch 13200] loss: 1.046, acc: 77.56 %\n",
      "[Epoch 4, Batch 13300] loss: 1.056, acc: 77.66 %\n",
      "[Epoch 4, Batch 13400] loss: 1.066, acc: 77.71 %\n",
      "[Epoch 4, Batch 13500] loss: 1.060, acc: 77.75 %\n",
      "[Epoch 4, Batch 13600] loss: 1.071, acc: 77.77 %\n",
      "[Epoch 4, Batch 13700] loss: 1.034, acc: 77.81 %\n",
      "[Epoch 4, Batch 13800] loss: 1.053, acc: 77.82 %\n",
      "[Epoch 4, Batch 13900] loss: 1.028, acc: 77.91 %\n",
      "[Epoch 4, Batch 14000] loss: 1.031, acc: 77.98 %\n",
      "[Epoch 4, Batch 14100] loss: 1.021, acc: 78.03 %\n",
      "[Epoch 4, Batch 14200] loss: 1.061, acc: 77.95 %\n",
      "[Epoch 4, Batch 14300] loss: 1.016, acc: 78.01 %\n",
      "[Epoch 4, Batch 14400] loss: 1.054, acc: 78.04 %\n",
      "[Epoch 4, Batch 14500] loss: 1.049, acc: 78.03 %\n",
      "[Epoch 4, Batch 14600] loss: 1.053, acc: 78.02 %\n",
      "[Epoch 4, Batch 14700] loss: 1.035, acc: 78.02 %\n",
      "[Epoch 4, Batch 14800] loss: 1.024, acc: 78.07 %\n",
      "[Epoch 4, Batch 14900] loss: 1.027, acc: 78.11 %\n",
      "[Epoch 4, Batch 15000] loss: 1.085, acc: 78.00 %\n",
      "[Epoch 4, Batch 15100] loss: 1.037, acc: 78.30 %\n",
      "[Epoch 4, Batch 15200] loss: 1.029, acc: 78.34 %\n",
      "[Epoch 4, Batch 15300] loss: 1.006, acc: 78.48 %\n",
      "[Epoch 4, Batch 15400] loss: 1.054, acc: 78.40 %\n",
      "[Epoch 4, Batch 15500] loss: 1.029, acc: 78.49 %\n",
      "[Epoch 4, Batch 15600] loss: 1.009, acc: 78.53 %\n",
      "[Epoch 4, Batch 15700] loss: 1.014, acc: 78.56 %\n",
      "[Epoch 4, Batch 15800] loss: 1.012, acc: 78.55 %\n",
      "[Epoch 4, Batch 15900] loss: 1.026, acc: 78.46 %\n",
      "[Epoch 4, Batch 16000] loss: 1.012, acc: 78.42 %\n",
      "[Epoch 4, Batch 16100] loss: 1.012, acc: 78.44 %\n",
      "[Epoch 4, Batch 16200] loss: 1.042, acc: 78.16 %\n",
      "[Epoch 4, Batch 16300] loss: 1.003, acc: 78.35 %\n",
      "[Epoch 4, Batch 16400] loss: 1.045, acc: 78.29 %\n",
      "[Epoch 4, Batch 16500] loss: 1.021, acc: 78.43 %\n",
      "[Epoch 4, Batch 16600] loss: 1.027, acc: 78.37 %\n",
      "[Epoch 4, Batch 16700] loss: 1.008, acc: 78.42 %\n",
      "[Epoch 4, Batch 16800] loss: 1.044, acc: 78.29 %\n",
      "[Epoch 4, Batch 16900] loss: 1.028, acc: 78.33 %\n",
      "[Epoch 4, Batch 17000] loss: 1.004, acc: 78.38 %\n",
      "[Epoch 4, Batch 17100] loss: 0.998, acc: 78.84 %\n",
      "[Epoch 4, Batch 17200] loss: 1.020, acc: 78.98 %\n",
      "[Epoch 4, Batch 17300] loss: 1.008, acc: 78.85 %\n",
      "[Epoch 4, Batch 17400] loss: 1.026, acc: 78.61 %\n",
      "[Epoch 4, Batch 17500] loss: 1.043, acc: 78.48 %\n",
      "[Epoch 4, Batch 17600] loss: 1.025, acc: 78.48 %\n",
      "[Epoch 4, Batch 17700] loss: 1.028, acc: 78.39 %\n",
      "[Epoch 4, Batch 17800] loss: 1.026, acc: 78.39 %\n",
      "[Epoch 4, Batch 17900] loss: 1.037, acc: 78.32 %\n",
      "[Epoch 4, Batch 18000] loss: 0.996, acc: 78.36 %\n",
      "[Epoch 4, Batch 18100] loss: 0.994, acc: 78.70 %\n",
      "[Epoch 4, Batch 18200] loss: 1.006, acc: 78.52 %\n",
      "[Epoch 4, Batch 18300] loss: 1.019, acc: 78.47 %\n",
      "[Epoch 4, Batch 18400] loss: 1.020, acc: 78.32 %\n",
      "[Epoch 4, Batch 18500] loss: 0.963, acc: 78.64 %\n",
      "[Epoch 4, Batch 18600] loss: 1.010, acc: 78.60 %\n",
      "[Epoch 4, Batch 18700] loss: 1.013, acc: 78.62 %\n",
      "[Epoch 4, Batch 18800] loss: 1.002, acc: 78.57 %\n",
      "[Epoch 4, Batch 18900] loss: 0.992, acc: 78.59 %\n",
      "[Epoch 4, Batch 19000] loss: 0.976, acc: 78.61 %\n",
      "[Epoch 4, Batch 19100] loss: 0.996, acc: 78.34 %\n",
      "[Epoch 4, Batch 19200] loss: 0.981, acc: 78.88 %\n",
      "[Epoch 4, Batch 19300] loss: 1.032, acc: 78.58 %\n",
      "[Epoch 4, Batch 19400] loss: 0.975, acc: 78.61 %\n",
      "[Epoch 4, Batch 19500] loss: 1.004, acc: 78.60 %\n",
      "[Epoch 4, Batch 19600] loss: 0.990, acc: 78.60 %\n",
      "[Epoch 4, Batch 19700] loss: 0.999, acc: 78.67 %\n",
      "[Epoch 4, Batch 19800] loss: 1.004, acc: 78.66 %\n",
      "[Epoch 4, Batch 19900] loss: 1.014, acc: 78.65 %\n",
      "[Epoch 4, Batch 20000] loss: 0.994, acc: 78.72 %\n",
      "****** Model checkpoint saved at epochs 5 ******\n",
      "epoch 6 learning rate : 3.956295201467611e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fe621f6c964666bb6585102d75e576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5, Batch   100] loss: 1.015, acc: 78.84 %\n",
      "[Epoch 5, Batch   200] loss: 1.017, acc: 78.21 %\n",
      "[Epoch 5, Batch   300] loss: 0.975, acc: 78.42 %\n",
      "[Epoch 5, Batch   400] loss: 1.018, acc: 78.51 %\n",
      "[Epoch 5, Batch   500] loss: 0.985, acc: 78.70 %\n",
      "[Epoch 5, Batch   600] loss: 0.977, acc: 78.77 %\n",
      "[Epoch 5, Batch   700] loss: 0.997, acc: 78.82 %\n",
      "[Epoch 5, Batch   800] loss: 0.944, acc: 78.85 %\n",
      "[Epoch 5, Batch   900] loss: 0.964, acc: 78.92 %\n",
      "[Epoch 5, Batch  1000] loss: 0.979, acc: 78.95 %\n",
      "[Epoch 5, Batch  1100] loss: 1.007, acc: 78.72 %\n",
      "[Epoch 5, Batch  1200] loss: 0.944, acc: 79.04 %\n",
      "[Epoch 5, Batch  1300] loss: 0.953, acc: 79.37 %\n",
      "[Epoch 5, Batch  1400] loss: 0.978, acc: 79.52 %\n",
      "[Epoch 5, Batch  1500] loss: 1.009, acc: 79.19 %\n",
      "[Epoch 5, Batch  1600] loss: 0.980, acc: 79.12 %\n",
      "[Epoch 5, Batch  1700] loss: 0.983, acc: 79.11 %\n",
      "[Epoch 5, Batch  1800] loss: 0.943, acc: 79.14 %\n",
      "[Epoch 5, Batch  1900] loss: 0.981, acc: 79.15 %\n",
      "[Epoch 5, Batch  2000] loss: 0.968, acc: 79.19 %\n",
      "[Epoch 5, Batch  2100] loss: 0.992, acc: 78.84 %\n",
      "[Epoch 5, Batch  2200] loss: 0.983, acc: 78.85 %\n",
      "[Epoch 5, Batch  2300] loss: 0.988, acc: 78.66 %\n",
      "[Epoch 5, Batch  2400] loss: 0.965, acc: 78.77 %\n",
      "[Epoch 5, Batch  2500] loss: 0.969, acc: 78.78 %\n",
      "[Epoch 5, Batch  2600] loss: 1.004, acc: 78.78 %\n",
      "[Epoch 5, Batch  2700] loss: 0.975, acc: 78.75 %\n",
      "[Epoch 5, Batch  2800] loss: 0.971, acc: 78.80 %\n",
      "[Epoch 5, Batch  2900] loss: 0.957, acc: 78.85 %\n",
      "[Epoch 5, Batch  3000] loss: 0.995, acc: 78.81 %\n",
      "[Epoch 5, Batch  3100] loss: 1.001, acc: 78.52 %\n",
      "[Epoch 5, Batch  3200] loss: 0.966, acc: 78.75 %\n",
      "[Epoch 5, Batch  3300] loss: 0.983, acc: 78.84 %\n",
      "[Epoch 5, Batch  3400] loss: 0.995, acc: 78.70 %\n",
      "[Epoch 5, Batch  3500] loss: 0.998, acc: 78.72 %\n",
      "[Epoch 5, Batch  3600] loss: 0.955, acc: 78.78 %\n",
      "[Epoch 5, Batch  3700] loss: 0.978, acc: 78.85 %\n",
      "[Epoch 5, Batch  3800] loss: 1.011, acc: 78.75 %\n",
      "[Epoch 5, Batch  3900] loss: 0.985, acc: 78.82 %\n",
      "[Epoch 5, Batch  4000] loss: 0.949, acc: 78.87 %\n",
      "[Epoch 5, Batch  4100] loss: 1.013, acc: 78.19 %\n",
      "[Epoch 5, Batch  4200] loss: 0.958, acc: 78.73 %\n",
      "[Epoch 5, Batch  4300] loss: 0.941, acc: 79.09 %\n",
      "[Epoch 5, Batch  4400] loss: 0.984, acc: 79.01 %\n",
      "[Epoch 5, Batch  4500] loss: 1.013, acc: 78.90 %\n",
      "[Epoch 5, Batch  4600] loss: 0.979, acc: 78.89 %\n",
      "[Epoch 5, Batch  4700] loss: 0.959, acc: 78.89 %\n",
      "[Epoch 5, Batch  4800] loss: 1.004, acc: 78.79 %\n",
      "[Epoch 5, Batch  4900] loss: 0.952, acc: 78.85 %\n",
      "[Epoch 5, Batch  5000] loss: 0.983, acc: 78.82 %\n",
      "[Epoch 5, Batch  5100] loss: 0.967, acc: 79.12 %\n",
      "[Epoch 5, Batch  5200] loss: 0.963, acc: 78.98 %\n",
      "[Epoch 5, Batch  5300] loss: 0.957, acc: 78.93 %\n",
      "[Epoch 5, Batch  5400] loss: 0.923, acc: 79.24 %\n",
      "[Epoch 5, Batch  5500] loss: 0.963, acc: 79.22 %\n",
      "[Epoch 5, Batch  5600] loss: 0.972, acc: 79.17 %\n",
      "[Epoch 5, Batch  5700] loss: 0.973, acc: 79.09 %\n",
      "[Epoch 5, Batch  5800] loss: 0.939, acc: 79.14 %\n",
      "[Epoch 5, Batch  5900] loss: 0.988, acc: 79.12 %\n",
      "[Epoch 5, Batch  6000] loss: 0.963, acc: 79.09 %\n",
      "[Epoch 5, Batch  6100] loss: 0.961, acc: 79.19 %\n",
      "[Epoch 5, Batch  6200] loss: 0.957, acc: 79.17 %\n",
      "[Epoch 5, Batch  6300] loss: 0.964, acc: 79.21 %\n",
      "[Epoch 5, Batch  6400] loss: 0.959, acc: 79.19 %\n",
      "[Epoch 5, Batch  6500] loss: 0.936, acc: 79.29 %\n",
      "[Epoch 5, Batch  6600] loss: 0.989, acc: 79.12 %\n",
      "[Epoch 5, Batch  6700] loss: 0.950, acc: 79.04 %\n",
      "[Epoch 5, Batch  6800] loss: 0.959, acc: 79.02 %\n",
      "[Epoch 5, Batch  6900] loss: 0.965, acc: 79.06 %\n",
      "[Epoch 5, Batch  7000] loss: 0.970, acc: 79.06 %\n",
      "[Epoch 5, Batch  7100] loss: 0.945, acc: 78.95 %\n",
      "[Epoch 5, Batch  7200] loss: 0.947, acc: 78.97 %\n",
      "[Epoch 5, Batch  7300] loss: 0.952, acc: 79.16 %\n",
      "[Epoch 5, Batch  7400] loss: 0.969, acc: 79.08 %\n",
      "[Epoch 5, Batch  7500] loss: 0.955, acc: 79.20 %\n",
      "[Epoch 5, Batch  7600] loss: 0.935, acc: 79.17 %\n",
      "[Epoch 5, Batch  7700] loss: 0.947, acc: 79.20 %\n",
      "[Epoch 5, Batch  7800] loss: 0.941, acc: 79.25 %\n",
      "[Epoch 5, Batch  7900] loss: 0.962, acc: 79.19 %\n",
      "[Epoch 5, Batch  8000] loss: 0.961, acc: 79.22 %\n",
      "[Epoch 5, Batch  8100] loss: 0.926, acc: 79.88 %\n",
      "[Epoch 5, Batch  8200] loss: 0.933, acc: 79.87 %\n",
      "[Epoch 5, Batch  8300] loss: 0.956, acc: 79.62 %\n",
      "[Epoch 5, Batch  8400] loss: 0.971, acc: 79.45 %\n",
      "[Epoch 5, Batch  8500] loss: 0.976, acc: 79.21 %\n",
      "[Epoch 5, Batch  8600] loss: 0.947, acc: 79.23 %\n",
      "[Epoch 5, Batch  8700] loss: 0.976, acc: 79.19 %\n",
      "[Epoch 5, Batch  8800] loss: 0.985, acc: 79.04 %\n",
      "[Epoch 5, Batch  8900] loss: 0.952, acc: 79.02 %\n",
      "[Epoch 5, Batch  9000] loss: 0.952, acc: 79.02 %\n",
      "[Epoch 5, Batch  9100] loss: 0.962, acc: 79.09 %\n",
      "[Epoch 5, Batch  9200] loss: 0.950, acc: 79.12 %\n",
      "[Epoch 5, Batch  9300] loss: 0.974, acc: 78.90 %\n",
      "[Epoch 5, Batch  9400] loss: 0.919, acc: 79.05 %\n",
      "[Epoch 5, Batch  9500] loss: 0.926, acc: 79.13 %\n",
      "[Epoch 5, Batch  9600] loss: 0.958, acc: 79.15 %\n",
      "[Epoch 5, Batch  9700] loss: 0.940, acc: 79.17 %\n",
      "[Epoch 5, Batch  9800] loss: 0.923, acc: 79.28 %\n",
      "[Epoch 5, Batch  9900] loss: 0.968, acc: 79.23 %\n",
      "[Epoch 5, Batch 10000] loss: 0.983, acc: 79.15 %\n",
      "[Epoch 5, Batch 10100] loss: 0.940, acc: 79.58 %\n",
      "[Epoch 5, Batch 10200] loss: 0.970, acc: 78.98 %\n",
      "[Epoch 5, Batch 10300] loss: 0.951, acc: 79.04 %\n",
      "[Epoch 5, Batch 10400] loss: 0.974, acc: 78.73 %\n",
      "[Epoch 5, Batch 10500] loss: 0.955, acc: 78.66 %\n",
      "[Epoch 5, Batch 10600] loss: 0.918, acc: 78.78 %\n",
      "[Epoch 5, Batch 10700] loss: 0.936, acc: 78.79 %\n",
      "[Epoch 5, Batch 10800] loss: 0.906, acc: 78.86 %\n",
      "[Epoch 5, Batch 10900] loss: 0.967, acc: 78.83 %\n",
      "[Epoch 5, Batch 11000] loss: 0.942, acc: 78.85 %\n",
      "[Epoch 5, Batch 11100] loss: 0.976, acc: 78.83 %\n",
      "[Epoch 5, Batch 11200] loss: 0.928, acc: 79.14 %\n",
      "[Epoch 5, Batch 11300] loss: 0.927, acc: 79.29 %\n",
      "[Epoch 5, Batch 11400] loss: 0.963, acc: 79.15 %\n",
      "[Epoch 5, Batch 11500] loss: 0.950, acc: 79.04 %\n",
      "[Epoch 5, Batch 11600] loss: 0.974, acc: 79.04 %\n",
      "[Epoch 5, Batch 11700] loss: 0.939, acc: 79.12 %\n",
      "[Epoch 5, Batch 11800] loss: 0.954, acc: 79.16 %\n",
      "[Epoch 5, Batch 11900] loss: 0.935, acc: 79.24 %\n",
      "[Epoch 5, Batch 12000] loss: 0.959, acc: 79.20 %\n",
      "[Epoch 5, Batch 12100] loss: 0.928, acc: 79.44 %\n",
      "[Epoch 5, Batch 12200] loss: 0.932, acc: 79.58 %\n",
      "[Epoch 5, Batch 12300] loss: 0.915, acc: 79.81 %\n",
      "[Epoch 5, Batch 12400] loss: 0.933, acc: 79.77 %\n",
      "[Epoch 5, Batch 12500] loss: 0.938, acc: 79.68 %\n",
      "[Epoch 5, Batch 12600] loss: 0.949, acc: 79.52 %\n",
      "[Epoch 5, Batch 12700] loss: 0.971, acc: 79.40 %\n",
      "[Epoch 5, Batch 12800] loss: 0.913, acc: 79.42 %\n",
      "[Epoch 5, Batch 12900] loss: 0.943, acc: 79.46 %\n",
      "[Epoch 5, Batch 13000] loss: 0.950, acc: 79.40 %\n",
      "[Epoch 5, Batch 13100] loss: 0.965, acc: 78.81 %\n",
      "[Epoch 5, Batch 13200] loss: 0.971, acc: 78.90 %\n",
      "[Epoch 5, Batch 13300] loss: 0.945, acc: 79.09 %\n",
      "[Epoch 5, Batch 13400] loss: 0.944, acc: 79.17 %\n",
      "[Epoch 5, Batch 13500] loss: 0.954, acc: 79.01 %\n",
      "[Epoch 5, Batch 13600] loss: 0.938, acc: 79.09 %\n",
      "[Epoch 5, Batch 13700] loss: 0.903, acc: 79.19 %\n",
      "[Epoch 5, Batch 13800] loss: 0.935, acc: 79.25 %\n",
      "[Epoch 5, Batch 13900] loss: 0.969, acc: 79.19 %\n",
      "[Epoch 5, Batch 14000] loss: 0.905, acc: 79.22 %\n",
      "[Epoch 5, Batch 14100] loss: 0.933, acc: 79.66 %\n",
      "[Epoch 5, Batch 14200] loss: 0.971, acc: 79.25 %\n",
      "[Epoch 5, Batch 14300] loss: 0.954, acc: 79.35 %\n",
      "[Epoch 5, Batch 14400] loss: 0.930, acc: 79.37 %\n",
      "[Epoch 5, Batch 14500] loss: 0.952, acc: 79.23 %\n",
      "[Epoch 5, Batch 14600] loss: 0.928, acc: 79.21 %\n",
      "[Epoch 5, Batch 14700] loss: 0.978, acc: 79.16 %\n",
      "[Epoch 5, Batch 14800] loss: 0.902, acc: 79.26 %\n",
      "[Epoch 5, Batch 14900] loss: 0.926, acc: 79.30 %\n",
      "[Epoch 5, Batch 15000] loss: 0.891, acc: 79.37 %\n",
      "[Epoch 5, Batch 15100] loss: 0.933, acc: 79.58 %\n",
      "[Epoch 5, Batch 15200] loss: 0.971, acc: 79.20 %\n",
      "[Epoch 5, Batch 15300] loss: 0.935, acc: 79.24 %\n",
      "[Epoch 5, Batch 15400] loss: 0.933, acc: 79.34 %\n",
      "[Epoch 5, Batch 15500] loss: 0.921, acc: 79.41 %\n",
      "[Epoch 5, Batch 15600] loss: 0.989, acc: 79.20 %\n",
      "[Epoch 5, Batch 15700] loss: 0.930, acc: 79.19 %\n",
      "[Epoch 5, Batch 15800] loss: 0.936, acc: 79.22 %\n",
      "[Epoch 5, Batch 15900] loss: 0.939, acc: 79.22 %\n",
      "[Epoch 5, Batch 16000] loss: 0.944, acc: 79.20 %\n",
      "[Epoch 5, Batch 16100] loss: 0.877, acc: 79.91 %\n",
      "[Epoch 5, Batch 16200] loss: 0.928, acc: 79.43 %\n",
      "[Epoch 5, Batch 16300] loss: 0.969, acc: 79.21 %\n",
      "[Epoch 5, Batch 16400] loss: 0.973, acc: 78.97 %\n",
      "[Epoch 5, Batch 16500] loss: 0.950, acc: 79.04 %\n",
      "[Epoch 5, Batch 16600] loss: 0.933, acc: 79.03 %\n",
      "[Epoch 5, Batch 16700] loss: 0.953, acc: 79.03 %\n",
      "[Epoch 5, Batch 16800] loss: 0.900, acc: 79.16 %\n",
      "[Epoch 5, Batch 16900] loss: 0.949, acc: 79.13 %\n",
      "[Epoch 5, Batch 17000] loss: 0.950, acc: 79.10 %\n",
      "[Epoch 5, Batch 17100] loss: 0.940, acc: 79.28 %\n",
      "[Epoch 5, Batch 17200] loss: 0.926, acc: 79.62 %\n",
      "[Epoch 5, Batch 17300] loss: 0.953, acc: 79.30 %\n",
      "[Epoch 5, Batch 17400] loss: 0.914, acc: 79.50 %\n",
      "[Epoch 5, Batch 17500] loss: 0.922, acc: 79.52 %\n",
      "[Epoch 5, Batch 17600] loss: 0.915, acc: 79.55 %\n",
      "[Epoch 5, Batch 17700] loss: 0.937, acc: 79.42 %\n",
      "[Epoch 5, Batch 17800] loss: 0.913, acc: 79.50 %\n",
      "[Epoch 5, Batch 17900] loss: 0.894, acc: 79.61 %\n",
      "[Epoch 5, Batch 18000] loss: 0.959, acc: 79.48 %\n",
      "[Epoch 5, Batch 18100] loss: 0.923, acc: 79.41 %\n",
      "[Epoch 5, Batch 18200] loss: 0.939, acc: 79.41 %\n",
      "[Epoch 5, Batch 18300] loss: 0.939, acc: 79.26 %\n",
      "[Epoch 5, Batch 18400] loss: 0.952, acc: 79.04 %\n",
      "[Epoch 5, Batch 18500] loss: 0.907, acc: 79.22 %\n",
      "[Epoch 5, Batch 18600] loss: 0.923, acc: 79.34 %\n",
      "[Epoch 5, Batch 18700] loss: 0.929, acc: 79.35 %\n",
      "[Epoch 5, Batch 18800] loss: 0.928, acc: 79.43 %\n",
      "[Epoch 5, Batch 18900] loss: 0.914, acc: 79.50 %\n",
      "[Epoch 5, Batch 19000] loss: 0.915, acc: 79.46 %\n",
      "[Epoch 5, Batch 19100] loss: 0.943, acc: 78.53 %\n",
      "[Epoch 5, Batch 19200] loss: 0.936, acc: 78.87 %\n",
      "[Epoch 5, Batch 19300] loss: 0.892, acc: 79.31 %\n",
      "[Epoch 5, Batch 19400] loss: 0.948, acc: 79.23 %\n",
      "[Epoch 5, Batch 19500] loss: 0.927, acc: 79.33 %\n",
      "[Epoch 5, Batch 19600] loss: 0.953, acc: 79.30 %\n",
      "[Epoch 5, Batch 19700] loss: 0.907, acc: 79.41 %\n",
      "[Epoch 5, Batch 19800] loss: 0.929, acc: 79.41 %\n",
      "[Epoch 5, Batch 19900] loss: 0.921, acc: 79.43 %\n",
      "[Epoch 5, Batch 20000] loss: 0.943, acc: 79.41 %\n",
      "****** Model checkpoint saved at epochs 6 ******\n",
      "epoch 7 learning rate : 3.8270909152852014e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbd688461b5405e94aeb920c5960572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6, Batch   100] loss: 0.961, acc: 78.34 %\n",
      "[Epoch 6, Batch   200] loss: 0.914, acc: 79.23 %\n",
      "[Epoch 6, Batch   300] loss: 0.913, acc: 79.53 %\n",
      "[Epoch 6, Batch   400] loss: 0.883, acc: 79.67 %\n",
      "[Epoch 6, Batch   500] loss: 0.906, acc: 79.64 %\n",
      "[Epoch 6, Batch   600] loss: 0.919, acc: 79.54 %\n",
      "[Epoch 6, Batch   700] loss: 0.937, acc: 79.40 %\n",
      "[Epoch 6, Batch   800] loss: 0.898, acc: 79.52 %\n",
      "[Epoch 6, Batch   900] loss: 0.895, acc: 79.56 %\n",
      "[Epoch 6, Batch  1000] loss: 0.884, acc: 79.63 %\n",
      "[Epoch 6, Batch  1100] loss: 0.935, acc: 79.23 %\n",
      "[Epoch 6, Batch  1200] loss: 0.914, acc: 79.34 %\n",
      "[Epoch 6, Batch  1300] loss: 0.920, acc: 79.46 %\n",
      "[Epoch 6, Batch  1400] loss: 0.876, acc: 79.84 %\n",
      "[Epoch 6, Batch  1500] loss: 0.906, acc: 79.87 %\n",
      "[Epoch 6, Batch  1600] loss: 0.880, acc: 79.91 %\n",
      "[Epoch 6, Batch  1700] loss: 0.899, acc: 79.95 %\n",
      "[Epoch 6, Batch  1800] loss: 0.919, acc: 79.92 %\n",
      "[Epoch 6, Batch  1900] loss: 0.925, acc: 79.85 %\n",
      "[Epoch 6, Batch  2000] loss: 0.896, acc: 79.83 %\n",
      "[Epoch 6, Batch  2100] loss: 0.910, acc: 80.00 %\n",
      "[Epoch 6, Batch  2200] loss: 0.873, acc: 80.16 %\n",
      "[Epoch 6, Batch  2300] loss: 0.926, acc: 79.89 %\n",
      "[Epoch 6, Batch  2400] loss: 0.908, acc: 79.89 %\n",
      "[Epoch 6, Batch  2500] loss: 0.901, acc: 79.82 %\n",
      "[Epoch 6, Batch  2600] loss: 0.910, acc: 79.80 %\n",
      "[Epoch 6, Batch  2700] loss: 0.919, acc: 79.85 %\n",
      "[Epoch 6, Batch  2800] loss: 0.898, acc: 79.89 %\n",
      "[Epoch 6, Batch  2900] loss: 0.921, acc: 79.90 %\n",
      "[Epoch 6, Batch  3000] loss: 0.934, acc: 79.75 %\n",
      "[Epoch 6, Batch  3100] loss: 0.917, acc: 79.61 %\n",
      "[Epoch 6, Batch  3200] loss: 0.901, acc: 79.64 %\n",
      "[Epoch 6, Batch  3300] loss: 0.898, acc: 79.90 %\n",
      "[Epoch 6, Batch  3400] loss: 0.897, acc: 79.87 %\n",
      "[Epoch 6, Batch  3500] loss: 0.922, acc: 79.72 %\n",
      "[Epoch 6, Batch  3600] loss: 0.876, acc: 79.93 %\n",
      "[Epoch 6, Batch  3700] loss: 0.893, acc: 79.92 %\n",
      "[Epoch 6, Batch  3800] loss: 0.888, acc: 79.94 %\n",
      "[Epoch 6, Batch  3900] loss: 0.879, acc: 79.98 %\n",
      "[Epoch 6, Batch  4000] loss: 0.900, acc: 79.95 %\n",
      "[Epoch 6, Batch  4100] loss: 0.887, acc: 80.27 %\n",
      "[Epoch 6, Batch  4200] loss: 0.896, acc: 80.23 %\n",
      "[Epoch 6, Batch  4300] loss: 0.894, acc: 80.26 %\n",
      "[Epoch 6, Batch  4400] loss: 0.939, acc: 79.95 %\n",
      "[Epoch 6, Batch  4500] loss: 0.910, acc: 79.85 %\n",
      "[Epoch 6, Batch  4600] loss: 0.894, acc: 79.87 %\n",
      "[Epoch 6, Batch  4700] loss: 0.910, acc: 79.91 %\n",
      "[Epoch 6, Batch  4800] loss: 0.892, acc: 79.97 %\n",
      "[Epoch 6, Batch  4900] loss: 0.910, acc: 79.95 %\n",
      "[Epoch 6, Batch  5000] loss: 0.886, acc: 79.99 %\n",
      "[Epoch 6, Batch  5100] loss: 0.918, acc: 79.75 %\n",
      "[Epoch 6, Batch  5200] loss: 0.901, acc: 79.59 %\n",
      "[Epoch 6, Batch  5300] loss: 0.922, acc: 79.72 %\n",
      "[Epoch 6, Batch  5400] loss: 0.893, acc: 79.86 %\n",
      "[Epoch 6, Batch  5500] loss: 0.897, acc: 79.84 %\n",
      "[Epoch 6, Batch  5600] loss: 0.915, acc: 79.81 %\n",
      "[Epoch 6, Batch  5700] loss: 0.912, acc: 79.79 %\n",
      "[Epoch 6, Batch  5800] loss: 0.914, acc: 79.75 %\n",
      "[Epoch 6, Batch  5900] loss: 0.922, acc: 79.62 %\n",
      "[Epoch 6, Batch  6000] loss: 0.916, acc: 79.64 %\n",
      "[Epoch 6, Batch  6100] loss: 0.918, acc: 78.72 %\n",
      "[Epoch 6, Batch  6200] loss: 0.877, acc: 79.48 %\n",
      "[Epoch 6, Batch  6300] loss: 0.928, acc: 79.38 %\n",
      "[Epoch 6, Batch  6400] loss: 0.892, acc: 79.54 %\n",
      "[Epoch 6, Batch  6500] loss: 0.941, acc: 79.36 %\n",
      "[Epoch 6, Batch  6600] loss: 0.902, acc: 79.43 %\n",
      "[Epoch 6, Batch  6700] loss: 0.929, acc: 79.46 %\n",
      "[Epoch 6, Batch  6800] loss: 0.882, acc: 79.49 %\n",
      "[Epoch 6, Batch  6900] loss: 0.901, acc: 79.49 %\n",
      "[Epoch 6, Batch  7000] loss: 0.906, acc: 79.51 %\n",
      "[Epoch 6, Batch  7100] loss: 0.898, acc: 80.39 %\n",
      "[Epoch 6, Batch  7200] loss: 0.918, acc: 79.93 %\n",
      "[Epoch 6, Batch  7300] loss: 0.882, acc: 80.06 %\n",
      "[Epoch 6, Batch  7400] loss: 0.882, acc: 79.98 %\n",
      "[Epoch 6, Batch  7500] loss: 0.890, acc: 80.01 %\n",
      "[Epoch 6, Batch  7600] loss: 0.860, acc: 80.07 %\n",
      "[Epoch 6, Batch  7700] loss: 0.876, acc: 80.12 %\n",
      "[Epoch 6, Batch  7800] loss: 0.894, acc: 80.11 %\n",
      "[Epoch 6, Batch  7900] loss: 0.904, acc: 80.04 %\n",
      "[Epoch 6, Batch  8000] loss: 0.862, acc: 80.15 %\n",
      "[Epoch 6, Batch  8100] loss: 0.916, acc: 79.84 %\n",
      "[Epoch 6, Batch  8200] loss: 0.896, acc: 79.82 %\n",
      "[Epoch 6, Batch  8300] loss: 0.921, acc: 79.71 %\n",
      "[Epoch 6, Batch  8400] loss: 0.951, acc: 79.40 %\n",
      "[Epoch 6, Batch  8500] loss: 0.907, acc: 79.43 %\n",
      "[Epoch 6, Batch  8600] loss: 0.855, acc: 79.72 %\n",
      "[Epoch 6, Batch  8700] loss: 0.885, acc: 79.85 %\n",
      "[Epoch 6, Batch  8800] loss: 0.922, acc: 79.79 %\n",
      "[Epoch 6, Batch  8900] loss: 0.874, acc: 79.90 %\n",
      "[Epoch 6, Batch  9000] loss: 0.918, acc: 79.88 %\n",
      "[Epoch 6, Batch  9100] loss: 0.915, acc: 79.50 %\n",
      "[Epoch 6, Batch  9200] loss: 0.882, acc: 79.84 %\n",
      "[Epoch 6, Batch  9300] loss: 0.898, acc: 80.06 %\n",
      "[Epoch 6, Batch  9400] loss: 0.893, acc: 79.97 %\n",
      "[Epoch 6, Batch  9500] loss: 0.933, acc: 79.84 %\n",
      "[Epoch 6, Batch  9600] loss: 0.866, acc: 79.91 %\n",
      "[Epoch 6, Batch  9700] loss: 0.904, acc: 79.91 %\n",
      "[Epoch 6, Batch  9800] loss: 0.903, acc: 79.89 %\n",
      "[Epoch 6, Batch  9900] loss: 0.881, acc: 79.87 %\n",
      "[Epoch 6, Batch 10000] loss: 0.893, acc: 79.90 %\n",
      "[Epoch 6, Batch 10100] loss: 0.902, acc: 80.09 %\n",
      "[Epoch 6, Batch 10200] loss: 0.911, acc: 79.77 %\n",
      "[Epoch 6, Batch 10300] loss: 0.882, acc: 79.87 %\n",
      "[Epoch 6, Batch 10400] loss: 0.891, acc: 79.88 %\n",
      "[Epoch 6, Batch 10500] loss: 0.908, acc: 79.69 %\n",
      "[Epoch 6, Batch 10600] loss: 0.902, acc: 79.76 %\n",
      "[Epoch 6, Batch 10700] loss: 0.869, acc: 79.89 %\n",
      "[Epoch 6, Batch 10800] loss: 0.892, acc: 79.91 %\n",
      "[Epoch 6, Batch 10900] loss: 0.902, acc: 79.90 %\n",
      "[Epoch 6, Batch 11000] loss: 0.904, acc: 79.89 %\n",
      "[Epoch 6, Batch 11100] loss: 0.923, acc: 79.12 %\n",
      "[Epoch 6, Batch 11200] loss: 0.846, acc: 79.80 %\n",
      "[Epoch 6, Batch 11300] loss: 0.886, acc: 79.90 %\n",
      "[Epoch 6, Batch 11400] loss: 0.899, acc: 79.91 %\n",
      "[Epoch 6, Batch 11500] loss: 0.887, acc: 79.98 %\n",
      "[Epoch 6, Batch 11600] loss: 0.886, acc: 79.94 %\n",
      "[Epoch 6, Batch 11700] loss: 0.859, acc: 79.96 %\n",
      "[Epoch 6, Batch 11800] loss: 0.913, acc: 79.91 %\n",
      "[Epoch 6, Batch 11900] loss: 0.882, acc: 79.94 %\n",
      "[Epoch 6, Batch 12000] loss: 0.888, acc: 79.94 %\n",
      "[Epoch 6, Batch 12100] loss: 0.860, acc: 80.59 %\n",
      "[Epoch 6, Batch 12200] loss: 0.881, acc: 80.44 %\n",
      "[Epoch 6, Batch 12300] loss: 0.901, acc: 80.09 %\n",
      "[Epoch 6, Batch 12400] loss: 0.899, acc: 80.12 %\n",
      "[Epoch 6, Batch 12500] loss: 0.875, acc: 80.15 %\n",
      "[Epoch 6, Batch 12600] loss: 0.895, acc: 80.11 %\n",
      "[Epoch 6, Batch 12700] loss: 0.917, acc: 80.01 %\n",
      "[Epoch 6, Batch 12800] loss: 0.864, acc: 80.11 %\n",
      "[Epoch 6, Batch 12900] loss: 0.868, acc: 80.16 %\n",
      "[Epoch 6, Batch 13000] loss: 0.918, acc: 80.11 %\n",
      "[Epoch 6, Batch 13100] loss: 0.900, acc: 79.25 %\n",
      "[Epoch 6, Batch 13200] loss: 0.887, acc: 79.74 %\n",
      "[Epoch 6, Batch 13300] loss: 0.874, acc: 79.84 %\n",
      "[Epoch 6, Batch 13400] loss: 0.886, acc: 79.85 %\n",
      "[Epoch 6, Batch 13500] loss: 0.861, acc: 79.93 %\n",
      "[Epoch 6, Batch 13600] loss: 0.878, acc: 80.04 %\n",
      "[Epoch 6, Batch 13700] loss: 0.889, acc: 79.99 %\n",
      "[Epoch 6, Batch 13800] loss: 0.878, acc: 80.01 %\n",
      "[Epoch 6, Batch 13900] loss: 0.891, acc: 80.01 %\n",
      "[Epoch 6, Batch 14000] loss: 0.862, acc: 80.06 %\n",
      "[Epoch 6, Batch 14100] loss: 0.904, acc: 79.70 %\n",
      "[Epoch 6, Batch 14200] loss: 0.886, acc: 79.84 %\n",
      "[Epoch 6, Batch 14300] loss: 0.867, acc: 80.17 %\n",
      "[Epoch 6, Batch 14400] loss: 0.891, acc: 80.09 %\n",
      "[Epoch 6, Batch 14500] loss: 0.856, acc: 80.10 %\n",
      "[Epoch 6, Batch 14600] loss: 0.851, acc: 80.18 %\n",
      "[Epoch 6, Batch 14700] loss: 0.889, acc: 80.10 %\n",
      "[Epoch 6, Batch 14800] loss: 0.893, acc: 80.07 %\n",
      "[Epoch 6, Batch 14900] loss: 0.887, acc: 80.04 %\n",
      "[Epoch 6, Batch 15000] loss: 0.887, acc: 80.01 %\n",
      "[Epoch 6, Batch 15100] loss: 0.900, acc: 79.67 %\n",
      "[Epoch 6, Batch 15200] loss: 0.876, acc: 79.61 %\n",
      "[Epoch 6, Batch 15300] loss: 0.875, acc: 79.69 %\n",
      "[Epoch 6, Batch 15400] loss: 0.884, acc: 79.75 %\n",
      "[Epoch 6, Batch 15500] loss: 0.868, acc: 79.87 %\n",
      "[Epoch 6, Batch 15600] loss: 0.901, acc: 79.87 %\n",
      "[Epoch 6, Batch 15700] loss: 0.873, acc: 79.91 %\n",
      "[Epoch 6, Batch 15800] loss: 0.856, acc: 79.96 %\n",
      "[Epoch 6, Batch 15900] loss: 0.871, acc: 79.95 %\n",
      "[Epoch 6, Batch 16000] loss: 0.863, acc: 80.03 %\n",
      "[Epoch 6, Batch 16100] loss: 0.906, acc: 78.98 %\n",
      "[Epoch 6, Batch 16200] loss: 0.893, acc: 79.37 %\n",
      "[Epoch 6, Batch 16300] loss: 0.857, acc: 79.85 %\n",
      "[Epoch 6, Batch 16400] loss: 0.874, acc: 79.77 %\n",
      "[Epoch 6, Batch 16500] loss: 0.875, acc: 79.73 %\n",
      "[Epoch 6, Batch 16600] loss: 0.895, acc: 79.75 %\n",
      "[Epoch 6, Batch 16700] loss: 0.881, acc: 79.85 %\n",
      "[Epoch 6, Batch 16800] loss: 0.900, acc: 79.91 %\n",
      "[Epoch 6, Batch 16900] loss: 0.881, acc: 79.90 %\n",
      "[Epoch 6, Batch 17000] loss: 0.868, acc: 79.94 %\n",
      "[Epoch 6, Batch 17100] loss: 0.869, acc: 80.02 %\n",
      "[Epoch 6, Batch 17200] loss: 0.891, acc: 79.92 %\n",
      "[Epoch 6, Batch 17300] loss: 0.871, acc: 80.09 %\n",
      "[Epoch 6, Batch 17400] loss: 0.862, acc: 80.21 %\n",
      "[Epoch 6, Batch 17500] loss: 0.884, acc: 80.06 %\n",
      "[Epoch 6, Batch 17600] loss: 0.871, acc: 80.13 %\n",
      "[Epoch 6, Batch 17700] loss: 0.882, acc: 80.14 %\n",
      "[Epoch 6, Batch 17800] loss: 0.873, acc: 80.08 %\n",
      "[Epoch 6, Batch 17900] loss: 0.897, acc: 80.07 %\n",
      "[Epoch 6, Batch 18000] loss: 0.900, acc: 79.99 %\n",
      "[Epoch 6, Batch 18100] loss: 0.870, acc: 79.95 %\n",
      "[Epoch 6, Batch 18200] loss: 0.869, acc: 80.43 %\n",
      "[Epoch 6, Batch 18300] loss: 0.888, acc: 80.30 %\n",
      "[Epoch 6, Batch 18400] loss: 0.869, acc: 80.15 %\n",
      "[Epoch 6, Batch 18500] loss: 0.860, acc: 80.33 %\n",
      "[Epoch 6, Batch 18600] loss: 0.870, acc: 80.34 %\n",
      "[Epoch 6, Batch 18700] loss: 0.909, acc: 80.25 %\n",
      "[Epoch 6, Batch 18800] loss: 0.885, acc: 80.23 %\n",
      "[Epoch 6, Batch 18900] loss: 0.926, acc: 80.11 %\n",
      "[Epoch 6, Batch 19000] loss: 0.901, acc: 80.06 %\n",
      "[Epoch 6, Batch 19100] loss: 0.868, acc: 79.88 %\n",
      "[Epoch 6, Batch 19200] loss: 0.869, acc: 80.17 %\n",
      "[Epoch 6, Batch 19300] loss: 0.857, acc: 80.20 %\n",
      "[Epoch 6, Batch 19400] loss: 0.881, acc: 80.12 %\n",
      "[Epoch 6, Batch 19500] loss: 0.877, acc: 80.12 %\n",
      "[Epoch 6, Batch 19600] loss: 0.897, acc: 80.02 %\n",
      "[Epoch 6, Batch 19700] loss: 0.880, acc: 79.98 %\n",
      "[Epoch 6, Batch 19800] loss: 0.885, acc: 80.01 %\n",
      "[Epoch 6, Batch 19900] loss: 0.877, acc: 80.07 %\n",
      "[Epoch 6, Batch 20000] loss: 0.832, acc: 80.13 %\n",
      "****** Model checkpoint saved at epochs 7 ******\n",
      "epoch 8 learning rate : 3.6180339887498948e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8af86529c743729580ba4d211ef40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7, Batch   100] loss: 0.889, acc: 80.05 %\n",
      "[Epoch 7, Batch   200] loss: 0.852, acc: 80.50 %\n",
      "[Epoch 7, Batch   300] loss: 0.851, acc: 80.62 %\n",
      "[Epoch 7, Batch   400] loss: 0.860, acc: 80.48 %\n",
      "[Epoch 7, Batch   500] loss: 0.855, acc: 80.41 %\n",
      "[Epoch 7, Batch   600] loss: 0.927, acc: 80.20 %\n",
      "[Epoch 7, Batch   700] loss: 0.901, acc: 80.06 %\n",
      "[Epoch 7, Batch   800] loss: 0.845, acc: 80.09 %\n",
      "[Epoch 7, Batch   900] loss: 0.883, acc: 80.07 %\n",
      "[Epoch 7, Batch  1000] loss: 0.855, acc: 80.14 %\n",
      "[Epoch 7, Batch  1100] loss: 0.887, acc: 80.27 %\n",
      "[Epoch 7, Batch  1200] loss: 0.900, acc: 80.23 %\n",
      "[Epoch 7, Batch  1300] loss: 0.870, acc: 80.21 %\n",
      "[Epoch 7, Batch  1400] loss: 0.840, acc: 80.27 %\n",
      "[Epoch 7, Batch  1500] loss: 0.867, acc: 80.23 %\n",
      "[Epoch 7, Batch  1600] loss: 0.843, acc: 80.27 %\n",
      "[Epoch 7, Batch  1700] loss: 0.850, acc: 80.38 %\n",
      "[Epoch 7, Batch  1800] loss: 0.875, acc: 80.39 %\n",
      "[Epoch 7, Batch  1900] loss: 0.873, acc: 80.36 %\n",
      "[Epoch 7, Batch  2000] loss: 0.905, acc: 80.27 %\n",
      "[Epoch 7, Batch  2100] loss: 0.829, acc: 81.22 %\n",
      "[Epoch 7, Batch  2200] loss: 0.887, acc: 80.50 %\n",
      "[Epoch 7, Batch  2300] loss: 0.887, acc: 80.42 %\n",
      "[Epoch 7, Batch  2400] loss: 0.879, acc: 80.55 %\n",
      "[Epoch 7, Batch  2500] loss: 0.840, acc: 80.58 %\n",
      "[Epoch 7, Batch  2600] loss: 0.858, acc: 80.64 %\n",
      "[Epoch 7, Batch  2700] loss: 0.841, acc: 80.66 %\n",
      "[Epoch 7, Batch  2800] loss: 0.870, acc: 80.67 %\n",
      "[Epoch 7, Batch  2900] loss: 0.864, acc: 80.63 %\n",
      "[Epoch 7, Batch  3000] loss: 0.896, acc: 80.58 %\n",
      "[Epoch 7, Batch  3100] loss: 0.865, acc: 80.64 %\n",
      "[Epoch 7, Batch  3200] loss: 0.888, acc: 80.13 %\n",
      "[Epoch 7, Batch  3300] loss: 0.867, acc: 80.19 %\n",
      "[Epoch 7, Batch  3400] loss: 0.869, acc: 80.21 %\n",
      "[Epoch 7, Batch  3500] loss: 0.870, acc: 80.16 %\n",
      "[Epoch 7, Batch  3600] loss: 0.836, acc: 80.35 %\n",
      "[Epoch 7, Batch  3700] loss: 0.837, acc: 80.41 %\n",
      "[Epoch 7, Batch  3800] loss: 0.877, acc: 80.35 %\n",
      "[Epoch 7, Batch  3900] loss: 0.872, acc: 80.37 %\n",
      "[Epoch 7, Batch  4000] loss: 0.863, acc: 80.45 %\n",
      "[Epoch 7, Batch  4100] loss: 0.844, acc: 80.52 %\n",
      "[Epoch 7, Batch  4200] loss: 0.898, acc: 80.36 %\n",
      "[Epoch 7, Batch  4300] loss: 0.842, acc: 80.62 %\n",
      "[Epoch 7, Batch  4400] loss: 0.873, acc: 80.50 %\n",
      "[Epoch 7, Batch  4500] loss: 0.868, acc: 80.48 %\n",
      "[Epoch 7, Batch  4600] loss: 0.887, acc: 80.47 %\n",
      "[Epoch 7, Batch  4700] loss: 0.872, acc: 80.53 %\n",
      "[Epoch 7, Batch  4800] loss: 0.866, acc: 80.48 %\n",
      "[Epoch 7, Batch  4900] loss: 0.856, acc: 80.53 %\n",
      "[Epoch 7, Batch  5000] loss: 0.894, acc: 80.51 %\n",
      "[Epoch 7, Batch  5100] loss: 0.845, acc: 81.47 %\n",
      "[Epoch 7, Batch  5200] loss: 0.855, acc: 81.16 %\n",
      "[Epoch 7, Batch  5300] loss: 0.870, acc: 80.93 %\n",
      "[Epoch 7, Batch  5400] loss: 0.852, acc: 80.84 %\n",
      "[Epoch 7, Batch  5500] loss: 0.874, acc: 80.68 %\n",
      "[Epoch 7, Batch  5600] loss: 0.858, acc: 80.55 %\n",
      "[Epoch 7, Batch  5700] loss: 0.842, acc: 80.61 %\n",
      "[Epoch 7, Batch  5800] loss: 0.865, acc: 80.55 %\n",
      "[Epoch 7, Batch  5900] loss: 0.837, acc: 80.58 %\n",
      "[Epoch 7, Batch  6000] loss: 0.863, acc: 80.57 %\n",
      "[Epoch 7, Batch  6100] loss: 0.856, acc: 80.84 %\n",
      "[Epoch 7, Batch  6200] loss: 0.864, acc: 80.66 %\n",
      "[Epoch 7, Batch  6300] loss: 0.843, acc: 80.59 %\n",
      "[Epoch 7, Batch  6400] loss: 0.839, acc: 80.70 %\n",
      "[Epoch 7, Batch  6500] loss: 0.833, acc: 80.68 %\n",
      "[Epoch 7, Batch  6600] loss: 0.840, acc: 80.76 %\n",
      "[Epoch 7, Batch  6700] loss: 0.865, acc: 80.71 %\n",
      "[Epoch 7, Batch  6800] loss: 0.859, acc: 80.66 %\n",
      "[Epoch 7, Batch  6900] loss: 0.891, acc: 80.60 %\n",
      "[Epoch 7, Batch  7000] loss: 0.850, acc: 80.57 %\n",
      "[Epoch 7, Batch  7100] loss: 0.828, acc: 81.12 %\n",
      "[Epoch 7, Batch  7200] loss: 0.883, acc: 80.47 %\n",
      "[Epoch 7, Batch  7300] loss: 0.884, acc: 80.21 %\n",
      "[Epoch 7, Batch  7400] loss: 0.849, acc: 80.39 %\n",
      "[Epoch 7, Batch  7500] loss: 0.922, acc: 80.27 %\n",
      "[Epoch 7, Batch  7600] loss: 0.864, acc: 80.33 %\n",
      "[Epoch 7, Batch  7700] loss: 0.870, acc: 80.37 %\n",
      "[Epoch 7, Batch  7800] loss: 0.856, acc: 80.38 %\n",
      "[Epoch 7, Batch  7900] loss: 0.850, acc: 80.37 %\n",
      "[Epoch 7, Batch  8000] loss: 0.860, acc: 80.39 %\n",
      "[Epoch 7, Batch  8100] loss: 0.854, acc: 80.28 %\n",
      "[Epoch 7, Batch  8200] loss: 0.886, acc: 79.96 %\n",
      "[Epoch 7, Batch  8300] loss: 0.852, acc: 80.08 %\n",
      "[Epoch 7, Batch  8400] loss: 0.856, acc: 80.13 %\n",
      "[Epoch 7, Batch  8500] loss: 0.848, acc: 80.24 %\n",
      "[Epoch 7, Batch  8600] loss: 0.831, acc: 80.45 %\n",
      "[Epoch 7, Batch  8700] loss: 0.853, acc: 80.52 %\n",
      "[Epoch 7, Batch  8800] loss: 0.839, acc: 80.52 %\n",
      "[Epoch 7, Batch  8900] loss: 0.827, acc: 80.56 %\n",
      "[Epoch 7, Batch  9000] loss: 0.849, acc: 80.55 %\n",
      "[Epoch 7, Batch  9100] loss: 0.853, acc: 80.92 %\n",
      "[Epoch 7, Batch  9200] loss: 0.811, acc: 81.19 %\n",
      "[Epoch 7, Batch  9300] loss: 0.875, acc: 80.99 %\n",
      "[Epoch 7, Batch  9400] loss: 0.866, acc: 80.78 %\n",
      "[Epoch 7, Batch  9500] loss: 0.862, acc: 80.72 %\n",
      "[Epoch 7, Batch  9600] loss: 0.844, acc: 80.61 %\n",
      "[Epoch 7, Batch  9700] loss: 0.859, acc: 80.59 %\n",
      "[Epoch 7, Batch  9800] loss: 0.855, acc: 80.58 %\n",
      "[Epoch 7, Batch  9900] loss: 0.846, acc: 80.63 %\n",
      "[Epoch 7, Batch 10000] loss: 0.877, acc: 80.57 %\n",
      "[Epoch 7, Batch 10100] loss: 0.818, acc: 81.06 %\n",
      "[Epoch 7, Batch 10200] loss: 0.873, acc: 80.69 %\n",
      "[Epoch 7, Batch 10300] loss: 0.818, acc: 80.88 %\n",
      "[Epoch 7, Batch 10400] loss: 0.801, acc: 81.11 %\n",
      "[Epoch 7, Batch 10500] loss: 0.815, acc: 81.16 %\n",
      "[Epoch 7, Batch 10600] loss: 0.827, acc: 81.12 %\n",
      "[Epoch 7, Batch 10700] loss: 0.892, acc: 80.91 %\n",
      "[Epoch 7, Batch 10800] loss: 0.876, acc: 80.81 %\n",
      "[Epoch 7, Batch 10900] loss: 0.848, acc: 80.84 %\n",
      "[Epoch 7, Batch 11000] loss: 0.815, acc: 80.90 %\n",
      "[Epoch 7, Batch 11100] loss: 0.867, acc: 80.09 %\n",
      "[Epoch 7, Batch 11200] loss: 0.836, acc: 80.41 %\n",
      "[Epoch 7, Batch 11300] loss: 0.856, acc: 80.24 %\n",
      "[Epoch 7, Batch 11400] loss: 0.865, acc: 80.20 %\n",
      "[Epoch 7, Batch 11500] loss: 0.867, acc: 80.19 %\n",
      "[Epoch 7, Batch 11600] loss: 0.874, acc: 80.27 %\n",
      "[Epoch 7, Batch 11700] loss: 0.852, acc: 80.28 %\n",
      "[Epoch 7, Batch 11800] loss: 0.871, acc: 80.25 %\n",
      "[Epoch 7, Batch 11900] loss: 0.829, acc: 80.37 %\n",
      "[Epoch 7, Batch 12000] loss: 0.857, acc: 80.36 %\n",
      "[Epoch 7, Batch 12100] loss: 0.875, acc: 79.83 %\n",
      "[Epoch 7, Batch 12200] loss: 0.886, acc: 79.93 %\n",
      "[Epoch 7, Batch 12300] loss: 0.834, acc: 80.33 %\n",
      "[Epoch 7, Batch 12400] loss: 0.848, acc: 80.28 %\n",
      "[Epoch 7, Batch 12500] loss: 0.890, acc: 80.21 %\n",
      "[Epoch 7, Batch 12600] loss: 0.848, acc: 80.20 %\n",
      "[Epoch 7, Batch 12700] loss: 0.830, acc: 80.33 %\n",
      "[Epoch 7, Batch 12800] loss: 0.831, acc: 80.38 %\n",
      "[Epoch 7, Batch 12900] loss: 0.856, acc: 80.36 %\n",
      "[Epoch 7, Batch 13000] loss: 0.845, acc: 80.42 %\n",
      "[Epoch 7, Batch 13100] loss: 0.882, acc: 79.75 %\n",
      "[Epoch 7, Batch 13200] loss: 0.883, acc: 79.62 %\n",
      "[Epoch 7, Batch 13300] loss: 0.899, acc: 79.88 %\n",
      "[Epoch 7, Batch 13400] loss: 0.870, acc: 79.96 %\n",
      "[Epoch 7, Batch 13500] loss: 0.864, acc: 79.96 %\n",
      "[Epoch 7, Batch 13600] loss: 0.821, acc: 80.15 %\n",
      "[Epoch 7, Batch 13700] loss: 0.862, acc: 80.15 %\n",
      "[Epoch 7, Batch 13800] loss: 0.848, acc: 80.21 %\n",
      "[Epoch 7, Batch 13900] loss: 0.828, acc: 80.30 %\n",
      "[Epoch 7, Batch 14000] loss: 0.867, acc: 80.24 %\n",
      "[Epoch 7, Batch 14100] loss: 0.859, acc: 80.41 %\n",
      "[Epoch 7, Batch 14200] loss: 0.877, acc: 80.38 %\n",
      "[Epoch 7, Batch 14300] loss: 0.830, acc: 80.56 %\n",
      "[Epoch 7, Batch 14400] loss: 0.834, acc: 80.67 %\n",
      "[Epoch 7, Batch 14500] loss: 0.845, acc: 80.72 %\n",
      "[Epoch 7, Batch 14600] loss: 0.832, acc: 80.69 %\n",
      "[Epoch 7, Batch 14700] loss: 0.847, acc: 80.67 %\n",
      "[Epoch 7, Batch 14800] loss: 0.846, acc: 80.65 %\n",
      "[Epoch 7, Batch 14900] loss: 0.886, acc: 80.56 %\n",
      "[Epoch 7, Batch 15000] loss: 0.870, acc: 80.50 %\n",
      "[Epoch 7, Batch 15100] loss: 0.858, acc: 80.78 %\n",
      "[Epoch 7, Batch 15200] loss: 0.840, acc: 80.62 %\n",
      "[Epoch 7, Batch 15300] loss: 0.845, acc: 80.69 %\n",
      "[Epoch 7, Batch 15400] loss: 0.865, acc: 80.66 %\n",
      "[Epoch 7, Batch 15500] loss: 0.849, acc: 80.71 %\n",
      "[Epoch 7, Batch 15600] loss: 0.859, acc: 80.70 %\n",
      "[Epoch 7, Batch 15700] loss: 0.850, acc: 80.68 %\n",
      "[Epoch 7, Batch 15800] loss: 0.835, acc: 80.72 %\n",
      "[Epoch 7, Batch 15900] loss: 0.836, acc: 80.75 %\n",
      "[Epoch 7, Batch 16000] loss: 0.866, acc: 80.73 %\n",
      "[Epoch 7, Batch 16100] loss: 0.807, acc: 81.56 %\n",
      "[Epoch 7, Batch 16200] loss: 0.813, acc: 81.28 %\n",
      "[Epoch 7, Batch 16300] loss: 0.836, acc: 81.12 %\n",
      "[Epoch 7, Batch 16400] loss: 0.850, acc: 80.88 %\n",
      "[Epoch 7, Batch 16500] loss: 0.842, acc: 80.81 %\n",
      "[Epoch 7, Batch 16600] loss: 0.856, acc: 80.72 %\n",
      "[Epoch 7, Batch 16700] loss: 0.875, acc: 80.58 %\n",
      "[Epoch 7, Batch 16800] loss: 0.856, acc: 80.64 %\n",
      "[Epoch 7, Batch 16900] loss: 0.865, acc: 80.56 %\n",
      "[Epoch 7, Batch 17000] loss: 0.861, acc: 80.56 %\n",
      "[Epoch 7, Batch 17100] loss: 0.864, acc: 80.53 %\n",
      "[Epoch 7, Batch 17200] loss: 0.869, acc: 80.30 %\n",
      "[Epoch 7, Batch 17300] loss: 0.807, acc: 80.70 %\n",
      "[Epoch 7, Batch 17400] loss: 0.881, acc: 80.44 %\n",
      "[Epoch 7, Batch 17500] loss: 0.822, acc: 80.52 %\n",
      "[Epoch 7, Batch 17600] loss: 0.878, acc: 80.42 %\n",
      "[Epoch 7, Batch 17700] loss: 0.847, acc: 80.44 %\n",
      "[Epoch 7, Batch 17800] loss: 0.870, acc: 80.29 %\n",
      "[Epoch 7, Batch 17900] loss: 0.841, acc: 80.31 %\n",
      "[Epoch 7, Batch 18000] loss: 0.838, acc: 80.39 %\n",
      "[Epoch 7, Batch 18100] loss: 0.844, acc: 80.69 %\n",
      "[Epoch 7, Batch 18200] loss: 0.839, acc: 80.99 %\n",
      "[Epoch 7, Batch 18300] loss: 0.879, acc: 80.52 %\n",
      "[Epoch 7, Batch 18400] loss: 0.847, acc: 80.48 %\n",
      "[Epoch 7, Batch 18500] loss: 0.864, acc: 80.41 %\n",
      "[Epoch 7, Batch 18600] loss: 0.841, acc: 80.47 %\n",
      "[Epoch 7, Batch 18700] loss: 0.867, acc: 80.44 %\n",
      "[Epoch 7, Batch 18800] loss: 0.830, acc: 80.52 %\n",
      "[Epoch 7, Batch 18900] loss: 0.887, acc: 80.38 %\n",
      "[Epoch 7, Batch 19000] loss: 0.817, acc: 80.43 %\n",
      "[Epoch 7, Batch 19100] loss: 0.853, acc: 80.59 %\n",
      "[Epoch 7, Batch 19200] loss: 0.861, acc: 80.24 %\n",
      "[Epoch 7, Batch 19300] loss: 0.868, acc: 80.38 %\n",
      "[Epoch 7, Batch 19400] loss: 0.863, acc: 80.36 %\n",
      "[Epoch 7, Batch 19500] loss: 0.864, acc: 80.36 %\n",
      "[Epoch 7, Batch 19600] loss: 0.827, acc: 80.46 %\n",
      "[Epoch 7, Batch 19700] loss: 0.819, acc: 80.65 %\n",
      "[Epoch 7, Batch 19800] loss: 0.881, acc: 80.56 %\n",
      "[Epoch 7, Batch 19900] loss: 0.829, acc: 80.62 %\n",
      "[Epoch 7, Batch 20000] loss: 0.817, acc: 80.67 %\n",
      "****** Model checkpoint saved at epochs 8 ******\n",
      "epoch 9 learning rate : 3.338261212717716e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2bfc1e39914d5288fb2112c033f858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8, Batch   100] loss: 0.836, acc: 81.14 %\n",
      "[Epoch 8, Batch   200] loss: 0.813, acc: 81.08 %\n",
      "[Epoch 8, Batch   300] loss: 0.833, acc: 81.07 %\n",
      "[Epoch 8, Batch   400] loss: 0.834, acc: 80.95 %\n",
      "[Epoch 8, Batch   500] loss: 0.830, acc: 80.91 %\n",
      "[Epoch 8, Batch   600] loss: 0.812, acc: 81.03 %\n",
      "[Epoch 8, Batch   700] loss: 0.858, acc: 80.93 %\n",
      "[Epoch 8, Batch   800] loss: 0.807, acc: 80.97 %\n",
      "[Epoch 8, Batch   900] loss: 0.831, acc: 80.96 %\n",
      "[Epoch 8, Batch  1000] loss: 0.840, acc: 80.93 %\n",
      "[Epoch 8, Batch  1100] loss: 0.821, acc: 81.02 %\n",
      "[Epoch 8, Batch  1200] loss: 0.833, acc: 81.04 %\n",
      "[Epoch 8, Batch  1300] loss: 0.812, acc: 81.15 %\n",
      "[Epoch 8, Batch  1400] loss: 0.857, acc: 80.95 %\n",
      "[Epoch 8, Batch  1500] loss: 0.829, acc: 81.02 %\n",
      "[Epoch 8, Batch  1600] loss: 0.830, acc: 80.92 %\n",
      "[Epoch 8, Batch  1700] loss: 0.838, acc: 80.85 %\n",
      "[Epoch 8, Batch  1800] loss: 0.823, acc: 80.88 %\n",
      "[Epoch 8, Batch  1900] loss: 0.841, acc: 80.86 %\n",
      "[Epoch 8, Batch  2000] loss: 0.821, acc: 80.86 %\n",
      "[Epoch 8, Batch  2100] loss: 0.825, acc: 81.25 %\n",
      "[Epoch 8, Batch  2200] loss: 0.841, acc: 80.92 %\n",
      "[Epoch 8, Batch  2300] loss: 0.865, acc: 80.82 %\n",
      "[Epoch 8, Batch  2400] loss: 0.791, acc: 81.11 %\n",
      "[Epoch 8, Batch  2500] loss: 0.792, acc: 81.13 %\n",
      "[Epoch 8, Batch  2600] loss: 0.841, acc: 81.14 %\n",
      "[Epoch 8, Batch  2700] loss: 0.843, acc: 81.10 %\n",
      "[Epoch 8, Batch  2800] loss: 0.843, acc: 81.11 %\n",
      "[Epoch 8, Batch  2900] loss: 0.860, acc: 81.00 %\n",
      "[Epoch 8, Batch  3000] loss: 0.845, acc: 80.95 %\n",
      "[Epoch 8, Batch  3100] loss: 0.840, acc: 80.66 %\n",
      "[Epoch 8, Batch  3200] loss: 0.859, acc: 80.25 %\n",
      "[Epoch 8, Batch  3300] loss: 0.812, acc: 80.43 %\n",
      "[Epoch 8, Batch  3400] loss: 0.844, acc: 80.44 %\n",
      "[Epoch 8, Batch  3500] loss: 0.856, acc: 80.36 %\n",
      "[Epoch 8, Batch  3600] loss: 0.815, acc: 80.44 %\n",
      "[Epoch 8, Batch  3700] loss: 0.844, acc: 80.52 %\n",
      "[Epoch 8, Batch  3800] loss: 0.817, acc: 80.61 %\n",
      "[Epoch 8, Batch  3900] loss: 0.837, acc: 80.60 %\n",
      "[Epoch 8, Batch  4000] loss: 0.848, acc: 80.61 %\n",
      "[Epoch 8, Batch  4100] loss: 0.890, acc: 79.91 %\n",
      "[Epoch 8, Batch  4200] loss: 0.830, acc: 80.41 %\n",
      "[Epoch 8, Batch  4300] loss: 0.828, acc: 80.59 %\n",
      "[Epoch 8, Batch  4400] loss: 0.848, acc: 80.66 %\n",
      "[Epoch 8, Batch  4500] loss: 0.819, acc: 80.81 %\n",
      "[Epoch 8, Batch  4600] loss: 0.815, acc: 80.91 %\n",
      "[Epoch 8, Batch  4700] loss: 0.801, acc: 81.02 %\n",
      "[Epoch 8, Batch  4800] loss: 0.827, acc: 81.06 %\n",
      "[Epoch 8, Batch  4900] loss: 0.846, acc: 81.04 %\n",
      "[Epoch 8, Batch  5000] loss: 0.867, acc: 80.98 %\n",
      "[Epoch 8, Batch  5100] loss: 0.862, acc: 80.41 %\n",
      "[Epoch 8, Batch  5200] loss: 0.847, acc: 80.48 %\n",
      "[Epoch 8, Batch  5300] loss: 0.833, acc: 80.63 %\n",
      "[Epoch 8, Batch  5400] loss: 0.821, acc: 80.72 %\n",
      "[Epoch 8, Batch  5500] loss: 0.833, acc: 80.80 %\n",
      "[Epoch 8, Batch  5600] loss: 0.807, acc: 80.85 %\n",
      "[Epoch 8, Batch  5700] loss: 0.834, acc: 80.91 %\n",
      "[Epoch 8, Batch  5800] loss: 0.811, acc: 80.93 %\n",
      "[Epoch 8, Batch  5900] loss: 0.884, acc: 80.84 %\n",
      "[Epoch 8, Batch  6000] loss: 0.845, acc: 80.87 %\n",
      "[Epoch 8, Batch  6100] loss: 0.843, acc: 80.03 %\n",
      "[Epoch 8, Batch  6200] loss: 0.817, acc: 80.56 %\n",
      "[Epoch 8, Batch  6300] loss: 0.835, acc: 80.71 %\n",
      "[Epoch 8, Batch  6400] loss: 0.833, acc: 80.74 %\n",
      "[Epoch 8, Batch  6500] loss: 0.889, acc: 80.52 %\n",
      "[Epoch 8, Batch  6600] loss: 0.824, acc: 80.47 %\n",
      "[Epoch 8, Batch  6700] loss: 0.844, acc: 80.49 %\n",
      "[Epoch 8, Batch  6800] loss: 0.873, acc: 80.37 %\n",
      "[Epoch 8, Batch  6900] loss: 0.841, acc: 80.37 %\n",
      "[Epoch 8, Batch  7000] loss: 0.884, acc: 80.29 %\n",
      "[Epoch 8, Batch  7100] loss: 0.850, acc: 80.14 %\n",
      "[Epoch 8, Batch  7200] loss: 0.811, acc: 80.71 %\n",
      "[Epoch 8, Batch  7300] loss: 0.844, acc: 80.68 %\n",
      "[Epoch 8, Batch  7400] loss: 0.829, acc: 80.77 %\n",
      "[Epoch 8, Batch  7500] loss: 0.825, acc: 80.82 %\n",
      "[Epoch 8, Batch  7600] loss: 0.803, acc: 80.85 %\n",
      "[Epoch 8, Batch  7700] loss: 0.809, acc: 80.80 %\n",
      "[Epoch 8, Batch  7800] loss: 0.802, acc: 80.89 %\n",
      "[Epoch 8, Batch  7900] loss: 0.844, acc: 80.86 %\n",
      "[Epoch 8, Batch  8000] loss: 0.854, acc: 80.83 %\n",
      "[Epoch 8, Batch  8100] loss: 0.866, acc: 80.02 %\n",
      "[Epoch 8, Batch  8200] loss: 0.834, acc: 80.32 %\n",
      "[Epoch 8, Batch  8300] loss: 0.812, acc: 80.73 %\n",
      "[Epoch 8, Batch  8400] loss: 0.838, acc: 80.68 %\n",
      "[Epoch 8, Batch  8500] loss: 0.857, acc: 80.62 %\n",
      "[Epoch 8, Batch  8600] loss: 0.813, acc: 80.79 %\n",
      "[Epoch 8, Batch  8700] loss: 0.832, acc: 80.84 %\n",
      "[Epoch 8, Batch  8800] loss: 0.819, acc: 80.79 %\n",
      "[Epoch 8, Batch  8900] loss: 0.818, acc: 80.88 %\n",
      "[Epoch 8, Batch  9000] loss: 0.779, acc: 81.01 %\n",
      "[Epoch 8, Batch  9100] loss: 0.818, acc: 81.30 %\n",
      "[Epoch 8, Batch  9200] loss: 0.846, acc: 81.12 %\n",
      "[Epoch 8, Batch  9300] loss: 0.834, acc: 81.04 %\n",
      "[Epoch 8, Batch  9400] loss: 0.828, acc: 80.99 %\n",
      "[Epoch 8, Batch  9500] loss: 0.822, acc: 81.07 %\n",
      "[Epoch 8, Batch  9600] loss: 0.846, acc: 81.03 %\n",
      "[Epoch 8, Batch  9700] loss: 0.870, acc: 80.88 %\n",
      "[Epoch 8, Batch  9800] loss: 0.844, acc: 80.80 %\n",
      "[Epoch 8, Batch  9900] loss: 0.824, acc: 80.88 %\n",
      "[Epoch 8, Batch 10000] loss: 0.801, acc: 80.93 %\n",
      "[Epoch 8, Batch 10100] loss: 0.837, acc: 80.53 %\n",
      "[Epoch 8, Batch 10200] loss: 0.856, acc: 80.48 %\n",
      "[Epoch 8, Batch 10300] loss: 0.798, acc: 80.98 %\n",
      "[Epoch 8, Batch 10400] loss: 0.813, acc: 80.99 %\n",
      "[Epoch 8, Batch 10500] loss: 0.821, acc: 81.04 %\n",
      "[Epoch 8, Batch 10600] loss: 0.837, acc: 81.07 %\n",
      "[Epoch 8, Batch 10700] loss: 0.857, acc: 81.02 %\n",
      "[Epoch 8, Batch 10800] loss: 0.825, acc: 80.96 %\n",
      "[Epoch 8, Batch 10900] loss: 0.816, acc: 80.98 %\n",
      "[Epoch 8, Batch 11000] loss: 0.879, acc: 80.93 %\n",
      "[Epoch 8, Batch 11100] loss: 0.821, acc: 81.33 %\n",
      "[Epoch 8, Batch 11200] loss: 0.806, acc: 81.45 %\n",
      "[Epoch 8, Batch 11300] loss: 0.833, acc: 81.24 %\n",
      "[Epoch 8, Batch 11400] loss: 0.813, acc: 81.21 %\n",
      "[Epoch 8, Batch 11500] loss: 0.855, acc: 81.00 %\n",
      "[Epoch 8, Batch 11600] loss: 0.806, acc: 81.02 %\n",
      "[Epoch 8, Batch 11700] loss: 0.807, acc: 80.97 %\n",
      "[Epoch 8, Batch 11800] loss: 0.820, acc: 81.00 %\n",
      "[Epoch 8, Batch 11900] loss: 0.821, acc: 81.02 %\n",
      "[Epoch 8, Batch 12000] loss: 0.844, acc: 80.97 %\n",
      "[Epoch 8, Batch 12100] loss: 0.794, acc: 81.75 %\n",
      "[Epoch 8, Batch 12200] loss: 0.830, acc: 81.48 %\n",
      "[Epoch 8, Batch 12300] loss: 0.806, acc: 81.45 %\n",
      "[Epoch 8, Batch 12400] loss: 0.795, acc: 81.62 %\n",
      "[Epoch 8, Batch 12500] loss: 0.831, acc: 81.50 %\n",
      "[Epoch 8, Batch 12600] loss: 0.849, acc: 81.34 %\n",
      "[Epoch 8, Batch 12700] loss: 0.838, acc: 81.22 %\n",
      "[Epoch 8, Batch 12800] loss: 0.843, acc: 81.14 %\n",
      "[Epoch 8, Batch 12900] loss: 0.830, acc: 81.10 %\n",
      "[Epoch 8, Batch 13000] loss: 0.818, acc: 81.09 %\n",
      "[Epoch 8, Batch 13100] loss: 0.811, acc: 81.22 %\n",
      "[Epoch 8, Batch 13200] loss: 0.831, acc: 81.02 %\n",
      "[Epoch 8, Batch 13300] loss: 0.831, acc: 80.88 %\n",
      "[Epoch 8, Batch 13400] loss: 0.836, acc: 80.87 %\n",
      "[Epoch 8, Batch 13500] loss: 0.826, acc: 80.83 %\n",
      "[Epoch 8, Batch 13600] loss: 0.864, acc: 80.71 %\n",
      "[Epoch 8, Batch 13700] loss: 0.819, acc: 80.74 %\n",
      "[Epoch 8, Batch 13800] loss: 0.843, acc: 80.73 %\n",
      "[Epoch 8, Batch 13900] loss: 0.863, acc: 80.76 %\n",
      "[Epoch 8, Batch 14000] loss: 0.834, acc: 80.76 %\n",
      "[Epoch 8, Batch 14100] loss: 0.858, acc: 80.92 %\n",
      "[Epoch 8, Batch 14200] loss: 0.852, acc: 80.66 %\n",
      "[Epoch 8, Batch 14300] loss: 0.828, acc: 80.81 %\n",
      "[Epoch 8, Batch 14400] loss: 0.823, acc: 80.99 %\n",
      "[Epoch 8, Batch 14500] loss: 0.796, acc: 81.19 %\n",
      "[Epoch 8, Batch 14600] loss: 0.829, acc: 81.21 %\n",
      "[Epoch 8, Batch 14700] loss: 0.812, acc: 81.19 %\n",
      "[Epoch 8, Batch 14800] loss: 0.834, acc: 81.16 %\n",
      "[Epoch 8, Batch 14900] loss: 0.823, acc: 81.09 %\n",
      "[Epoch 8, Batch 15000] loss: 0.847, acc: 81.08 %\n",
      "[Epoch 8, Batch 15100] loss: 0.786, acc: 81.95 %\n",
      "[Epoch 8, Batch 15200] loss: 0.837, acc: 81.36 %\n",
      "[Epoch 8, Batch 15300] loss: 0.821, acc: 81.34 %\n",
      "[Epoch 8, Batch 15400] loss: 0.794, acc: 81.34 %\n",
      "[Epoch 8, Batch 15500] loss: 0.845, acc: 81.09 %\n",
      "[Epoch 8, Batch 15600] loss: 0.861, acc: 80.98 %\n",
      "[Epoch 8, Batch 15700] loss: 0.877, acc: 80.85 %\n",
      "[Epoch 8, Batch 15800] loss: 0.823, acc: 80.87 %\n",
      "[Epoch 8, Batch 15900] loss: 0.823, acc: 80.84 %\n",
      "[Epoch 8, Batch 16000] loss: 0.834, acc: 80.84 %\n",
      "[Epoch 8, Batch 16100] loss: 0.835, acc: 81.41 %\n",
      "[Epoch 8, Batch 16200] loss: 0.843, acc: 80.76 %\n",
      "[Epoch 8, Batch 16300] loss: 0.799, acc: 80.89 %\n",
      "[Epoch 8, Batch 16400] loss: 0.836, acc: 80.93 %\n",
      "[Epoch 8, Batch 16500] loss: 0.828, acc: 80.93 %\n",
      "[Epoch 8, Batch 16600] loss: 0.812, acc: 80.89 %\n",
      "[Epoch 8, Batch 16700] loss: 0.802, acc: 80.94 %\n",
      "[Epoch 8, Batch 16800] loss: 0.843, acc: 80.88 %\n",
      "[Epoch 8, Batch 16900] loss: 0.823, acc: 80.88 %\n",
      "[Epoch 8, Batch 17000] loss: 0.809, acc: 80.95 %\n",
      "[Epoch 8, Batch 17100] loss: 0.849, acc: 80.23 %\n",
      "[Epoch 8, Batch 17200] loss: 0.850, acc: 80.51 %\n",
      "[Epoch 8, Batch 17300] loss: 0.819, acc: 80.79 %\n",
      "[Epoch 8, Batch 17400] loss: 0.815, acc: 80.93 %\n",
      "[Epoch 8, Batch 17500] loss: 0.818, acc: 80.89 %\n",
      "[Epoch 8, Batch 17600] loss: 0.804, acc: 81.04 %\n",
      "[Epoch 8, Batch 17700] loss: 0.847, acc: 80.97 %\n",
      "[Epoch 8, Batch 17800] loss: 0.832, acc: 80.94 %\n",
      "[Epoch 8, Batch 17900] loss: 0.858, acc: 80.87 %\n",
      "[Epoch 8, Batch 18000] loss: 0.833, acc: 80.84 %\n",
      "[Epoch 8, Batch 18100] loss: 0.807, acc: 81.00 %\n",
      "[Epoch 8, Batch 18200] loss: 0.840, acc: 80.91 %\n",
      "[Epoch 8, Batch 18300] loss: 0.800, acc: 80.95 %\n",
      "[Epoch 8, Batch 18400] loss: 0.810, acc: 81.10 %\n",
      "[Epoch 8, Batch 18500] loss: 0.827, acc: 81.18 %\n",
      "[Epoch 8, Batch 18600] loss: 0.830, acc: 81.09 %\n",
      "[Epoch 8, Batch 18700] loss: 0.805, acc: 81.12 %\n",
      "[Epoch 8, Batch 18800] loss: 0.810, acc: 81.11 %\n",
      "[Epoch 8, Batch 18900] loss: 0.783, acc: 81.20 %\n",
      "[Epoch 8, Batch 19000] loss: 0.813, acc: 81.19 %\n",
      "[Epoch 8, Batch 19100] loss: 0.866, acc: 79.64 %\n",
      "[Epoch 8, Batch 19200] loss: 0.787, acc: 80.52 %\n",
      "[Epoch 8, Batch 19300] loss: 0.830, acc: 80.79 %\n",
      "[Epoch 8, Batch 19400] loss: 0.825, acc: 80.89 %\n",
      "[Epoch 8, Batch 19500] loss: 0.798, acc: 80.97 %\n",
      "[Epoch 8, Batch 19600] loss: 0.805, acc: 80.98 %\n",
      "[Epoch 8, Batch 19700] loss: 0.821, acc: 80.99 %\n",
      "[Epoch 8, Batch 19800] loss: 0.843, acc: 80.98 %\n",
      "[Epoch 8, Batch 19900] loss: 0.821, acc: 80.99 %\n",
      "[Epoch 8, Batch 20000] loss: 0.804, acc: 81.05 %\n",
      "****** Model checkpoint saved at epochs 9 ******\n",
      "epoch 10 learning rate : 2.9999999999999997e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4858eca071014e9e8c59b96ad7efdb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9, Batch   100] loss: 0.825, acc: 81.19 %\n",
      "[Epoch 9, Batch   200] loss: 0.804, acc: 80.98 %\n",
      "[Epoch 9, Batch   300] loss: 0.823, acc: 81.01 %\n",
      "[Epoch 9, Batch   400] loss: 0.789, acc: 81.18 %\n",
      "[Epoch 9, Batch   500] loss: 0.844, acc: 80.94 %\n",
      "[Epoch 9, Batch   600] loss: 0.816, acc: 81.03 %\n",
      "[Epoch 9, Batch   700] loss: 0.796, acc: 81.02 %\n",
      "[Epoch 9, Batch   800] loss: 0.780, acc: 81.17 %\n",
      "[Epoch 9, Batch   900] loss: 0.793, acc: 81.23 %\n",
      "[Epoch 9, Batch  1000] loss: 0.813, acc: 81.22 %\n",
      "[Epoch 9, Batch  1100] loss: 0.846, acc: 80.41 %\n",
      "[Epoch 9, Batch  1200] loss: 0.821, acc: 80.71 %\n",
      "[Epoch 9, Batch  1300] loss: 0.798, acc: 81.19 %\n",
      "[Epoch 9, Batch  1400] loss: 0.778, acc: 81.22 %\n",
      "[Epoch 9, Batch  1500] loss: 0.795, acc: 81.29 %\n",
      "[Epoch 9, Batch  1600] loss: 0.808, acc: 81.27 %\n",
      "[Epoch 9, Batch  1700] loss: 0.825, acc: 81.29 %\n",
      "[Epoch 9, Batch  1800] loss: 0.811, acc: 81.26 %\n",
      "[Epoch 9, Batch  1900] loss: 0.788, acc: 81.31 %\n",
      "[Epoch 9, Batch  2000] loss: 0.806, acc: 81.26 %\n",
      "[Epoch 9, Batch  2100] loss: 0.781, acc: 81.62 %\n",
      "[Epoch 9, Batch  2200] loss: 0.865, acc: 80.83 %\n",
      "[Epoch 9, Batch  2300] loss: 0.831, acc: 80.96 %\n",
      "[Epoch 9, Batch  2400] loss: 0.820, acc: 81.12 %\n",
      "[Epoch 9, Batch  2500] loss: 0.795, acc: 81.20 %\n",
      "[Epoch 9, Batch  2600] loss: 0.805, acc: 81.30 %\n",
      "[Epoch 9, Batch  2700] loss: 0.815, acc: 81.33 %\n",
      "[Epoch 9, Batch  2800] loss: 0.817, acc: 81.29 %\n",
      "[Epoch 9, Batch  2900] loss: 0.788, acc: 81.32 %\n",
      "[Epoch 9, Batch  3000] loss: 0.781, acc: 81.41 %\n",
      "[Epoch 9, Batch  3100] loss: 0.850, acc: 80.30 %\n",
      "[Epoch 9, Batch  3200] loss: 0.761, acc: 81.12 %\n",
      "[Epoch 9, Batch  3300] loss: 0.795, acc: 81.05 %\n",
      "[Epoch 9, Batch  3400] loss: 0.762, acc: 81.32 %\n",
      "[Epoch 9, Batch  3500] loss: 0.793, acc: 81.32 %\n",
      "[Epoch 9, Batch  3600] loss: 0.799, acc: 81.37 %\n",
      "[Epoch 9, Batch  3700] loss: 0.822, acc: 81.34 %\n",
      "[Epoch 9, Batch  3800] loss: 0.801, acc: 81.34 %\n",
      "[Epoch 9, Batch  3900] loss: 0.829, acc: 81.26 %\n",
      "[Epoch 9, Batch  4000] loss: 0.851, acc: 81.18 %\n",
      "[Epoch 9, Batch  4100] loss: 0.847, acc: 80.16 %\n",
      "[Epoch 9, Batch  4200] loss: 0.806, acc: 80.82 %\n",
      "[Epoch 9, Batch  4300] loss: 0.806, acc: 81.00 %\n",
      "[Epoch 9, Batch  4400] loss: 0.817, acc: 81.09 %\n",
      "[Epoch 9, Batch  4500] loss: 0.822, acc: 81.09 %\n",
      "[Epoch 9, Batch  4600] loss: 0.790, acc: 81.18 %\n",
      "[Epoch 9, Batch  4700] loss: 0.811, acc: 81.21 %\n",
      "[Epoch 9, Batch  4800] loss: 0.814, acc: 81.26 %\n",
      "[Epoch 9, Batch  4900] loss: 0.798, acc: 81.22 %\n",
      "[Epoch 9, Batch  5000] loss: 0.780, acc: 81.27 %\n",
      "[Epoch 9, Batch  5100] loss: 0.829, acc: 81.30 %\n",
      "[Epoch 9, Batch  5200] loss: 0.804, acc: 81.59 %\n",
      "[Epoch 9, Batch  5300] loss: 0.833, acc: 81.46 %\n",
      "[Epoch 9, Batch  5400] loss: 0.832, acc: 81.36 %\n",
      "[Epoch 9, Batch  5500] loss: 0.830, acc: 81.24 %\n",
      "[Epoch 9, Batch  5600] loss: 0.807, acc: 81.15 %\n",
      "[Epoch 9, Batch  5700] loss: 0.812, acc: 81.13 %\n",
      "[Epoch 9, Batch  5800] loss: 0.817, acc: 81.10 %\n",
      "[Epoch 9, Batch  5900] loss: 0.813, acc: 81.15 %\n",
      "[Epoch 9, Batch  6000] loss: 0.787, acc: 81.21 %\n",
      "[Epoch 9, Batch  6100] loss: 0.814, acc: 81.09 %\n",
      "[Epoch 9, Batch  6200] loss: 0.830, acc: 81.02 %\n",
      "[Epoch 9, Batch  6300] loss: 0.762, acc: 81.41 %\n",
      "[Epoch 9, Batch  6400] loss: 0.810, acc: 81.37 %\n",
      "[Epoch 9, Batch  6500] loss: 0.833, acc: 81.35 %\n",
      "[Epoch 9, Batch  6600] loss: 0.820, acc: 81.30 %\n",
      "[Epoch 9, Batch  6700] loss: 0.837, acc: 81.31 %\n",
      "[Epoch 9, Batch  6800] loss: 0.819, acc: 81.35 %\n",
      "[Epoch 9, Batch  6900] loss: 0.809, acc: 81.32 %\n",
      "[Epoch 9, Batch  7000] loss: 0.767, acc: 81.35 %\n",
      "[Epoch 9, Batch  7100] loss: 0.792, acc: 81.48 %\n",
      "[Epoch 9, Batch  7200] loss: 0.828, acc: 81.08 %\n",
      "[Epoch 9, Batch  7300] loss: 0.784, acc: 81.20 %\n",
      "[Epoch 9, Batch  7400] loss: 0.818, acc: 81.23 %\n",
      "[Epoch 9, Batch  7500] loss: 0.805, acc: 81.23 %\n",
      "[Epoch 9, Batch  7600] loss: 0.816, acc: 81.16 %\n",
      "[Epoch 9, Batch  7700] loss: 0.803, acc: 81.15 %\n",
      "[Epoch 9, Batch  7800] loss: 0.836, acc: 81.11 %\n",
      "[Epoch 9, Batch  7900] loss: 0.794, acc: 81.13 %\n",
      "[Epoch 9, Batch  8000] loss: 0.780, acc: 81.18 %\n",
      "[Epoch 9, Batch  8100] loss: 0.781, acc: 81.47 %\n",
      "[Epoch 9, Batch  8200] loss: 0.837, acc: 80.90 %\n",
      "[Epoch 9, Batch  8300] loss: 0.799, acc: 81.07 %\n",
      "[Epoch 9, Batch  8400] loss: 0.768, acc: 81.33 %\n",
      "[Epoch 9, Batch  8500] loss: 0.790, acc: 81.41 %\n",
      "[Epoch 9, Batch  8600] loss: 0.808, acc: 81.40 %\n",
      "[Epoch 9, Batch  8700] loss: 0.832, acc: 81.32 %\n",
      "[Epoch 9, Batch  8800] loss: 0.808, acc: 81.33 %\n",
      "[Epoch 9, Batch  8900] loss: 0.832, acc: 81.29 %\n",
      "[Epoch 9, Batch  9000] loss: 0.838, acc: 81.25 %\n",
      "[Epoch 9, Batch  9100] loss: 0.799, acc: 81.06 %\n",
      "[Epoch 9, Batch  9200] loss: 0.848, acc: 80.86 %\n",
      "[Epoch 9, Batch  9300] loss: 0.803, acc: 81.11 %\n",
      "[Epoch 9, Batch  9400] loss: 0.812, acc: 81.26 %\n",
      "[Epoch 9, Batch  9500] loss: 0.804, acc: 81.19 %\n",
      "[Epoch 9, Batch  9600] loss: 0.807, acc: 81.21 %\n",
      "[Epoch 9, Batch  9700] loss: 0.819, acc: 81.24 %\n",
      "[Epoch 9, Batch  9800] loss: 0.802, acc: 81.24 %\n",
      "[Epoch 9, Batch  9900] loss: 0.830, acc: 81.24 %\n",
      "[Epoch 9, Batch 10000] loss: 0.820, acc: 81.23 %\n",
      "[Epoch 9, Batch 10100] loss: 0.812, acc: 80.91 %\n",
      "[Epoch 9, Batch 10200] loss: 0.791, acc: 81.09 %\n",
      "[Epoch 9, Batch 10300] loss: 0.819, acc: 81.02 %\n",
      "[Epoch 9, Batch 10400] loss: 0.804, acc: 81.15 %\n",
      "[Epoch 9, Batch 10500] loss: 0.786, acc: 81.21 %\n",
      "[Epoch 9, Batch 10600] loss: 0.804, acc: 81.20 %\n",
      "[Epoch 9, Batch 10700] loss: 0.807, acc: 81.22 %\n",
      "[Epoch 9, Batch 10800] loss: 0.830, acc: 81.19 %\n",
      "[Epoch 9, Batch 10900] loss: 0.800, acc: 81.24 %\n",
      "[Epoch 9, Batch 11000] loss: 0.803, acc: 81.23 %\n",
      "[Epoch 9, Batch 11100] loss: 0.834, acc: 80.45 %\n",
      "[Epoch 9, Batch 11200] loss: 0.822, acc: 80.79 %\n",
      "[Epoch 9, Batch 11300] loss: 0.816, acc: 81.04 %\n",
      "[Epoch 9, Batch 11400] loss: 0.791, acc: 81.10 %\n",
      "[Epoch 9, Batch 11500] loss: 0.816, acc: 81.08 %\n",
      "[Epoch 9, Batch 11600] loss: 0.799, acc: 81.16 %\n",
      "[Epoch 9, Batch 11700] loss: 0.787, acc: 81.19 %\n",
      "[Epoch 9, Batch 11800] loss: 0.776, acc: 81.27 %\n",
      "[Epoch 9, Batch 11900] loss: 0.754, acc: 81.32 %\n",
      "[Epoch 9, Batch 12000] loss: 0.825, acc: 81.29 %\n",
      "[Epoch 9, Batch 12100] loss: 0.803, acc: 81.31 %\n",
      "[Epoch 9, Batch 12200] loss: 0.817, acc: 81.34 %\n",
      "[Epoch 9, Batch 12300] loss: 0.799, acc: 81.12 %\n",
      "[Epoch 9, Batch 12400] loss: 0.791, acc: 81.31 %\n",
      "[Epoch 9, Batch 12500] loss: 0.821, acc: 81.17 %\n",
      "[Epoch 9, Batch 12600] loss: 0.841, acc: 81.06 %\n",
      "[Epoch 9, Batch 12700] loss: 0.814, acc: 81.04 %\n",
      "[Epoch 9, Batch 12800] loss: 0.804, acc: 81.09 %\n",
      "[Epoch 9, Batch 12900] loss: 0.807, acc: 81.13 %\n",
      "[Epoch 9, Batch 13000] loss: 0.819, acc: 81.13 %\n",
      "[Epoch 9, Batch 13100] loss: 0.810, acc: 80.80 %\n",
      "[Epoch 9, Batch 13200] loss: 0.823, acc: 80.70 %\n",
      "[Epoch 9, Batch 13300] loss: 0.818, acc: 80.85 %\n",
      "[Epoch 9, Batch 13400] loss: 0.795, acc: 80.98 %\n",
      "[Epoch 9, Batch 13500] loss: 0.812, acc: 81.03 %\n",
      "[Epoch 9, Batch 13600] loss: 0.800, acc: 81.10 %\n",
      "[Epoch 9, Batch 13700] loss: 0.770, acc: 81.25 %\n",
      "[Epoch 9, Batch 13800] loss: 0.824, acc: 81.21 %\n",
      "[Epoch 9, Batch 13900] loss: 0.810, acc: 81.20 %\n",
      "[Epoch 9, Batch 14000] loss: 0.805, acc: 81.19 %\n",
      "[Epoch 9, Batch 14100] loss: 0.818, acc: 81.11 %\n",
      "[Epoch 9, Batch 14200] loss: 0.826, acc: 81.03 %\n",
      "[Epoch 9, Batch 14300] loss: 0.784, acc: 81.32 %\n",
      "[Epoch 9, Batch 14400] loss: 0.836, acc: 81.23 %\n",
      "[Epoch 9, Batch 14500] loss: 0.796, acc: 81.24 %\n",
      "[Epoch 9, Batch 14600] loss: 0.822, acc: 81.28 %\n",
      "[Epoch 9, Batch 14700] loss: 0.810, acc: 81.17 %\n",
      "[Epoch 9, Batch 14800] loss: 0.786, acc: 81.14 %\n",
      "[Epoch 9, Batch 14900] loss: 0.813, acc: 81.18 %\n",
      "[Epoch 9, Batch 15000] loss: 0.815, acc: 81.14 %\n",
      "[Epoch 9, Batch 15100] loss: 0.817, acc: 81.03 %\n",
      "[Epoch 9, Batch 15200] loss: 0.805, acc: 81.04 %\n",
      "[Epoch 9, Batch 15300] loss: 0.783, acc: 81.24 %\n",
      "[Epoch 9, Batch 15400] loss: 0.835, acc: 81.21 %\n",
      "[Epoch 9, Batch 15500] loss: 0.774, acc: 81.32 %\n",
      "[Epoch 9, Batch 15600] loss: 0.810, acc: 81.26 %\n",
      "[Epoch 9, Batch 15700] loss: 0.801, acc: 81.22 %\n",
      "[Epoch 9, Batch 15800] loss: 0.817, acc: 81.19 %\n",
      "[Epoch 9, Batch 15900] loss: 0.802, acc: 81.25 %\n",
      "[Epoch 9, Batch 16000] loss: 0.810, acc: 81.26 %\n",
      "[Epoch 9, Batch 16100] loss: 0.805, acc: 81.39 %\n",
      "[Epoch 9, Batch 16200] loss: 0.766, acc: 81.62 %\n",
      "[Epoch 9, Batch 16300] loss: 0.788, acc: 81.52 %\n",
      "[Epoch 9, Batch 16400] loss: 0.801, acc: 81.50 %\n",
      "[Epoch 9, Batch 16500] loss: 0.805, acc: 81.42 %\n",
      "[Epoch 9, Batch 16600] loss: 0.780, acc: 81.46 %\n",
      "[Epoch 9, Batch 16700] loss: 0.811, acc: 81.42 %\n",
      "[Epoch 9, Batch 16800] loss: 0.770, acc: 81.51 %\n",
      "[Epoch 9, Batch 16900] loss: 0.806, acc: 81.48 %\n",
      "[Epoch 9, Batch 17000] loss: 0.821, acc: 81.44 %\n",
      "[Epoch 9, Batch 17100] loss: 0.819, acc: 81.17 %\n",
      "[Epoch 9, Batch 17200] loss: 0.815, acc: 81.01 %\n",
      "[Epoch 9, Batch 17300] loss: 0.777, acc: 81.19 %\n",
      "[Epoch 9, Batch 17400] loss: 0.785, acc: 81.35 %\n",
      "[Epoch 9, Batch 17500] loss: 0.807, acc: 81.29 %\n",
      "[Epoch 9, Batch 17600] loss: 0.810, acc: 81.27 %\n",
      "[Epoch 9, Batch 17700] loss: 0.827, acc: 81.25 %\n",
      "[Epoch 9, Batch 17800] loss: 0.801, acc: 81.15 %\n",
      "[Epoch 9, Batch 17900] loss: 0.806, acc: 81.14 %\n",
      "[Epoch 9, Batch 18000] loss: 0.811, acc: 81.17 %\n",
      "[Epoch 9, Batch 18100] loss: 0.783, acc: 81.64 %\n",
      "[Epoch 9, Batch 18200] loss: 0.790, acc: 81.48 %\n",
      "[Epoch 9, Batch 18300] loss: 0.777, acc: 81.61 %\n",
      "[Epoch 9, Batch 18400] loss: 0.825, acc: 81.41 %\n",
      "[Epoch 9, Batch 18500] loss: 0.812, acc: 81.39 %\n",
      "[Epoch 9, Batch 18600] loss: 0.830, acc: 81.30 %\n",
      "[Epoch 9, Batch 18700] loss: 0.806, acc: 81.37 %\n",
      "[Epoch 9, Batch 18800] loss: 0.779, acc: 81.46 %\n",
      "[Epoch 9, Batch 18900] loss: 0.798, acc: 81.44 %\n",
      "[Epoch 9, Batch 19000] loss: 0.790, acc: 81.49 %\n",
      "[Epoch 9, Batch 19100] loss: 0.798, acc: 81.42 %\n",
      "[Epoch 9, Batch 19200] loss: 0.824, acc: 81.30 %\n",
      "[Epoch 9, Batch 19300] loss: 0.815, acc: 81.05 %\n",
      "[Epoch 9, Batch 19400] loss: 0.774, acc: 81.35 %\n",
      "[Epoch 9, Batch 19500] loss: 0.811, acc: 81.28 %\n",
      "[Epoch 9, Batch 19600] loss: 0.814, acc: 81.33 %\n",
      "[Epoch 9, Batch 19700] loss: 0.813, acc: 81.23 %\n",
      "[Epoch 9, Batch 19800] loss: 0.812, acc: 81.24 %\n",
      "[Epoch 9, Batch 19900] loss: 0.788, acc: 81.26 %\n",
      "[Epoch 9, Batch 20000] loss: 0.827, acc: 81.21 %\n",
      "****** Model checkpoint saved at epochs 10 ******\n",
      "epoch 11 learning rate : 2.6180339887498946e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1309c428834d5cbbfbfc9621e2bdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10, Batch   100] loss: 0.803, acc: 81.52 %\n",
      "[Epoch 10, Batch   200] loss: 0.766, acc: 81.72 %\n",
      "[Epoch 10, Batch   300] loss: 0.781, acc: 81.60 %\n",
      "[Epoch 10, Batch   400] loss: 0.812, acc: 81.44 %\n",
      "[Epoch 10, Batch   500] loss: 0.813, acc: 81.46 %\n",
      "[Epoch 10, Batch   600] loss: 0.786, acc: 81.45 %\n",
      "[Epoch 10, Batch   700] loss: 0.786, acc: 81.44 %\n",
      "[Epoch 10, Batch   800] loss: 0.805, acc: 81.49 %\n",
      "[Epoch 10, Batch   900] loss: 0.771, acc: 81.53 %\n",
      "[Epoch 10, Batch  1000] loss: 0.780, acc: 81.59 %\n",
      "[Epoch 10, Batch  1100] loss: 0.815, acc: 81.47 %\n",
      "[Epoch 10, Batch  1200] loss: 0.806, acc: 81.32 %\n",
      "[Epoch 10, Batch  1300] loss: 0.801, acc: 81.11 %\n",
      "[Epoch 10, Batch  1400] loss: 0.783, acc: 81.46 %\n",
      "[Epoch 10, Batch  1500] loss: 0.803, acc: 81.44 %\n",
      "[Epoch 10, Batch  1600] loss: 0.803, acc: 81.38 %\n",
      "[Epoch 10, Batch  1700] loss: 0.824, acc: 81.26 %\n",
      "[Epoch 10, Batch  1800] loss: 0.842, acc: 81.23 %\n",
      "[Epoch 10, Batch  1900] loss: 0.780, acc: 81.29 %\n",
      "[Epoch 10, Batch  2000] loss: 0.772, acc: 81.36 %\n",
      "[Epoch 10, Batch  2100] loss: 0.788, acc: 81.12 %\n",
      "[Epoch 10, Batch  2200] loss: 0.804, acc: 81.39 %\n",
      "[Epoch 10, Batch  2300] loss: 0.805, acc: 81.38 %\n",
      "[Epoch 10, Batch  2400] loss: 0.820, acc: 81.24 %\n",
      "[Epoch 10, Batch  2500] loss: 0.759, acc: 81.41 %\n",
      "[Epoch 10, Batch  2600] loss: 0.790, acc: 81.42 %\n",
      "[Epoch 10, Batch  2700] loss: 0.828, acc: 81.33 %\n",
      "[Epoch 10, Batch  2800] loss: 0.773, acc: 81.35 %\n",
      "[Epoch 10, Batch  2900] loss: 0.772, acc: 81.39 %\n",
      "[Epoch 10, Batch  3000] loss: 0.787, acc: 81.52 %\n",
      "[Epoch 10, Batch  3100] loss: 0.822, acc: 80.38 %\n",
      "[Epoch 10, Batch  3200] loss: 0.778, acc: 81.12 %\n",
      "[Epoch 10, Batch  3300] loss: 0.815, acc: 81.15 %\n",
      "[Epoch 10, Batch  3400] loss: 0.800, acc: 81.14 %\n",
      "[Epoch 10, Batch  3500] loss: 0.782, acc: 81.30 %\n",
      "[Epoch 10, Batch  3600] loss: 0.782, acc: 81.36 %\n",
      "[Epoch 10, Batch  3700] loss: 0.801, acc: 81.36 %\n",
      "[Epoch 10, Batch  3800] loss: 0.830, acc: 81.31 %\n",
      "[Epoch 10, Batch  3900] loss: 0.762, acc: 81.39 %\n",
      "[Epoch 10, Batch  4000] loss: 0.824, acc: 81.38 %\n",
      "[Epoch 10, Batch  4100] loss: 0.817, acc: 81.09 %\n",
      "[Epoch 10, Batch  4200] loss: 0.806, acc: 81.30 %\n",
      "[Epoch 10, Batch  4300] loss: 0.794, acc: 81.44 %\n",
      "[Epoch 10, Batch  4400] loss: 0.807, acc: 81.28 %\n",
      "[Epoch 10, Batch  4500] loss: 0.789, acc: 81.38 %\n",
      "[Epoch 10, Batch  4600] loss: 0.750, acc: 81.59 %\n",
      "[Epoch 10, Batch  4700] loss: 0.797, acc: 81.57 %\n",
      "[Epoch 10, Batch  4800] loss: 0.804, acc: 81.57 %\n",
      "[Epoch 10, Batch  4900] loss: 0.809, acc: 81.53 %\n",
      "[Epoch 10, Batch  5000] loss: 0.817, acc: 81.49 %\n",
      "[Epoch 10, Batch  5100] loss: 0.745, acc: 81.98 %\n",
      "[Epoch 10, Batch  5200] loss: 0.808, acc: 82.05 %\n",
      "[Epoch 10, Batch  5300] loss: 0.793, acc: 82.02 %\n",
      "[Epoch 10, Batch  5400] loss: 0.772, acc: 82.09 %\n",
      "[Epoch 10, Batch  5500] loss: 0.764, acc: 82.20 %\n",
      "[Epoch 10, Batch  5600] loss: 0.807, acc: 82.11 %\n",
      "[Epoch 10, Batch  5700] loss: 0.743, acc: 82.11 %\n",
      "[Epoch 10, Batch  5800] loss: 0.799, acc: 82.02 %\n",
      "[Epoch 10, Batch  5900] loss: 0.787, acc: 82.01 %\n",
      "[Epoch 10, Batch  6000] loss: 0.766, acc: 82.02 %\n",
      "[Epoch 10, Batch  6100] loss: 0.781, acc: 81.45 %\n",
      "[Epoch 10, Batch  6200] loss: 0.781, acc: 81.81 %\n",
      "[Epoch 10, Batch  6300] loss: 0.820, acc: 81.62 %\n",
      "[Epoch 10, Batch  6400] loss: 0.796, acc: 81.50 %\n",
      "[Epoch 10, Batch  6500] loss: 0.809, acc: 81.51 %\n",
      "[Epoch 10, Batch  6600] loss: 0.774, acc: 81.52 %\n",
      "[Epoch 10, Batch  6700] loss: 0.790, acc: 81.55 %\n",
      "[Epoch 10, Batch  6800] loss: 0.762, acc: 81.64 %\n",
      "[Epoch 10, Batch  6900] loss: 0.763, acc: 81.64 %\n",
      "[Epoch 10, Batch  7000] loss: 0.780, acc: 81.71 %\n",
      "[Epoch 10, Batch  7100] loss: 0.803, acc: 81.59 %\n",
      "[Epoch 10, Batch  7200] loss: 0.811, acc: 81.47 %\n",
      "[Epoch 10, Batch  7300] loss: 0.763, acc: 81.61 %\n",
      "[Epoch 10, Batch  7400] loss: 0.781, acc: 81.56 %\n",
      "[Epoch 10, Batch  7500] loss: 0.795, acc: 81.50 %\n",
      "[Epoch 10, Batch  7600] loss: 0.817, acc: 81.44 %\n",
      "[Epoch 10, Batch  7700] loss: 0.778, acc: 81.58 %\n",
      "[Epoch 10, Batch  7800] loss: 0.795, acc: 81.52 %\n",
      "[Epoch 10, Batch  7900] loss: 0.779, acc: 81.56 %\n",
      "[Epoch 10, Batch  8000] loss: 0.821, acc: 81.52 %\n",
      "[Epoch 10, Batch  8100] loss: 0.753, acc: 81.70 %\n",
      "[Epoch 10, Batch  8200] loss: 0.793, acc: 81.66 %\n",
      "[Epoch 10, Batch  8300] loss: 0.794, acc: 81.60 %\n",
      "[Epoch 10, Batch  8400] loss: 0.809, acc: 81.54 %\n",
      "[Epoch 10, Batch  8500] loss: 0.800, acc: 81.62 %\n",
      "[Epoch 10, Batch  8600] loss: 0.783, acc: 81.58 %\n",
      "[Epoch 10, Batch  8700] loss: 0.779, acc: 81.57 %\n",
      "[Epoch 10, Batch  8800] loss: 0.780, acc: 81.65 %\n",
      "[Epoch 10, Batch  8900] loss: 0.762, acc: 81.69 %\n",
      "[Epoch 10, Batch  9000] loss: 0.799, acc: 81.60 %\n",
      "[Epoch 10, Batch  9100] loss: 0.805, acc: 81.12 %\n",
      "[Epoch 10, Batch  9200] loss: 0.804, acc: 81.22 %\n",
      "[Epoch 10, Batch  9300] loss: 0.809, acc: 81.31 %\n",
      "[Epoch 10, Batch  9400] loss: 0.772, acc: 81.39 %\n",
      "[Epoch 10, Batch  9500] loss: 0.819, acc: 81.35 %\n",
      "[Epoch 10, Batch  9600] loss: 0.792, acc: 81.38 %\n",
      "[Epoch 10, Batch  9700] loss: 0.780, acc: 81.50 %\n",
      "[Epoch 10, Batch  9800] loss: 0.795, acc: 81.53 %\n",
      "[Epoch 10, Batch  9900] loss: 0.826, acc: 81.49 %\n",
      "[Epoch 10, Batch 10000] loss: 0.763, acc: 81.60 %\n",
      "[Epoch 10, Batch 10100] loss: 0.839, acc: 80.33 %\n",
      "[Epoch 10, Batch 10200] loss: 0.788, acc: 80.86 %\n",
      "[Epoch 10, Batch 10300] loss: 0.782, acc: 81.11 %\n",
      "[Epoch 10, Batch 10400] loss: 0.754, acc: 81.45 %\n",
      "[Epoch 10, Batch 10500] loss: 0.796, acc: 81.42 %\n",
      "[Epoch 10, Batch 10600] loss: 0.800, acc: 81.42 %\n",
      "[Epoch 10, Batch 10700] loss: 0.810, acc: 81.44 %\n",
      "[Epoch 10, Batch 10800] loss: 0.817, acc: 81.42 %\n",
      "[Epoch 10, Batch 10900] loss: 0.783, acc: 81.45 %\n",
      "[Epoch 10, Batch 11000] loss: 0.776, acc: 81.48 %\n",
      "[Epoch 10, Batch 11100] loss: 0.761, acc: 82.30 %\n",
      "[Epoch 10, Batch 11200] loss: 0.809, acc: 82.01 %\n",
      "[Epoch 10, Batch 11300] loss: 0.800, acc: 81.79 %\n",
      "[Epoch 10, Batch 11400] loss: 0.807, acc: 81.74 %\n",
      "[Epoch 10, Batch 11500] loss: 0.796, acc: 81.72 %\n",
      "[Epoch 10, Batch 11600] loss: 0.791, acc: 81.72 %\n",
      "[Epoch 10, Batch 11700] loss: 0.775, acc: 81.73 %\n",
      "[Epoch 10, Batch 11800] loss: 0.808, acc: 81.70 %\n",
      "[Epoch 10, Batch 11900] loss: 0.791, acc: 81.65 %\n",
      "[Epoch 10, Batch 12000] loss: 0.790, acc: 81.70 %\n",
      "[Epoch 10, Batch 12100] loss: 0.783, acc: 81.14 %\n",
      "[Epoch 10, Batch 12200] loss: 0.794, acc: 81.01 %\n",
      "[Epoch 10, Batch 12300] loss: 0.809, acc: 81.09 %\n",
      "[Epoch 10, Batch 12400] loss: 0.778, acc: 81.12 %\n",
      "[Epoch 10, Batch 12500] loss: 0.794, acc: 81.23 %\n",
      "[Epoch 10, Batch 12600] loss: 0.784, acc: 81.29 %\n",
      "[Epoch 10, Batch 12700] loss: 0.777, acc: 81.39 %\n",
      "[Epoch 10, Batch 12800] loss: 0.759, acc: 81.54 %\n",
      "[Epoch 10, Batch 12900] loss: 0.808, acc: 81.49 %\n",
      "[Epoch 10, Batch 13000] loss: 0.793, acc: 81.51 %\n",
      "[Epoch 10, Batch 13100] loss: 0.809, acc: 81.19 %\n",
      "[Epoch 10, Batch 13200] loss: 0.773, acc: 81.46 %\n",
      "[Epoch 10, Batch 13300] loss: 0.791, acc: 81.73 %\n",
      "[Epoch 10, Batch 13400] loss: 0.797, acc: 81.72 %\n",
      "[Epoch 10, Batch 13500] loss: 0.767, acc: 81.70 %\n",
      "[Epoch 10, Batch 13600] loss: 0.780, acc: 81.66 %\n",
      "[Epoch 10, Batch 13700] loss: 0.787, acc: 81.64 %\n",
      "[Epoch 10, Batch 13800] loss: 0.791, acc: 81.65 %\n",
      "[Epoch 10, Batch 13900] loss: 0.821, acc: 81.60 %\n",
      "[Epoch 10, Batch 14000] loss: 0.797, acc: 81.63 %\n",
      "[Epoch 10, Batch 14100] loss: 0.789, acc: 81.62 %\n",
      "[Epoch 10, Batch 14200] loss: 0.787, acc: 81.48 %\n",
      "[Epoch 10, Batch 14300] loss: 0.777, acc: 81.64 %\n",
      "[Epoch 10, Batch 14400] loss: 0.781, acc: 81.57 %\n",
      "[Epoch 10, Batch 14500] loss: 0.782, acc: 81.68 %\n",
      "[Epoch 10, Batch 14600] loss: 0.791, acc: 81.66 %\n",
      "[Epoch 10, Batch 14700] loss: 0.796, acc: 81.60 %\n",
      "[Epoch 10, Batch 14800] loss: 0.791, acc: 81.61 %\n",
      "[Epoch 10, Batch 14900] loss: 0.835, acc: 81.50 %\n",
      "[Epoch 10, Batch 15000] loss: 0.754, acc: 81.53 %\n",
      "[Epoch 10, Batch 15100] loss: 0.759, acc: 82.00 %\n",
      "[Epoch 10, Batch 15200] loss: 0.784, acc: 82.24 %\n",
      "[Epoch 10, Batch 15300] loss: 0.783, acc: 81.83 %\n",
      "[Epoch 10, Batch 15400] loss: 0.753, acc: 81.91 %\n",
      "[Epoch 10, Batch 15500] loss: 0.794, acc: 81.85 %\n",
      "[Epoch 10, Batch 15600] loss: 0.797, acc: 81.78 %\n",
      "[Epoch 10, Batch 15700] loss: 0.826, acc: 81.71 %\n",
      "[Epoch 10, Batch 15800] loss: 0.811, acc: 81.63 %\n",
      "[Epoch 10, Batch 15900] loss: 0.754, acc: 81.70 %\n",
      "[Epoch 10, Batch 16000] loss: 0.785, acc: 81.64 %\n",
      "[Epoch 10, Batch 16100] loss: 0.788, acc: 81.55 %\n",
      "[Epoch 10, Batch 16200] loss: 0.797, acc: 81.47 %\n",
      "[Epoch 10, Batch 16300] loss: 0.793, acc: 81.58 %\n",
      "[Epoch 10, Batch 16400] loss: 0.814, acc: 81.39 %\n",
      "[Epoch 10, Batch 16500] loss: 0.764, acc: 81.54 %\n",
      "[Epoch 10, Batch 16600] loss: 0.806, acc: 81.43 %\n",
      "[Epoch 10, Batch 16700] loss: 0.803, acc: 81.38 %\n",
      "[Epoch 10, Batch 16800] loss: 0.782, acc: 81.48 %\n",
      "[Epoch 10, Batch 16900] loss: 0.768, acc: 81.57 %\n",
      "[Epoch 10, Batch 17000] loss: 0.800, acc: 81.55 %\n",
      "[Epoch 10, Batch 17100] loss: 0.748, acc: 82.62 %\n",
      "[Epoch 10, Batch 17200] loss: 0.789, acc: 82.10 %\n",
      "[Epoch 10, Batch 17300] loss: 0.780, acc: 82.07 %\n",
      "[Epoch 10, Batch 17400] loss: 0.847, acc: 81.59 %\n",
      "[Epoch 10, Batch 17500] loss: 0.804, acc: 81.46 %\n",
      "[Epoch 10, Batch 17600] loss: 0.779, acc: 81.47 %\n",
      "[Epoch 10, Batch 17700] loss: 0.784, acc: 81.54 %\n",
      "[Epoch 10, Batch 17800] loss: 0.750, acc: 81.60 %\n",
      "[Epoch 10, Batch 17900] loss: 0.757, acc: 81.62 %\n",
      "[Epoch 10, Batch 18000] loss: 0.811, acc: 81.59 %\n",
      "[Epoch 10, Batch 18100] loss: 0.778, acc: 81.83 %\n",
      "[Epoch 10, Batch 18200] loss: 0.784, acc: 81.59 %\n",
      "[Epoch 10, Batch 18300] loss: 0.770, acc: 81.67 %\n",
      "[Epoch 10, Batch 18400] loss: 0.769, acc: 81.70 %\n",
      "[Epoch 10, Batch 18500] loss: 0.781, acc: 81.73 %\n",
      "[Epoch 10, Batch 18600] loss: 0.786, acc: 81.65 %\n",
      "[Epoch 10, Batch 18700] loss: 0.799, acc: 81.65 %\n",
      "[Epoch 10, Batch 18800] loss: 0.781, acc: 81.63 %\n",
      "[Epoch 10, Batch 18900] loss: 0.804, acc: 81.57 %\n",
      "[Epoch 10, Batch 19000] loss: 0.770, acc: 81.63 %\n",
      "[Epoch 10, Batch 19100] loss: 0.775, acc: 81.28 %\n",
      "[Epoch 10, Batch 19200] loss: 0.795, acc: 81.40 %\n",
      "[Epoch 10, Batch 19300] loss: 0.808, acc: 81.46 %\n",
      "[Epoch 10, Batch 19400] loss: 0.780, acc: 81.51 %\n",
      "[Epoch 10, Batch 19500] loss: 0.819, acc: 81.44 %\n",
      "[Epoch 10, Batch 19600] loss: 0.770, acc: 81.54 %\n",
      "[Epoch 10, Batch 19700] loss: 0.773, acc: 81.63 %\n",
      "[Epoch 10, Batch 19800] loss: 0.789, acc: 81.66 %\n",
      "[Epoch 10, Batch 19900] loss: 0.806, acc: 81.61 %\n",
      "[Epoch 10, Batch 20000] loss: 0.760, acc: 81.62 %\n",
      "****** Model checkpoint saved at epochs 11 ******\n",
      "epoch 12 learning rate : 2.209056926535307e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bde5b2058b4739980a29f72fa35f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11, Batch   100] loss: 0.759, acc: 82.25 %\n",
      "[Epoch 11, Batch   200] loss: 0.776, acc: 81.94 %\n",
      "[Epoch 11, Batch   300] loss: 0.755, acc: 82.21 %\n",
      "[Epoch 11, Batch   400] loss: 0.740, acc: 82.21 %\n",
      "[Epoch 11, Batch   500] loss: 0.782, acc: 82.03 %\n",
      "[Epoch 11, Batch   600] loss: 0.771, acc: 82.09 %\n",
      "[Epoch 11, Batch   700] loss: 0.804, acc: 81.94 %\n",
      "[Epoch 11, Batch   800] loss: 0.759, acc: 81.96 %\n",
      "[Epoch 11, Batch   900] loss: 0.787, acc: 81.92 %\n",
      "[Epoch 11, Batch  1000] loss: 0.801, acc: 81.86 %\n",
      "[Epoch 11, Batch  1100] loss: 0.791, acc: 81.67 %\n",
      "[Epoch 11, Batch  1200] loss: 0.752, acc: 82.21 %\n",
      "[Epoch 11, Batch  1300] loss: 0.794, acc: 82.00 %\n",
      "[Epoch 11, Batch  1400] loss: 0.757, acc: 81.95 %\n",
      "[Epoch 11, Batch  1500] loss: 0.788, acc: 81.86 %\n",
      "[Epoch 11, Batch  1600] loss: 0.809, acc: 81.71 %\n",
      "[Epoch 11, Batch  1700] loss: 0.762, acc: 81.67 %\n",
      "[Epoch 11, Batch  1800] loss: 0.771, acc: 81.71 %\n",
      "[Epoch 11, Batch  1900] loss: 0.780, acc: 81.77 %\n",
      "[Epoch 11, Batch  2000] loss: 0.809, acc: 81.74 %\n",
      "[Epoch 11, Batch  2100] loss: 0.801, acc: 81.17 %\n",
      "[Epoch 11, Batch  2200] loss: 0.788, acc: 81.44 %\n",
      "[Epoch 11, Batch  2300] loss: 0.756, acc: 81.75 %\n",
      "[Epoch 11, Batch  2400] loss: 0.780, acc: 81.70 %\n",
      "[Epoch 11, Batch  2500] loss: 0.746, acc: 81.83 %\n",
      "[Epoch 11, Batch  2600] loss: 0.755, acc: 81.92 %\n",
      "[Epoch 11, Batch  2700] loss: 0.771, acc: 81.90 %\n",
      "[Epoch 11, Batch  2800] loss: 0.760, acc: 82.01 %\n",
      "[Epoch 11, Batch  2900] loss: 0.769, acc: 81.95 %\n",
      "[Epoch 11, Batch  3000] loss: 0.799, acc: 81.93 %\n",
      "[Epoch 11, Batch  3100] loss: 0.776, acc: 81.78 %\n",
      "[Epoch 11, Batch  3200] loss: 0.746, acc: 82.34 %\n",
      "[Epoch 11, Batch  3300] loss: 0.796, acc: 82.01 %\n",
      "[Epoch 11, Batch  3400] loss: 0.786, acc: 81.90 %\n",
      "[Epoch 11, Batch  3500] loss: 0.794, acc: 81.88 %\n",
      "[Epoch 11, Batch  3600] loss: 0.782, acc: 81.75 %\n",
      "[Epoch 11, Batch  3700] loss: 0.777, acc: 81.77 %\n",
      "[Epoch 11, Batch  3800] loss: 0.768, acc: 81.84 %\n",
      "[Epoch 11, Batch  3900] loss: 0.759, acc: 81.87 %\n",
      "[Epoch 11, Batch  4000] loss: 0.784, acc: 81.89 %\n",
      "[Epoch 11, Batch  4100] loss: 0.796, acc: 81.56 %\n",
      "[Epoch 11, Batch  4200] loss: 0.795, acc: 81.47 %\n",
      "[Epoch 11, Batch  4300] loss: 0.753, acc: 81.91 %\n",
      "[Epoch 11, Batch  4400] loss: 0.786, acc: 81.91 %\n",
      "[Epoch 11, Batch  4500] loss: 0.782, acc: 81.91 %\n",
      "[Epoch 11, Batch  4600] loss: 0.804, acc: 81.90 %\n",
      "[Epoch 11, Batch  4700] loss: 0.790, acc: 81.78 %\n",
      "[Epoch 11, Batch  4800] loss: 0.791, acc: 81.68 %\n",
      "[Epoch 11, Batch  4900] loss: 0.779, acc: 81.74 %\n",
      "[Epoch 11, Batch  5000] loss: 0.789, acc: 81.77 %\n",
      "[Epoch 11, Batch  5100] loss: 0.762, acc: 81.97 %\n",
      "[Epoch 11, Batch  5200] loss: 0.775, acc: 82.07 %\n",
      "[Epoch 11, Batch  5300] loss: 0.804, acc: 81.89 %\n",
      "[Epoch 11, Batch  5400] loss: 0.727, acc: 82.00 %\n",
      "[Epoch 11, Batch  5500] loss: 0.814, acc: 81.86 %\n",
      "[Epoch 11, Batch  5600] loss: 0.759, acc: 81.94 %\n",
      "[Epoch 11, Batch  5700] loss: 0.769, acc: 81.92 %\n",
      "[Epoch 11, Batch  5800] loss: 0.799, acc: 81.80 %\n",
      "[Epoch 11, Batch  5900] loss: 0.766, acc: 81.81 %\n",
      "[Epoch 11, Batch  6000] loss: 0.806, acc: 81.80 %\n",
      "[Epoch 11, Batch  6100] loss: 0.742, acc: 82.95 %\n",
      "[Epoch 11, Batch  6200] loss: 0.751, acc: 82.56 %\n",
      "[Epoch 11, Batch  6300] loss: 0.761, acc: 82.24 %\n",
      "[Epoch 11, Batch  6400] loss: 0.805, acc: 82.09 %\n",
      "[Epoch 11, Batch  6500] loss: 0.754, acc: 82.10 %\n",
      "[Epoch 11, Batch  6600] loss: 0.787, acc: 81.99 %\n",
      "[Epoch 11, Batch  6700] loss: 0.791, acc: 81.87 %\n",
      "[Epoch 11, Batch  6800] loss: 0.793, acc: 81.83 %\n",
      "[Epoch 11, Batch  6900] loss: 0.767, acc: 81.87 %\n",
      "[Epoch 11, Batch  7000] loss: 0.788, acc: 81.84 %\n",
      "[Epoch 11, Batch  7100] loss: 0.736, acc: 82.75 %\n",
      "[Epoch 11, Batch  7200] loss: 0.792, acc: 81.81 %\n",
      "[Epoch 11, Batch  7300] loss: 0.798, acc: 81.76 %\n",
      "[Epoch 11, Batch  7400] loss: 0.775, acc: 81.79 %\n",
      "[Epoch 11, Batch  7500] loss: 0.776, acc: 81.75 %\n",
      "[Epoch 11, Batch  7600] loss: 0.801, acc: 81.70 %\n",
      "[Epoch 11, Batch  7700] loss: 0.791, acc: 81.69 %\n",
      "[Epoch 11, Batch  7800] loss: 0.752, acc: 81.74 %\n",
      "[Epoch 11, Batch  7900] loss: 0.771, acc: 81.68 %\n",
      "[Epoch 11, Batch  8000] loss: 0.784, acc: 81.68 %\n",
      "[Epoch 11, Batch  8100] loss: 0.778, acc: 81.66 %\n",
      "[Epoch 11, Batch  8200] loss: 0.789, acc: 81.53 %\n",
      "[Epoch 11, Batch  8300] loss: 0.773, acc: 81.84 %\n",
      "[Epoch 11, Batch  8400] loss: 0.757, acc: 82.00 %\n",
      "[Epoch 11, Batch  8500] loss: 0.777, acc: 81.97 %\n",
      "[Epoch 11, Batch  8600] loss: 0.779, acc: 81.97 %\n",
      "[Epoch 11, Batch  8700] loss: 0.790, acc: 81.95 %\n",
      "[Epoch 11, Batch  8800] loss: 0.797, acc: 81.91 %\n",
      "[Epoch 11, Batch  8900] loss: 0.764, acc: 81.91 %\n",
      "[Epoch 11, Batch  9000] loss: 0.782, acc: 81.88 %\n",
      "[Epoch 11, Batch  9100] loss: 0.748, acc: 82.12 %\n",
      "[Epoch 11, Batch  9200] loss: 0.801, acc: 81.91 %\n",
      "[Epoch 11, Batch  9300] loss: 0.765, acc: 82.03 %\n",
      "[Epoch 11, Batch  9400] loss: 0.772, acc: 81.99 %\n",
      "[Epoch 11, Batch  9500] loss: 0.771, acc: 81.98 %\n",
      "[Epoch 11, Batch  9600] loss: 0.753, acc: 81.89 %\n",
      "[Epoch 11, Batch  9700] loss: 0.769, acc: 81.92 %\n",
      "[Epoch 11, Batch  9800] loss: 0.787, acc: 81.89 %\n",
      "[Epoch 11, Batch  9900] loss: 0.780, acc: 81.84 %\n",
      "[Epoch 11, Batch 10000] loss: 0.802, acc: 81.83 %\n",
      "[Epoch 11, Batch 10100] loss: 0.745, acc: 82.56 %\n",
      "[Epoch 11, Batch 10200] loss: 0.771, acc: 82.31 %\n",
      "[Epoch 11, Batch 10300] loss: 0.748, acc: 82.29 %\n",
      "[Epoch 11, Batch 10400] loss: 0.791, acc: 82.09 %\n",
      "[Epoch 11, Batch 10500] loss: 0.762, acc: 82.05 %\n",
      "[Epoch 11, Batch 10600] loss: 0.783, acc: 82.03 %\n",
      "[Epoch 11, Batch 10700] loss: 0.722, acc: 82.18 %\n",
      "[Epoch 11, Batch 10800] loss: 0.770, acc: 82.14 %\n",
      "[Epoch 11, Batch 10900] loss: 0.754, acc: 82.19 %\n",
      "[Epoch 11, Batch 11000] loss: 0.759, acc: 82.15 %\n",
      "[Epoch 11, Batch 11100] loss: 0.783, acc: 81.66 %\n",
      "[Epoch 11, Batch 11200] loss: 0.771, acc: 81.77 %\n",
      "[Epoch 11, Batch 11300] loss: 0.779, acc: 81.80 %\n",
      "[Epoch 11, Batch 11400] loss: 0.783, acc: 81.73 %\n",
      "[Epoch 11, Batch 11500] loss: 0.781, acc: 81.76 %\n",
      "[Epoch 11, Batch 11600] loss: 0.785, acc: 81.69 %\n",
      "[Epoch 11, Batch 11700] loss: 0.790, acc: 81.70 %\n",
      "[Epoch 11, Batch 11800] loss: 0.780, acc: 81.67 %\n",
      "[Epoch 11, Batch 11900] loss: 0.818, acc: 81.57 %\n",
      "[Epoch 11, Batch 12000] loss: 0.779, acc: 81.60 %\n",
      "[Epoch 11, Batch 12100] loss: 0.751, acc: 82.81 %\n",
      "[Epoch 11, Batch 12200] loss: 0.747, acc: 82.52 %\n",
      "[Epoch 11, Batch 12300] loss: 0.756, acc: 82.40 %\n",
      "[Epoch 11, Batch 12400] loss: 0.764, acc: 82.30 %\n",
      "[Epoch 11, Batch 12500] loss: 0.767, acc: 82.10 %\n",
      "[Epoch 11, Batch 12600] loss: 0.766, acc: 82.08 %\n",
      "[Epoch 11, Batch 12700] loss: 0.774, acc: 82.11 %\n",
      "[Epoch 11, Batch 12800] loss: 0.749, acc: 82.10 %\n",
      "[Epoch 11, Batch 12900] loss: 0.746, acc: 82.18 %\n",
      "[Epoch 11, Batch 13000] loss: 0.784, acc: 82.10 %\n",
      "[Epoch 11, Batch 13100] loss: 0.775, acc: 81.69 %\n",
      "[Epoch 11, Batch 13200] loss: 0.760, acc: 81.81 %\n",
      "[Epoch 11, Batch 13300] loss: 0.755, acc: 81.84 %\n",
      "[Epoch 11, Batch 13400] loss: 0.767, acc: 81.96 %\n",
      "[Epoch 11, Batch 13500] loss: 0.757, acc: 82.08 %\n",
      "[Epoch 11, Batch 13600] loss: 0.749, acc: 82.19 %\n",
      "[Epoch 11, Batch 13700] loss: 0.746, acc: 82.27 %\n",
      "[Epoch 11, Batch 13800] loss: 0.736, acc: 82.26 %\n",
      "[Epoch 11, Batch 13900] loss: 0.780, acc: 82.19 %\n",
      "[Epoch 11, Batch 14000] loss: 0.789, acc: 82.16 %\n",
      "[Epoch 11, Batch 14100] loss: 0.771, acc: 81.84 %\n",
      "[Epoch 11, Batch 14200] loss: 0.760, acc: 82.19 %\n",
      "[Epoch 11, Batch 14300] loss: 0.800, acc: 82.02 %\n",
      "[Epoch 11, Batch 14400] loss: 0.732, acc: 82.18 %\n",
      "[Epoch 11, Batch 14500] loss: 0.751, acc: 82.26 %\n",
      "[Epoch 11, Batch 14600] loss: 0.767, acc: 82.25 %\n",
      "[Epoch 11, Batch 14700] loss: 0.789, acc: 82.17 %\n",
      "[Epoch 11, Batch 14800] loss: 0.770, acc: 82.11 %\n",
      "[Epoch 11, Batch 14900] loss: 0.752, acc: 82.08 %\n",
      "[Epoch 11, Batch 15000] loss: 0.769, acc: 82.06 %\n",
      "[Epoch 11, Batch 15100] loss: 0.769, acc: 82.02 %\n",
      "[Epoch 11, Batch 15200] loss: 0.779, acc: 81.88 %\n",
      "[Epoch 11, Batch 15300] loss: 0.759, acc: 81.87 %\n",
      "[Epoch 11, Batch 15400] loss: 0.805, acc: 81.72 %\n",
      "[Epoch 11, Batch 15500] loss: 0.782, acc: 81.60 %\n",
      "[Epoch 11, Batch 15600] loss: 0.798, acc: 81.57 %\n",
      "[Epoch 11, Batch 15700] loss: 0.763, acc: 81.70 %\n",
      "[Epoch 11, Batch 15800] loss: 0.767, acc: 81.75 %\n",
      "[Epoch 11, Batch 15900] loss: 0.777, acc: 81.77 %\n",
      "[Epoch 11, Batch 16000] loss: 0.780, acc: 81.74 %\n",
      "[Epoch 11, Batch 16100] loss: 0.775, acc: 81.91 %\n",
      "[Epoch 11, Batch 16200] loss: 0.795, acc: 81.33 %\n",
      "[Epoch 11, Batch 16300] loss: 0.771, acc: 81.29 %\n",
      "[Epoch 11, Batch 16400] loss: 0.772, acc: 81.41 %\n",
      "[Epoch 11, Batch 16500] loss: 0.755, acc: 81.68 %\n",
      "[Epoch 11, Batch 16600] loss: 0.743, acc: 81.73 %\n",
      "[Epoch 11, Batch 16700] loss: 0.772, acc: 81.80 %\n",
      "[Epoch 11, Batch 16800] loss: 0.770, acc: 81.84 %\n",
      "[Epoch 11, Batch 16900] loss: 0.774, acc: 81.81 %\n",
      "[Epoch 11, Batch 17000] loss: 0.763, acc: 81.87 %\n",
      "[Epoch 11, Batch 17100] loss: 0.765, acc: 82.23 %\n",
      "[Epoch 11, Batch 17200] loss: 0.799, acc: 81.63 %\n",
      "[Epoch 11, Batch 17300] loss: 0.767, acc: 81.81 %\n",
      "[Epoch 11, Batch 17400] loss: 0.802, acc: 81.77 %\n",
      "[Epoch 11, Batch 17500] loss: 0.763, acc: 81.81 %\n",
      "[Epoch 11, Batch 17600] loss: 0.777, acc: 81.82 %\n",
      "[Epoch 11, Batch 17700] loss: 0.755, acc: 81.84 %\n",
      "[Epoch 11, Batch 17800] loss: 0.757, acc: 81.90 %\n",
      "[Epoch 11, Batch 17900] loss: 0.788, acc: 81.84 %\n",
      "[Epoch 11, Batch 18000] loss: 0.771, acc: 81.84 %\n",
      "[Epoch 11, Batch 18100] loss: 0.806, acc: 81.08 %\n",
      "[Epoch 11, Batch 18200] loss: 0.775, acc: 81.54 %\n",
      "[Epoch 11, Batch 18300] loss: 0.752, acc: 81.91 %\n",
      "[Epoch 11, Batch 18400] loss: 0.802, acc: 81.93 %\n",
      "[Epoch 11, Batch 18500] loss: 0.781, acc: 81.76 %\n",
      "[Epoch 11, Batch 18600] loss: 0.774, acc: 81.74 %\n",
      "[Epoch 11, Batch 18700] loss: 0.737, acc: 81.81 %\n",
      "[Epoch 11, Batch 18800] loss: 0.751, acc: 81.96 %\n",
      "[Epoch 11, Batch 18900] loss: 0.757, acc: 81.98 %\n",
      "[Epoch 11, Batch 19000] loss: 0.773, acc: 81.95 %\n",
      "[Epoch 11, Batch 19100] loss: 0.770, acc: 81.77 %\n",
      "[Epoch 11, Batch 19200] loss: 0.768, acc: 81.82 %\n",
      "[Epoch 11, Batch 19300] loss: 0.809, acc: 81.42 %\n",
      "[Epoch 11, Batch 19400] loss: 0.781, acc: 81.39 %\n",
      "[Epoch 11, Batch 19500] loss: 0.774, acc: 81.54 %\n",
      "[Epoch 11, Batch 19600] loss: 0.802, acc: 81.43 %\n",
      "[Epoch 11, Batch 19700] loss: 0.762, acc: 81.50 %\n",
      "[Epoch 11, Batch 19800] loss: 0.777, acc: 81.55 %\n",
      "[Epoch 11, Batch 19900] loss: 0.792, acc: 81.56 %\n",
      "[Epoch 11, Batch 20000] loss: 0.767, acc: 81.58 %\n",
      "****** Model checkpoint saved at epochs 12 ******\n",
      "epoch 13 learning rate : 1.7909430734646934e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcd481ca12b4ae5b60702ff3ed383f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12, Batch   100] loss: 0.744, acc: 82.36 %\n",
      "[Epoch 12, Batch   200] loss: 0.770, acc: 82.17 %\n",
      "[Epoch 12, Batch   300] loss: 0.748, acc: 82.07 %\n",
      "[Epoch 12, Batch   400] loss: 0.760, acc: 82.12 %\n",
      "[Epoch 12, Batch   500] loss: 0.731, acc: 82.31 %\n",
      "[Epoch 12, Batch   600] loss: 0.732, acc: 82.36 %\n",
      "[Epoch 12, Batch   700] loss: 0.741, acc: 82.40 %\n",
      "[Epoch 12, Batch   800] loss: 0.729, acc: 82.47 %\n",
      "[Epoch 12, Batch   900] loss: 0.759, acc: 82.44 %\n",
      "[Epoch 12, Batch  1000] loss: 0.767, acc: 82.36 %\n",
      "[Epoch 12, Batch  1100] loss: 0.755, acc: 82.12 %\n",
      "[Epoch 12, Batch  1200] loss: 0.771, acc: 81.92 %\n",
      "[Epoch 12, Batch  1300] loss: 0.796, acc: 81.91 %\n",
      "[Epoch 12, Batch  1400] loss: 0.775, acc: 81.85 %\n",
      "[Epoch 12, Batch  1500] loss: 0.781, acc: 81.80 %\n",
      "[Epoch 12, Batch  1600] loss: 0.733, acc: 81.85 %\n",
      "[Epoch 12, Batch  1700] loss: 0.762, acc: 81.92 %\n",
      "[Epoch 12, Batch  1800] loss: 0.756, acc: 81.95 %\n",
      "[Epoch 12, Batch  1900] loss: 0.776, acc: 81.90 %\n",
      "[Epoch 12, Batch  2000] loss: 0.750, acc: 81.94 %\n",
      "[Epoch 12, Batch  2100] loss: 0.751, acc: 81.81 %\n",
      "[Epoch 12, Batch  2200] loss: 0.776, acc: 81.65 %\n",
      "[Epoch 12, Batch  2300] loss: 0.760, acc: 81.84 %\n",
      "[Epoch 12, Batch  2400] loss: 0.761, acc: 81.91 %\n",
      "[Epoch 12, Batch  2500] loss: 0.763, acc: 81.99 %\n",
      "[Epoch 12, Batch  2600] loss: 0.768, acc: 81.97 %\n",
      "[Epoch 12, Batch  2700] loss: 0.765, acc: 81.96 %\n",
      "[Epoch 12, Batch  2800] loss: 0.735, acc: 81.99 %\n",
      "[Epoch 12, Batch  2900] loss: 0.755, acc: 82.01 %\n",
      "[Epoch 12, Batch  3000] loss: 0.759, acc: 82.05 %\n",
      "[Epoch 12, Batch  3100] loss: 0.756, acc: 82.06 %\n",
      "[Epoch 12, Batch  3200] loss: 0.783, acc: 81.95 %\n",
      "[Epoch 12, Batch  3300] loss: 0.750, acc: 82.12 %\n",
      "[Epoch 12, Batch  3400] loss: 0.771, acc: 81.96 %\n",
      "[Epoch 12, Batch  3500] loss: 0.773, acc: 81.93 %\n",
      "[Epoch 12, Batch  3600] loss: 0.755, acc: 81.89 %\n",
      "[Epoch 12, Batch  3700] loss: 0.790, acc: 81.79 %\n",
      "[Epoch 12, Batch  3800] loss: 0.794, acc: 81.71 %\n",
      "[Epoch 12, Batch  3900] loss: 0.777, acc: 81.68 %\n",
      "[Epoch 12, Batch  4000] loss: 0.763, acc: 81.71 %\n",
      "[Epoch 12, Batch  4100] loss: 0.778, acc: 82.06 %\n",
      "[Epoch 12, Batch  4200] loss: 0.780, acc: 81.69 %\n",
      "[Epoch 12, Batch  4300] loss: 0.806, acc: 81.29 %\n",
      "[Epoch 12, Batch  4400] loss: 0.782, acc: 81.33 %\n",
      "[Epoch 12, Batch  4500] loss: 0.759, acc: 81.47 %\n",
      "[Epoch 12, Batch  4600] loss: 0.747, acc: 81.65 %\n",
      "[Epoch 12, Batch  4700] loss: 0.743, acc: 81.76 %\n",
      "[Epoch 12, Batch  4800] loss: 0.750, acc: 81.85 %\n",
      "[Epoch 12, Batch  4900] loss: 0.742, acc: 81.90 %\n",
      "[Epoch 12, Batch  5000] loss: 0.769, acc: 81.87 %\n",
      "[Epoch 12, Batch  5100] loss: 0.755, acc: 81.92 %\n",
      "[Epoch 12, Batch  5200] loss: 0.744, acc: 82.30 %\n",
      "[Epoch 12, Batch  5300] loss: 0.759, acc: 82.19 %\n",
      "[Epoch 12, Batch  5400] loss: 0.736, acc: 82.18 %\n",
      "[Epoch 12, Batch  5500] loss: 0.784, acc: 82.06 %\n",
      "[Epoch 12, Batch  5600] loss: 0.726, acc: 82.14 %\n",
      "[Epoch 12, Batch  5700] loss: 0.759, acc: 82.16 %\n",
      "[Epoch 12, Batch  5800] loss: 0.753, acc: 82.16 %\n",
      "[Epoch 12, Batch  5900] loss: 0.766, acc: 82.14 %\n",
      "[Epoch 12, Batch  6000] loss: 0.747, acc: 82.20 %\n",
      "[Epoch 12, Batch  6100] loss: 0.748, acc: 82.25 %\n",
      "[Epoch 12, Batch  6200] loss: 0.783, acc: 82.04 %\n",
      "[Epoch 12, Batch  6300] loss: 0.776, acc: 82.08 %\n",
      "[Epoch 12, Batch  6400] loss: 0.732, acc: 82.29 %\n",
      "[Epoch 12, Batch  6500] loss: 0.781, acc: 82.12 %\n",
      "[Epoch 12, Batch  6600] loss: 0.764, acc: 82.03 %\n",
      "[Epoch 12, Batch  6700] loss: 0.805, acc: 81.93 %\n",
      "[Epoch 12, Batch  6800] loss: 0.758, acc: 81.93 %\n",
      "[Epoch 12, Batch  6900] loss: 0.800, acc: 81.89 %\n",
      "[Epoch 12, Batch  7000] loss: 0.764, acc: 81.88 %\n",
      "[Epoch 12, Batch  7100] loss: 0.732, acc: 82.56 %\n",
      "[Epoch 12, Batch  7200] loss: 0.767, acc: 82.30 %\n",
      "[Epoch 12, Batch  7300] loss: 0.772, acc: 82.10 %\n",
      "[Epoch 12, Batch  7400] loss: 0.719, acc: 82.25 %\n",
      "[Epoch 12, Batch  7500] loss: 0.778, acc: 82.15 %\n",
      "[Epoch 12, Batch  7600] loss: 0.737, acc: 82.22 %\n",
      "[Epoch 12, Batch  7700] loss: 0.757, acc: 82.23 %\n",
      "[Epoch 12, Batch  7800] loss: 0.778, acc: 82.18 %\n",
      "[Epoch 12, Batch  7900] loss: 0.758, acc: 82.20 %\n",
      "[Epoch 12, Batch  8000] loss: 0.742, acc: 82.24 %\n",
      "[Epoch 12, Batch  8100] loss: 0.780, acc: 81.66 %\n",
      "[Epoch 12, Batch  8200] loss: 0.759, acc: 82.10 %\n",
      "[Epoch 12, Batch  8300] loss: 0.733, acc: 82.37 %\n",
      "[Epoch 12, Batch  8400] loss: 0.790, acc: 82.19 %\n",
      "[Epoch 12, Batch  8500] loss: 0.758, acc: 82.25 %\n",
      "[Epoch 12, Batch  8600] loss: 0.781, acc: 82.04 %\n",
      "[Epoch 12, Batch  8700] loss: 0.745, acc: 82.14 %\n",
      "[Epoch 12, Batch  8800] loss: 0.754, acc: 82.10 %\n",
      "[Epoch 12, Batch  8900] loss: 0.799, acc: 82.07 %\n",
      "[Epoch 12, Batch  9000] loss: 0.761, acc: 82.10 %\n",
      "[Epoch 12, Batch  9100] loss: 0.781, acc: 82.08 %\n",
      "[Epoch 12, Batch  9200] loss: 0.780, acc: 81.80 %\n",
      "[Epoch 12, Batch  9300] loss: 0.778, acc: 81.82 %\n",
      "[Epoch 12, Batch  9400] loss: 0.731, acc: 82.05 %\n",
      "[Epoch 12, Batch  9500] loss: 0.740, acc: 82.10 %\n",
      "[Epoch 12, Batch  9600] loss: 0.775, acc: 82.06 %\n",
      "[Epoch 12, Batch  9700] loss: 0.774, acc: 81.96 %\n",
      "[Epoch 12, Batch  9800] loss: 0.779, acc: 81.96 %\n",
      "[Epoch 12, Batch  9900] loss: 0.762, acc: 81.99 %\n",
      "[Epoch 12, Batch 10000] loss: 0.747, acc: 82.02 %\n",
      "[Epoch 12, Batch 10100] loss: 0.737, acc: 82.47 %\n",
      "[Epoch 12, Batch 10200] loss: 0.766, acc: 82.16 %\n",
      "[Epoch 12, Batch 10300] loss: 0.771, acc: 82.15 %\n",
      "[Epoch 12, Batch 10400] loss: 0.750, acc: 82.15 %\n",
      "[Epoch 12, Batch 10500] loss: 0.772, acc: 82.23 %\n",
      "[Epoch 12, Batch 10600] loss: 0.753, acc: 82.17 %\n",
      "[Epoch 12, Batch 10700] loss: 0.782, acc: 82.08 %\n",
      "[Epoch 12, Batch 10800] loss: 0.756, acc: 82.13 %\n",
      "[Epoch 12, Batch 10900] loss: 0.737, acc: 82.19 %\n",
      "[Epoch 12, Batch 11000] loss: 0.734, acc: 82.20 %\n",
      "[Epoch 12, Batch 11100] loss: 0.761, acc: 81.80 %\n",
      "[Epoch 12, Batch 11200] loss: 0.755, acc: 81.98 %\n",
      "[Epoch 12, Batch 11300] loss: 0.743, acc: 82.18 %\n",
      "[Epoch 12, Batch 11400] loss: 0.741, acc: 82.20 %\n",
      "[Epoch 12, Batch 11500] loss: 0.746, acc: 82.18 %\n",
      "[Epoch 12, Batch 11600] loss: 0.795, acc: 82.04 %\n",
      "[Epoch 12, Batch 11700] loss: 0.761, acc: 82.04 %\n",
      "[Epoch 12, Batch 11800] loss: 0.769, acc: 82.04 %\n",
      "[Epoch 12, Batch 11900] loss: 0.755, acc: 82.15 %\n",
      "[Epoch 12, Batch 12000] loss: 0.731, acc: 82.21 %\n",
      "[Epoch 12, Batch 12100] loss: 0.731, acc: 82.52 %\n",
      "[Epoch 12, Batch 12200] loss: 0.783, acc: 81.88 %\n",
      "[Epoch 12, Batch 12300] loss: 0.786, acc: 81.68 %\n",
      "[Epoch 12, Batch 12400] loss: 0.768, acc: 81.87 %\n",
      "[Epoch 12, Batch 12500] loss: 0.779, acc: 81.84 %\n",
      "[Epoch 12, Batch 12600] loss: 0.773, acc: 81.87 %\n",
      "[Epoch 12, Batch 12700] loss: 0.762, acc: 81.94 %\n",
      "[Epoch 12, Batch 12800] loss: 0.774, acc: 81.98 %\n",
      "[Epoch 12, Batch 12900] loss: 0.753, acc: 82.00 %\n",
      "[Epoch 12, Batch 13000] loss: 0.781, acc: 81.96 %\n",
      "[Epoch 12, Batch 13100] loss: 0.741, acc: 82.55 %\n",
      "[Epoch 12, Batch 13200] loss: 0.741, acc: 82.51 %\n",
      "[Epoch 12, Batch 13300] loss: 0.747, acc: 82.38 %\n",
      "[Epoch 12, Batch 13400] loss: 0.755, acc: 82.36 %\n",
      "[Epoch 12, Batch 13500] loss: 0.793, acc: 82.10 %\n",
      "[Epoch 12, Batch 13600] loss: 0.780, acc: 82.08 %\n",
      "[Epoch 12, Batch 13700] loss: 0.769, acc: 82.07 %\n",
      "[Epoch 12, Batch 13800] loss: 0.771, acc: 82.02 %\n",
      "[Epoch 12, Batch 13900] loss: 0.753, acc: 82.04 %\n",
      "[Epoch 12, Batch 14000] loss: 0.759, acc: 82.08 %\n",
      "[Epoch 12, Batch 14100] loss: 0.756, acc: 82.33 %\n",
      "[Epoch 12, Batch 14200] loss: 0.747, acc: 82.49 %\n",
      "[Epoch 12, Batch 14300] loss: 0.755, acc: 82.52 %\n",
      "[Epoch 12, Batch 14400] loss: 0.792, acc: 82.30 %\n",
      "[Epoch 12, Batch 14500] loss: 0.774, acc: 82.13 %\n",
      "[Epoch 12, Batch 14600] loss: 0.761, acc: 82.13 %\n",
      "[Epoch 12, Batch 14700] loss: 0.796, acc: 82.02 %\n",
      "[Epoch 12, Batch 14800] loss: 0.766, acc: 82.01 %\n",
      "[Epoch 12, Batch 14900] loss: 0.765, acc: 81.99 %\n",
      "[Epoch 12, Batch 15000] loss: 0.749, acc: 82.00 %\n",
      "[Epoch 12, Batch 15100] loss: 0.750, acc: 82.36 %\n",
      "[Epoch 12, Batch 15200] loss: 0.744, acc: 82.45 %\n",
      "[Epoch 12, Batch 15300] loss: 0.752, acc: 82.53 %\n",
      "[Epoch 12, Batch 15400] loss: 0.756, acc: 82.46 %\n",
      "[Epoch 12, Batch 15500] loss: 0.757, acc: 82.34 %\n",
      "[Epoch 12, Batch 15600] loss: 0.763, acc: 82.26 %\n",
      "[Epoch 12, Batch 15700] loss: 0.758, acc: 82.25 %\n",
      "[Epoch 12, Batch 15800] loss: 0.755, acc: 82.28 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainer = FineTuner()\n",
    "    trainer.process(load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6fafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainer = PreTrainer()\n",
    "    trainer.process(load=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
